[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STBDA22",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nJul 28, 2023\n\n\n[STBDA] 14wk: GAN\n\n\n김보람 \n\n\n\n\nJul 20, 2023\n\n\n[STBDA] 13wk: 오버피팅, 학습과정분석\n\n\n김보람 \n\n\n\n\nJul 17, 2023\n\n\n[STBDA] 12wk: CONV,MAXPOOL,CNN\n\n\n김보람 \n\n\n\n\nJul 13, 2023\n\n\n[STBDA] 11wk: MaxPool2D, Conv2D\n\n\n김보람 \n\n\n\n\nJul 12, 2023\n\n\n[STBDA] 10wk: softmax function, 평가지표, flatten layer\n\n\n김보람 \n\n\n\n\nJul 6, 2023\n\n\n[STBDA] 09wk(2): 확률적 경사하강법\n\n\n김보람 \n\n\n\n\nJun 26, 2023\n\n\n[STBDA] 07wk: Logistic regression\n\n\n김보람 \n\n\n\n\nJun 23, 2023\n\n\n[STBDA] 06wk: keras\n\n\n김보람 \n\n\n\n\nJun 20, 2023\n\n\n[STBDA] 05wk: tensorflow,keras\n\n\n김보람 \n\n\n\n\nJun 19, 2023\n\n\n[STBDA] 04wk: tensorflow_경사하강법\n\n\n김보람 \n\n\n\n\nJun 19, 2023\n\n\n[STBDA] 03wk: tensorflow_Variable, 미분\n\n\n김보람 \n\n\n\n\nJun 16, 2023\n\n\n[STBDA] 02wk: tensorflow_matrix\n\n\n김보람 \n\n\n\n\nJun 16, 2023\n\n\n[STBDA] 01wk: 단순선형회귀\n\n\n김보람 \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2022_04_18_(7주차)_4월18일.html",
    "href": "posts/2022_04_18_(7주차)_4월18일.html",
    "title": "[STBDA] 07wk: Logistic regression",
    "section": "",
    "text": "해당 강의노트는 전북대학교 최규빈교수님 STBDA2022 자료임\n\n\nimports\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport tensorflow.experimental.numpy as tnp\n\n2023-06-26 15:05:17.002455: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n\n\n\ntnp.experimental_enable_numpy_behavior()\n\n\nimport graphviz\ndef gv(s): return graphviz.Source('digraph G{ rankdir=\"LR\"'+s + '; }')\n\n\n\npiece-wise linear regression\nmodel: \\(y_i=\\begin{cases} x_i +0.3\\epsilon_i & x\\leq 0 \\\\ 3.5x_i +0.3\\epsilon_i & x&gt;0 \\end{cases}\\)\n\nnp.random.seed(43052)\nN=100\nx = np.linspace(-1,1,N)\nlamb = lambda x: x*1+np.random.normal()*0.3 if x&lt;0 else x*3.5+np.random.normal()*0.3\ny= np.array(list(map(lamb,x)))\ny\n\narray([-0.88497385, -0.65454563, -0.61676249, -0.84702584, -0.84785569,\n       -0.79220455, -1.3777105 , -1.27341781, -1.41643729, -1.26404671,\n       -0.79590224, -0.78824395, -0.86064773, -0.52468679, -1.18247354,\n       -0.29327295, -0.69373049, -0.90561768, -1.07554911, -0.7225404 ,\n       -0.69867774, -0.34811037,  0.11188474, -1.05046296, -0.03840085,\n       -0.38356861, -0.24299798, -0.58403161, -0.20344022, -0.13872303,\n       -0.529586  , -0.27814478, -0.10852781, -0.38294596,  0.02669763,\n       -0.23042603, -0.77720364, -0.34287396, -0.04512022, -0.30180793,\n       -0.26711438, -0.51880349, -0.53939672, -0.32052379, -0.32080763,\n        0.28917092,  0.18175206, -0.48988124, -0.08084459,  0.37706178,\n        0.14478908,  0.07621827, -0.071864  ,  0.05143365,  0.33932009,\n       -0.35071776,  0.87742867,  0.51370399,  0.34863976,  0.55855514,\n        1.14196717,  0.86421076,  0.72957843,  0.57342304,  1.54803332,\n        0.98840018,  1.11129366,  1.42410801,  1.44322465,  1.25926455,\n        1.12940772,  1.46516829,  1.16365096,  1.45560853,  1.9530553 ,\n        2.45940445,  1.52921129,  1.8606463 ,  1.86406718,  1.5866523 ,\n        1.49033473,  2.35242686,  2.12246412,  2.41951931,  2.43615052,\n        1.96024441,  2.65843789,  2.46854394,  2.76381882,  2.78547462,\n        2.56568465,  3.15212157,  3.11482949,  3.17901774,  3.31268904,\n        3.60977818,  3.40949166,  3.30306495,  3.74590922,  3.85610433])\n\n\n\nplt.plot(x,y,'.')\n\n\n\n\n\n풀이1: 단순회귀모형\n\nx= x.reshape(N,1)\ny= y.reshape(N,1)\n\n\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(1))\nnet.compile(optimizer=tf.optimizers.SGD(0.1),loss='mse')\nnet.fit(x,y,batch_size=N,epochs=1000,verbose=0) # numpy로 해도 돌아감\n\n&lt;keras.callbacks.History at 0x7fa2800be6a0&gt;\n\n\n\nnet.weights\n\n[&lt;tf.Variable 'dense/kernel:0' shape=(1, 1) dtype=float32, numpy=array([[2.2616348]], dtype=float32)&gt;,\n &lt;tf.Variable 'dense/bias:0' shape=(1,) dtype=float32, numpy=array([0.6069048], dtype=float32)&gt;]\n\n\n\nyhat = x * 2.2616348 + 0.6069048\nyhat = net.predict(x)\n\n4/4 [==============================] - 0s 502us/step\n\n\n\nplt.plot(x,y,'.')\nplt.plot(x,yhat,'--')\n\n\n\n\n- 실패: 이 모형은 epoch을 10억번 돌려도 실패할 모형임 - 왜? 아키텍처 설계자체가 틀렸음 - 꺽인부분을 표현하기에는 아키텍처의 표현력이 너무 부족하다 -&gt; under fit의 문제\n\n\n풀이2: 비선형 활성화 함수의 도입\n- 여기에서 비선형 활성화 함수는 relu\n- 네트워크를 아래와 같이 수정하자.\n(수정전) hat은 생략\n\n#collapse\ngv('''\n\"x\" -&gt; \"x*w,    bias=True\"[label=\"*w\"] ;\n\"x*w,    bias=True\" -&gt; \"y\"[label=\"indentity\"] ''')\n\n\n\n\n(수정후) hat은 생략\n\n#collapse\ngv('''\n\"x\" -&gt; \"x*w,    bias=True\"[label=\"*w\"] ;\n\"x*w,    bias=True\" -&gt; \"y\"[label=\"relu\"] ''')\n\n\n\n\n\n마지막에 \\(f(x)=x\\) 라는 함수대신에 relu를 취하는 것으로 구조를 약간 변경\n활성화함수(acitivation function)를 indentity에서 relu로 변경\n\n- relu함수란?\n\n_x = np.linspace(-1,1,100)\ntf.nn.relu(_x)\n\n&lt;tf.Tensor: shape=(100,), dtype=float64, numpy=\narray([0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.01010101, 0.03030303, 0.05050505, 0.07070707, 0.09090909,\n       0.11111111, 0.13131313, 0.15151515, 0.17171717, 0.19191919,\n       0.21212121, 0.23232323, 0.25252525, 0.27272727, 0.29292929,\n       0.31313131, 0.33333333, 0.35353535, 0.37373737, 0.39393939,\n       0.41414141, 0.43434343, 0.45454545, 0.47474747, 0.49494949,\n       0.51515152, 0.53535354, 0.55555556, 0.57575758, 0.5959596 ,\n       0.61616162, 0.63636364, 0.65656566, 0.67676768, 0.6969697 ,\n       0.71717172, 0.73737374, 0.75757576, 0.77777778, 0.7979798 ,\n       0.81818182, 0.83838384, 0.85858586, 0.87878788, 0.8989899 ,\n       0.91919192, 0.93939394, 0.95959596, 0.97979798, 1.        ])&gt;\n\n\n\nplt.plot(_x,_x)\nplt.plot(_x,tf.nn.relu(_x))\n\n\n\n\n\n파란색을 주황색으로 바꿔주는 것이 렐루함수임\n\\(f(x)=\\max(0,x)=\\begin{cases} 0 & x\\leq 0 \\\\ x & x&gt;0 \\end{cases}\\)\n\n- 아키텍처: \\(\\hat{y}_i=relu(\\hat{w}_0+\\hat{w}_1x_i)\\), \\(relu(x)=\\max(0,x)\\)\n- 풀이시작\n1단계\n\nnet2 = tf.keras.Sequential()\n\n2단계\n\ntf.random.set_seed(43053)\nl1 = tf.keras.layers.Dense(1, input_shape=(1,))\na1 = tf.keras.layers.Activation(tf.nn.relu)\n\n\nnet2.add(l1)\n\n\nnet2.layers\n\n[&lt;keras.layers.core.dense.Dense at 0x7fa264620d90&gt;]\n\n\n\nnet2.add(a1)\n\n\nnet2.layers\n\n[&lt;keras.layers.core.dense.Dense at 0x7fa264620d90&gt;,\n &lt;keras.layers.core.activation.Activation at 0x7fa337f9caf0&gt;]\n\n\n\nl1.get_weights()\n\n[array([[1.6202813]], dtype=float32), array([0.], dtype=float32)]\n\n\n\nnet2.get_weights()\n\n[array([[1.6202813]], dtype=float32), array([0.], dtype=float32)]\n\n\n(네트워크 상황 확인)\n\nu1= l1(x)\n#u1= x@l1.weights[0] + l1.weights[1]\n\n\nv1= a1(u1)\n#v1= tf.nn.relu(u1)\n\n\nplt.plot(x,x)\nplt.plot(x,u1,'--r')\nplt.plot(x,v1,'--b')\n\n\n\n\n3단계\n\nnet2.compile(optimizer=tf.optimizers.SGD(0.1),loss='mse')\n\n4단계\n\nnet2.fit(x,y,epochs=1000,verbose=0,batch_size=N)\n\n&lt;keras.callbacks.History at 0x7fa337ecbd60&gt;\n\n\n- result\n\nyhat = tf.nn.relu(x@l1.weights[0] + l1.weights[1])\nyhat = net2.predict(x)\nyhat = net2(x)\nyhat = a1(l1(x))\nyhat = net2.layers[1](net2.layers[0](x))\n\n4/4 [==============================] - 0s 519us/step\n\n\n\n위는 다 같은 코드.\n\n\nplt.plot(x,y,'.')\nplt.plot(x,yhat,'--')\n\n\n\n\n- discussion - 이것 역시 수백억번 에폭을 반복해도 이 이상 적합이 힘들다 \\(\\to\\) 모형의 표현력이 떨어진다. - 해결책: 주황색점선이 2개 있다면 어떨까?\n\n\n풀이3: 노드수추가 + 레이어추가\n목표: 2개의 주황색 점선을 만들자.\n1단계\n\nnet3 = tf.keras.Sequential()\n\n2단계\n\ntf.random.set_seed(43053)\nl1 = tf.keras.layers.Dense(2,input_shape=(1,)) # 2로 하면 직선이 두개\na1 = tf.keras.layers.Activation(tf.nn.relu)\n\n\nnet3.add(l1)\nnet3.add(a1)\n\n(네트워크 상황 확인)\n\nl1(x).shape\n# l1(x) : (100,1) -&gt; (100,2)\n\nTensorShape([100, 2])\n\n\n\nplt.plot(x,x)\nplt.plot(x,l1(x),'--')\n\n\n\n\n\nplt.plot(x,x)\nplt.plot(x,a1(l1(x)),'--')\n\n\n\n\n- 이 상태에서는 yhat이 안나온다. 왜? - 차원이 안맞음. a1(l1(x))의 차원은 (N,2)인데 최종적인 yhat의 차원은 (N,1)이어야 함. - 차원이 어찌저찌 맞다고 쳐도 relu를 통과하면 항상 yhat&gt;0 임. 따라서 음수값을 가지는 y는 0으로 밖에 맞출 수 없음.\n- 해결책: a1(l1(x))에 연속으로(Sequential하게!) 또 다른 레이어를 설계! (N,2) -&gt; (N,1) 이 되도록! - yhat= bias + weight1 * a1(l1(x))[0] + weight2 * a1(l1(x))[1]\n- 즉 a1(l1(x)) 를 새로운 입력으로 해석하고 출력을 만들어주는 선형모형을 다시태우면 된다. - 입력차원: 2 - 출력차원: 1\n\nnet3.layers\n\n[&lt;keras.layers.core.dense.Dense at 0x7fa337e5f6a0&gt;,\n &lt;keras.layers.core.activation.Activation at 0x7fa337f9cac0&gt;]\n\n\n\ntf.random.set_seed(43053)\nl2 = tf.keras.layers.Dense(1, input_shape=(2,))\n\n\nnet3.add(l2)\n\n\nnet3.layers\n\n[&lt;keras.layers.core.dense.Dense at 0x7fa337e5f6a0&gt;,\n &lt;keras.layers.core.activation.Activation at 0x7fa337f9cac0&gt;,\n &lt;keras.layers.core.dense.Dense at 0x7fa337c887f0&gt;]\n\n\n\nnet3.summary()\n\nModel: \"sequential_2\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense_2 (Dense)             (None, 2)                 4         \n                                                                 \n activation_1 (Activation)   (None, 2)                 0         \n                                                                 \n dense_3 (Dense)             (None, 1)                 3         \n                                                                 \n=================================================================\nTotal params: 7\nTrainable params: 7\nNon-trainable params: 0\n_________________________________________________________________\n\n\n- 추정해야할 파라메터수가 4,0,3으로 나온다.\n- 수식표현: \\(X \\to X@W^{(1)}+b^{(1)} \\to relu(X@W^{(1)}+b^{(1)}) \\to relu(X@W^{(1)}+b^{(1)})@W^{(2)}+b^{(2)}=yhat\\)\n\n\\(X\\): (N,1)\n\\(W^{(1)}\\): (1,2) ==&gt; 파라메터 2개 추정\n\\(b^{(1)}\\): (2,) ==&gt; 파라메터 2개가 추가 // 여기까지 추정할 파라메터는 4개\n\\(W^{(2)}\\): (2,1) ==&gt; 파라메터 2개 추정\n\\(b^{(2)}\\): (1,) ==&gt; 파라메터 1개가 추가 // 따라서 3개\n\n- 참고: 추정할 파라메터수가 많다 = 복잡한 모형이다. - 초거대AI: 추정할 파라메터수가 엄청 많은..\n\nnet3.weights\n\n[&lt;tf.Variable 'dense_2/kernel:0' shape=(1, 2) dtype=float32, numpy=array([[0.98630846, 0.59210145]], dtype=float32)&gt;,\n &lt;tf.Variable 'dense_2/bias:0' shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)&gt;,\n &lt;tf.Variable 'dense_3/kernel:0' shape=(2, 1) dtype=float32, numpy=\n array([[0.52757335],\n        [0.33660662]], dtype=float32)&gt;,\n &lt;tf.Variable 'dense_3/bias:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)&gt;]\n\n\n\nl1.weights\n\n[&lt;tf.Variable 'dense_2/kernel:0' shape=(1, 2) dtype=float32, numpy=array([[0.98630846, 0.59210145]], dtype=float32)&gt;,\n &lt;tf.Variable 'dense_2/bias:0' shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)&gt;]\n\n\n\nl2.weights\n\n[&lt;tf.Variable 'dense_3/kernel:0' shape=(2, 1) dtype=float32, numpy=\n array([[0.52757335],\n        [0.33660662]], dtype=float32)&gt;,\n &lt;tf.Variable 'dense_3/bias:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)&gt;]\n\n\n- 좀 더 간단한 수식표현: \\(X \\to (u_1 \\to v_1) \\to (u_2 \\to v_2) = yhat\\) - \\(u_1= X@W^{(1)}+b^{(1)}\\) - \\(v_1= relu(u_1)\\) - \\(u_2= v_1@W^{(2)}+b^{(2)}\\) - \\(v_2= indentity(u_2):=yhat\\)\n\n#collapse\ngv('''\nsubgraph cluster_1{\n    style=filled;\n    color=lightgrey;\n    \"X\"\n    label = \"Layer 0\"\n}\nsubgraph cluster_2{\n    style=filled;\n    color=lightgrey;\n    \"X\" -&gt; \"u1[:,0]\"[label=\"*W1[0,0]\"]\n    \"X\" -&gt; \"u1[:,1]\"[label=\"*W1[0,1]\"]\n    \"u1[:,0]\" -&gt; \"v1[:,0]\"[label=\"relu\"]\n    \"u1[:,1]\" -&gt; \"v1[:,1]\"[label=\"relu\"]\n    label = \"Layer 1\"\n}\nsubgraph cluster_3{\n    style=filled;\n    color=lightgrey;\n    \"v1[:,0]\" -&gt; \"yhat\"[label=\"*W2[0,0]\"]\n    \"v1[:,1]\" -&gt; \"yhat\"[label=\"*W2[1,0]\"]\n    label = \"Layer 2\"\n}\n''')\n\n\n\n\n\n#collapse\ngv('''\nsubgraph cluster_1{\n    style=filled;\n    color=lightgrey;\n    \"X\"\n    label = \"Layer 0\"\n}\nsubgraph cluster_2{\n    style=filled;\n    color=lightgrey;\n    \"X\" -&gt; \"node1\"\n    \"X\" -&gt; \"node2\"\n    label = \"Layer 1: relu\"\n}\nsubgraph cluster_3{\n    style=filled;\n    color=lightgrey;\n    \"node1\" -&gt; \"yhat\"\n    \"node2\" -&gt; \"yhat\"\n    label = \"Layer 2\"\n}\n''')\n\n\n\n\n3단계\n\nnet3.compile(loss='mse',optimizer=tf.optimizers.SGD(0.1))\n\n4단계\n\nnet3.fit(x,y,epochs=1000,verbose=0, batch_size=N)\n\n&lt;keras.callbacks.History at 0x7fa337ca7610&gt;\n\n\n- 결과확인\n\nnet3.weights\n\n[&lt;tf.Variable 'dense_2/kernel:0' shape=(1, 2) dtype=float32, numpy=array([[1.7125574 , 0.96457523]], dtype=float32)&gt;,\n &lt;tf.Variable 'dense_2/bias:0' shape=(2,) dtype=float32, numpy=array([-0.10849824,  0.80890274], dtype=float32)&gt;,\n &lt;tf.Variable 'dense_3/kernel:0' shape=(2, 1) dtype=float32, numpy=\n array([[1.5033181],\n        [1.1611973]], dtype=float32)&gt;,\n &lt;tf.Variable 'dense_3/bias:0' shape=(1,) dtype=float32, numpy=array([-0.9116387], dtype=float32)&gt;]\n\n\n\nplt.plot(x,y,'.')\nplt.plot(x,net3(x),'--')\n\n\n\n\n- 분석\n\nplt.plot(x,y,'.')\nplt.plot(x,l1(x),'--')\n\n\n\n\n\nplt.plot(x,y,'.')\nplt.plot(x,a1(l1(x)),'--')\n\n\n\n\n\nplt.plot(x,y,'.')\nplt.plot(x,l2(a1(l1(x))),'--')\n\n\n\n\n- 마지막 2개의 그림을 분석\n\nl2.weights\n\n[&lt;tf.Variable 'dense_3/kernel:0' shape=(2, 1) dtype=float32, numpy=\n array([[1.5033181],\n        [1.1611973]], dtype=float32)&gt;,\n &lt;tf.Variable 'dense_3/bias:0' shape=(1,) dtype=float32, numpy=array([-0.9116387], dtype=float32)&gt;]\n\n\n\nfig, (ax1,ax2,ax3) = plt.subplots(1,3)\nfig.set_figwidth(12)\nax1.plot(x,y,'.')\nax1.plot(x,a1(l1(x))[:,0],'--r')\nax1.plot(x,a1(l1(x))[:,1],'--b')\nax2.plot(x,y,'.')\nax2.plot(x,a1(l1(x))[:,0]*1.5033181,'--r')\nax2.plot(x,a1(l1(x))[:,1]*(1.1611973)-0.9116387,'--b')\nax3.plot(x,y,'.')\nax3.plot(x,a1(l1(x))[:,0]*1.5033181+a1(l1(x))[:,1]*(1.1611973)-0.9116387,'--')\n\n\n\n\n\n\n\n풀이3의 실패\n\ntf.random.set_seed(43054)\n## 1단계\nnet3 = tf.keras.Sequential()\n## 2단계\nnet3.add(tf.keras.layers.Dense(2))\nnet3.add(tf.keras.layers.Activation('relu'))\nnet3.add(tf.keras.layers.Dense(1))\n## 3단계\nnet3.compile(optimizer=tf.optimizers.SGD(0.1),loss='mse')\n## 4단계\nnet3.fit(x,y,epochs=1000,verbose=0,batch_size=N)\n\n&lt;keras.callbacks.History at 0x7fa305a1a310&gt;\n\n\n\nplt.plot(x,y,'.')\nplt.plot(x,net3(x),'--')\n\n\n\n\n- 엥? 에폭이 부족한가?\n\nnet3.fit(x,y,epochs=10000,verbose=0,batch_size=N)\nplt.plot(x,y,'.')\nplt.plot(x,net3(x),'--')\n\n\n\n\n- 실패분석\n\nl1,a1,l2 = net3.layers\n\n\nl2.weights\n\n[&lt;tf.Variable 'dense_7/kernel:0' shape=(2, 1) dtype=float32, numpy=\n array([[ 1.7770029],\n        [-0.7268499]], dtype=float32)&gt;,\n &lt;tf.Variable 'dense_7/bias:0' shape=(1,) dtype=float32, numpy=array([-0.60076195], dtype=float32)&gt;]\n\n\n\nfig, (ax1,ax2,ax3,ax4) = plt.subplots(1,4)\nfig.set_figwidth(16)\nax1.plot(x,y,'.')\nax1.plot(x,l1(x)[:,0],'--r')\nax1.plot(x,l1(x)[:,1],'--b')\nax2.plot(x,y,'.')\nax2.plot(x,a1(l1(x))[:,0],'--r')\nax2.plot(x,a1(l1(x))[:,1],'--b')\nax3.plot(x,y,'.')\nax3.plot(x,a1(l1(x))[:,0]*1.7770029,'--r')\nax3.plot(x,a1(l1(x))[:,1]*(-0.7268499)+(-0.60076195),'--b')\nax4.plot(x,y,'.')\nax4.plot(x,a1(l1(x))[:,0]*1.7770029+a1(l1(x))[:,1]*(-0.7268499)+(-0.60076195),'--')\n\n\n\n\n\n보니까 파란색선이 하는 역할을 없음\n그런데 생각해보니까 이 상황에서는 파란색선이 할수 있는 일이 별로 없음\n왜? 지금은 나름 빨간색선에 의해서 최적화가 된 상태임 \\(\\to\\) 빨간선이 뭔가 하려고하면 최적화된 상태가 깨질 수 있음 (loss 증가)\n즉 이 상황 자체가 나름 최적회된 상태이다. 이러한 현상을 “global minimum을 찾지 못하고 local minimum에 빠졌다”라고 표현한다.\n\n확인:\n\nnet3.weights\n\n[&lt;tf.Variable 'dense_6/kernel:0' shape=(1, 2) dtype=float32, numpy=array([[1.9579618 , 0.46560898]], dtype=float32)&gt;,\n &lt;tf.Variable 'dense_6/bias:0' shape=(2,) dtype=float32, numpy=array([ 0.34100613, -0.4658857 ], dtype=float32)&gt;,\n &lt;tf.Variable 'dense_7/kernel:0' shape=(2, 1) dtype=float32, numpy=\n array([[ 1.7770029],\n        [-0.7268499]], dtype=float32)&gt;,\n &lt;tf.Variable 'dense_7/bias:0' shape=(1,) dtype=float32, numpy=array([-0.60076195], dtype=float32)&gt;]\n\n\n\nW1= tf.Variable(tnp.array([[1.9579618,  0.46560898 ]]))\nb1= tf.Variable(tnp.array([0.34100613,  -0.4658857 ]))\nW2= tf.Variable(tnp.array([[1.7770029],[-0.7268499 ]]))\nb2= tf.Variable(tnp.array([-0.60076195]))\n\n\nwith tf.GradientTape() as tape:\n    u = tf.constant(x) @ W1 + b1\n    v = tf.nn.relu(u)\n    yhat = v@W2 + b2\n    loss = tf.losses.mse(y,yhat)\n\n\ntape.gradient(loss,[W1,b1,W2,b2])\n\n[&lt;tf.Tensor: shape=(1, 2), dtype=float64, numpy=array([[-6.01630956e-05,  0.00000000e+00]])&gt;,\n &lt;tf.Tensor: shape=(2,), dtype=float64, numpy=array([-1.22677221e-05,  0.00000000e+00])&gt;,\n &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy=\n array([[-6.86439011e-05],\n        [ 0.00000000e+00]])&gt;,\n &lt;tf.Tensor: shape=(1,), dtype=float64, numpy=array([-3.2899797e-05])&gt;]\n\n\n예상대로 계수값이 거의 다 0이다.\n\n\n풀이4: 노드수를 더 추가한다면?\n- 노드수를 더 추가해보면 어떻게 될까? (주황색 점선이 더 여러개 있다면?)\n\n#collapse\ngv('''\nsubgraph cluster_1{\n    style=filled;\n    color=lightgrey;\n    \"X\"\n    label = \"Layer 0\"\n}\nsubgraph cluster_2{\n    style=filled;\n    color=lightgrey;\n    \"X\" -&gt; \"node1\"\n    \"X\" -&gt; \"node2\"\n    \"X\" -&gt; \"...\"\n    \"X\" -&gt; \"node512\"\n    label = \"Layer 1: relu\"\n}\nsubgraph cluster_3{\n    style=filled;\n    color=lightgrey;\n    \"node1\" -&gt; \"yhat\"\n    \"node2\" -&gt; \"yhat\"\n    \"...\" -&gt; \"yhat\"\n    \"node512\" -&gt; \"yhat\"\n    label = \"Layer 2\"\n}\n''')\n\n\n\n\n\ntf.random.set_seed(43056)\nnet4= tf.keras.Sequential()\nnet4.add(tf.keras.layers.Dense(512,activation='relu')) # 이렇게 해도됩니다.\nnet4.add(tf.keras.layers.Dense(1))\nnet4.compile(loss='mse',optimizer=tf.optimizers.SGD(0.1))\nnet4.fit(x,y,epochs=1000,verbose=0,batch_size=N)\n\n&lt;keras.callbacks.History at 0x7fa30336f880&gt;\n\n\n\nplt.plot(x,y,'.')\nplt.plot(x,net4(x),'--')\n\n\n\n\n\n잘된다..\n한두개의 노드가 역할을 못해도 다른노드들이 잘 보완해주는듯!\n\n- 노드수가 많으면 무조건 좋다? -&gt; 대부분 나쁘지 않음. 그런데 종종 맞추지 말아야할것도 맞춤.. (overfit)\n\nnp.random.seed(43052)\nN=100\n_x = np.linspace(0,1,N).reshape(N,1)\n_y = np.random.normal(loc=0,scale=0.001,size=(N,1))\nplt.plot(_x,_y)\n\n\n\n\n\ntf.random.set_seed(43052)\nnet4 = tf.keras.Sequential()\nnet4.add(tf.keras.layers.Dense(512,activation='relu'))\nnet4.add(tf.keras.layers.Dense(1))\nnet4.compile(loss='mse',optimizer=tf.optimizers.SGD(0.5))\nnet4.fit(_x,_y,epochs=1000,verbose=0,batch_size=N)\n\n&lt;keras.callbacks.History at 0x7fa324f1d040&gt;\n\n\n\nplt.plot(_x,_y)\nplt.plot(_x,net4(_x),'--')\n\n\n\n\n\n이 예제는 추후 다시 공부할 예정\n\n\n\n\nLogistic regression\n\nmotive\n- 현실에서 이런 경우가 많음 - \\(x\\)가 커질수록 (혹은 작아질수록) 성공확률이 올라간다.\n- 이러한 모형은 아래와 같이 설계할 수 있음 &lt;– 외우세요!! - \\(y_i \\sim Ber(\\pi_i)\\), where \\(\\pi_i=\\frac{\\exp(w_0+w_1x_i)}{1+\\exp(w_0+w_1x_i)}\\)\n\n\\(\\hat{y}_i =\\frac{\\exp(\\hat{w}_0+\\hat{w}_1x_i)}{1+\\exp(\\hat{w}_0+\\hat{w}_1x_i)}=\\frac{1}{1+exp(-\\hat{w}_0-\\hat{w}_1x_i)}\\)\n\\(loss=-\\frac{1}{n}\\sum_{i=1}^{n}\\big(y_i\\log(\\hat{y}_i)+(1-y_i)\\log(1-\\hat{y}_i)\\big)\\)\n\n- 위와 같은 손실함수를 BCEloss라고 부른다. (BCE는 Binary Cross Entropy의 약자)\n\n\n예제\n\nN = 2000\n\n\nx = tnp.linspace(-1,1,N).reshape(N,1)\nw0 = -1\nw1 = 5\nu = w0 + x*w1\n#v = tf.constant(np.exp(u)/(1+np.exp(u))) # v=πi\nv = tf.nn.sigmoid(u)\ny = tf.constant(np.random.binomial(1,v),dtype=tf.float64)\n\n\nplt.plot(x,y,'.',alpha=0.02)\nplt.plot(x,v,'--r')\n\n\n\n\n- 이 아키텍처(yhat을 얻어내는 과정)를 다어어그램으로 나타내면 아래와 같다.\n\n#collapse\ngv('''\nsubgraph cluster_1{\n    style=filled;\n    color=lightgrey;\n    \"x\"\n    label = \"Layer 0\"\n}\nsubgraph cluster_2{\n    style=filled;\n    color=lightgrey;\n    \"x\" -&gt; \"x*w, bias=True\"[label=\"*w\"]\n    \"x*w, bias=True\" -&gt; \"yhat\"[label=\"sigmoid\"]\n    label = \"Layer 1\"\n}\n''')\n\n\n\n\n- 또는 간단하게 아래와 같이 쓸 수 있다.\n\n#collapse\ngv('''\nsubgraph cluster_1{\n    style=filled;\n    color=lightgrey;\n    x\n    label = \"Layer 0\"\n}\nsubgraph cluster_2{\n    style=filled;\n    color=lightgrey;\n    x -&gt; \"node1=yhat\"\n    label = \"Layer 1: sigmoid\"\n}\n''')\n\n\n\n\n- 케라스를 이용하여 적합을 해보면\n\n\\(loss=-\\frac{1}{n}\\sum_{i=1}^{n}\\big(y_i\\log(\\hat{y}_i)+(1-y_i)\\log(1-\\hat{y}_i)\\big)\\)\n\n\ntf.random.set_seed(43052)\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(1,activation='sigmoid'))\nbceloss_fn = lambda y,yhat: -tf.reduce_mean(y*tnp.log(yhat) + (1-y)*tnp.log(1-yhat))\nnet.compile(loss=bceloss_fn, optimizer=tf.optimizers.SGD(0.1))\nnet.fit(x,y,epochs=1000,verbose=0,batch_size=N)\n\nWARNING:tensorflow:From /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\nInstructions for updating:\nLambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n\n\n&lt;keras.callbacks.History at 0x7fa324cddbb0&gt;\n\n\n\nnet.weights\n\n[&lt;tf.Variable 'dense_12/kernel:0' shape=(1, 1) dtype=float32, numpy=array([[4.307486]], dtype=float32)&gt;,\n &lt;tf.Variable 'dense_12/bias:0' shape=(1,) dtype=float32, numpy=array([-0.82411796], dtype=float32)&gt;]\n\n\n\nplt.plot(x,y,'.',alpha=0.1)\nplt.plot(x,v,'--r')\nplt.plot(x,net(x),'--b')\n\n\n\n\n\n\nMSE loss?\n- mse loss를 쓰면 왜 안되는지?\n\ntf.random.set_seed(43052)\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(1,activation='sigmoid'))\nmseloss_fn = lambda y,yhat: tf.reduce_mean((y-yhat)**2)\nnet.compile(loss=mseloss_fn, optimizer=tf.optimizers.SGD(0.1))\nnet.fit(x,y,epochs=1000,verbose=0,batch_size=N)\n\n&lt;keras.callbacks.History at 0x7fa325764a00&gt;\n\n\n\nplt.plot(x,y,'.',alpha=0.1)\nplt.plot(x,v,'--r')\nplt.plot(x,net(x),'--b')\n\n\n\n\n\n일단 BCE loss와 비교해보니까 동일 초기값, 동일 epochs에서 적합이 별로임\n\n\n\nMSE loss vs BCE loss\n- MSEloss, BCEloss의 시각화\n\nw0, w1 = np.meshgrid(np.arange(-10,3,0.2), np.arange(-1,10,0.2), indexing='ij')\nw0, w1 = w0.reshape(-1), w1.reshape(-1)\n\ndef loss_fn1(w0,w1):\n    u = w0+w1*x\n    yhat = np.exp(u)/(np.exp(u)+1)\n    return mseloss_fn(y,yhat)\n\ndef loss_fn2(w0,w1):\n    u = w0+w1*x\n    yhat = np.exp(u)/(np.exp(u)+1)\n    return bceloss_fn(y,yhat)\n\nloss1 = list(map(loss_fn1,w0,w1))\nloss2 = list(map(loss_fn2,w0,w1))\n\n\nfig = plt.figure()\nfig.set_figwidth(9)\nfig.set_figheight(9)\nax1=fig.add_subplot(1,2,1,projection='3d')\nax2=fig.add_subplot(1,2,2,projection='3d')\nax1.elev=15\nax2.elev=15\nax1.azim=75\nax2.azim=75\nax1.scatter(w0,w1,loss1,s=0.1)\nax2.scatter(w0,w1,loss2,s=0.1)\n\n&lt;mpl_toolkits.mplot3d.art3d.Path3DCollection at 0x7fa3254a7cd0&gt;\n\n\n\n\n\n\n왼쪽곡면(MSEloss)보다 오른쪽곡면(BCEloss)이 좀더 예쁘게 생김 -&gt; 오른쪽 곡면에서 더 학습이 잘될것 같음\n\n\n\n학습과정 시각화예시1\n- 파라메터학습과정 시각화 // 옵티마이저: SGD, 초기값: (w0,w1) = (-3.0,-1.0)\n\n데이터정리\n\n\nX = tf.concat([tf.ones(N,dtype=tf.float64).reshape(N,1),x],axis=1)\nX\n\n&lt;tf.Tensor: shape=(2000, 2), dtype=float64, numpy=\narray([[ 1.       , -1.       ],\n       [ 1.       , -0.9989995],\n       [ 1.       , -0.997999 ],\n       ...,\n       [ 1.       ,  0.997999 ],\n       [ 1.       ,  0.9989995],\n       [ 1.       ,  1.       ]])&gt;\n\n\n\n1ter돌려봄\n\n\nnet_mse = tf.keras.Sequential()\nnet_mse.add(tf.keras.layers.Dense(1,use_bias=False,activation='sigmoid'))\nnet_mse.compile(optimizer=tf.optimizers.SGD(0.1),loss=mseloss_fn)\nnet_mse.fit(X,y,epochs=1,batch_size=N)\n\n1/1 [==============================] - 0s 85ms/step - loss: 0.2281\n\n\n&lt;keras.callbacks.History at 0x7fa3253f1fd0&gt;\n\n\n\nnet_bce = tf.keras.Sequential()\nnet_bce.add(tf.keras.layers.Dense(1,use_bias=False,activation='sigmoid'))\nnet_bce.compile(optimizer=tf.optimizers.SGD(0.1),loss=bceloss_fn)\nnet_bce.fit(X,y,epochs=1,batch_size=N)\n\n1/1 [==============================] - 0s 94ms/step - loss: 0.7711\n\n\n&lt;keras.callbacks.History at 0x7fa32546da30&gt;\n\n\n\nnet_mse.get_weights(), net_bce.get_weights()\n\n([array([[0.19067296],\n         [0.35189584]], dtype=float32)],\n [array([[-1.0962652 ],\n         [-0.14414385]], dtype=float32)])\n\n\n\nnet_mse.set_weights([tnp.array([[-3.0 ],[ -1.0]],dtype=tf.float32)])\nnet_bce.set_weights([tnp.array([[-3.0 ],[ -1.0]],dtype=tf.float32)])\n\n\nnet_mse.get_weights(), net_bce.get_weights()\n\n([array([[-3.],\n         [-1.]], dtype=float32)],\n [array([[-3.],\n         [-1.]], dtype=float32)])\n\n\n\n학습과정기록: 15에폭마다 기록\n\n\nWhat_mse = tnp.array([[-3.0 ],[ -1.0]],dtype=tf.float32)\nWhat_bce = tnp.array([[-3.0 ],[ -1.0]],dtype=tf.float32)\n\n\nfor k in range(29):\n    net_mse.fit(X,y,epochs=15,batch_size=N,verbose=0)\n    net_bce.fit(X,y,epochs=15,batch_size=N,verbose=0)\n    What_mse = tf.concat([What_mse,net_mse.weights[0]],axis=1)\n    What_bce = tf.concat([What_bce,net_bce.weights[0]],axis=1)\n\n\n시각화\n\n\nfrom matplotlib import animation\nplt.rcParams[\"animation.html\"] = \"jshtml\"\n\n\nfig = plt.figure()\nfig.set_figwidth(6)\nfig.set_figheight(6)\nfig.suptitle(\"SGD, Winit=(-3,-1)\")\nax1=fig.add_subplot(2,2,1,projection='3d')\nax2=fig.add_subplot(2,2,2,projection='3d')\nax1.elev=15;ax2.elev=15;ax1.azim=75;ax2.azim=75\nax3=fig.add_subplot(2,2,3)\nax4=fig.add_subplot(2,2,4)\n\nax1.scatter(w0,w1,loss1,s=0.1);ax1.scatter(-1,5,loss_fn1(-1,5),color='red',marker='*',s=200)\nax2.scatter(w0,w1,loss2,s=0.1);ax2.scatter(-1,5,loss_fn2(-1,5),color='red',marker='*',s=200)\n\nax3.plot(x,y,','); ax3.plot(x,v,'--r');\nline3, = ax3.plot(x,1/(1+np.exp(-X@What_mse[:,0])),'--b')\nax4.plot(x,y,','); ax4.plot(x,v,'--r')\nline4, = ax4.plot(x,1/(1+np.exp(-X@What_bce[:,0])),'--b')\n\ndef animate(i):\n    _w0_mse,_w1_mse = What_mse[:,i]\n    _w0_bce,_w1_bce = What_bce[:,i]\n    ax1.scatter(_w0_mse, _w1_mse, loss_fn1(_w0_mse, _w1_mse),color='gray')\n    ax2.scatter(_w0_bce, _w1_bce, loss_fn2(_w0_bce, _w1_bce),color='gray')\n    line3.set_ydata(1/(1+np.exp(-X@What_mse[:,i])))\n    line4.set_ydata(1/(1+np.exp(-X@What_bce[:,i])))\n\nani = animation.FuncAnimation(fig, animate, frames=30)\nplt.close()\nani\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\n학습과정 시각화예시2\n- 파라메터학습과정 시각화 // 옵티마이저: Adam, 초기값: (w0,w1) = (-3.0,-1.0)\n\n데이터정리\n\n\nX = tf.concat([tf.ones(N,dtype=tf.float64).reshape(N,1),x],axis=1)\nX\n\n&lt;tf.Tensor: shape=(2000, 2), dtype=float64, numpy=\narray([[ 1.       , -1.       ],\n       [ 1.       , -0.9989995],\n       [ 1.       , -0.997999 ],\n       ...,\n       [ 1.       ,  0.997999 ],\n       [ 1.       ,  0.9989995],\n       [ 1.       ,  1.       ]])&gt;\n\n\n\n1ter돌려봄\n\n\nnet_mse = tf.keras.Sequential()\nnet_mse.add(tf.keras.layers.Dense(1,use_bias=False,activation='sigmoid'))\nnet_mse.compile(optimizer=tf.optimizers.Adam(0.1),loss=mseloss_fn)\nnet_mse.fit(X,y,epochs=1,batch_size=N)\n\n1/1 [==============================] - 0s 102ms/step - loss: 0.3403\n\n\n&lt;keras.callbacks.History at 0x7fa32518ef10&gt;\n\n\n\nnet_bce = tf.keras.Sequential()\nnet_bce.add(tf.keras.layers.Dense(1,use_bias=False,activation='sigmoid'))\nnet_bce.compile(optimizer=tf.optimizers.Adam(0.1),loss=bceloss_fn)\nnet_bce.fit(X,y,epochs=1,batch_size=N)\n\n1/1 [==============================] - 0s 106ms/step - loss: 0.8690\n\n\n&lt;keras.callbacks.History at 0x7fa324bbaa00&gt;\n\n\n\nnet_mse.get_weights(), net_bce.get_weights()\n\n([array([[1.2018752 ],\n         [0.73809683]], dtype=float32)],\n [array([[-0.9399656],\n         [-0.5219858]], dtype=float32)])\n\n\n\nnet_mse.set_weights([tnp.array([[-3.0 ],[ -1.0]],dtype=tf.float32)])\nnet_bce.set_weights([tnp.array([[-3.0 ],[ -1.0]],dtype=tf.float32)])\n\n\nnet_mse.get_weights(), net_bce.get_weights()\n\n([array([[-3.],\n         [-1.]], dtype=float32)],\n [array([[-3.],\n         [-1.]], dtype=float32)])\n\n\n\n학습과정기록: 15에폭마다 기록\n\n\nWhat_mse = tnp.array([[-3.0 ],[ -1.0]],dtype=tf.float32)\nWhat_bce = tnp.array([[-3.0 ],[ -1.0]],dtype=tf.float32)\n\n\nfor k in range(29):\n    net_mse.fit(X,y,epochs=15,batch_size=N,verbose=0)\n    net_bce.fit(X,y,epochs=15,batch_size=N,verbose=0)\n    What_mse = tf.concat([What_mse,net_mse.weights[0]],axis=1)\n    What_bce = tf.concat([What_bce,net_bce.weights[0]],axis=1)\n\n\n시각화\n\n\nfrom matplotlib import animation\nplt.rcParams[\"animation.html\"] = \"jshtml\"\n\n\nfig = plt.figure()\nfig.set_figwidth(6)\nfig.set_figheight(6)\nfig.suptitle(\"Adam, Winit=(-3,-1)\")\nax1=fig.add_subplot(2,2,1,projection='3d')\nax2=fig.add_subplot(2,2,2,projection='3d')\nax1.elev=15;ax2.elev=15;ax1.azim=75;ax2.azim=75\nax3=fig.add_subplot(2,2,3)\nax4=fig.add_subplot(2,2,4)\n\nax1.scatter(w0,w1,loss1,s=0.1);ax1.scatter(-1,5,loss_fn1(-1,5),color='red',marker='*',s=200)\nax2.scatter(w0,w1,loss2,s=0.1);ax2.scatter(-1,5,loss_fn2(-1,5),color='red',marker='*',s=200)\n\nax3.plot(x,y,','); ax3.plot(x,v,'--r');\nline3, = ax3.plot(x,1/(1+np.exp(-X@What_mse[:,0])),'--b')\nax4.plot(x,y,','); ax4.plot(x,v,'--r')\nline4, = ax4.plot(x,1/(1+np.exp(-X@What_bce[:,0])),'--b')\n\ndef animate(i):\n    _w0_mse,_w1_mse = What_mse[:,i]\n    _w0_bce,_w1_bce = What_bce[:,i]\n    ax1.scatter(_w0_mse, _w1_mse, loss_fn1(_w0_mse, _w1_mse),color='gray')\n    ax2.scatter(_w0_bce, _w1_bce, loss_fn2(_w0_bce, _w1_bce),color='gray')\n    line3.set_ydata(1/(1+np.exp(-X@What_mse[:,i])))\n    line4.set_ydata(1/(1+np.exp(-X@What_bce[:,i])))\n\nani = animation.FuncAnimation(fig, animate, frames=30)\nplt.close()\nani\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\n학습과정 시각화예시3\n- 파라메터학습과정 시각화 // 옵티마이저: Adam, 초기값: (w0,w1) = (-10.0,-1.0)\n\n데이터정리\n\n\nX = tf.concat([tf.ones(N,dtype=tf.float64).reshape(N,1),x],axis=1)\nX\n\n&lt;tf.Tensor: shape=(2000, 2), dtype=float64, numpy=\narray([[ 1.       , -1.       ],\n       [ 1.       , -0.9989995],\n       [ 1.       , -0.997999 ],\n       ...,\n       [ 1.       ,  0.997999 ],\n       [ 1.       ,  0.9989995],\n       [ 1.       ,  1.       ]])&gt;\n\n\n\n1ter돌려봄\n\n\nnet_mse = tf.keras.Sequential()\nnet_mse.add(tf.keras.layers.Dense(1,use_bias=False,activation='sigmoid'))\nnet_mse.compile(optimizer=tf.optimizers.Adam(0.1),loss=mseloss_fn)\nnet_mse.fit(X,y,epochs=1,batch_size=N)\n\n1/1 [==============================] - 0s 100ms/step - loss: 0.4499\n\n\n&lt;keras.callbacks.History at 0x7fa324a3d0d0&gt;\n\n\n\nnet_bce = tf.keras.Sequential()\nnet_bce.add(tf.keras.layers.Dense(1,use_bias=False,activation='sigmoid'))\nnet_bce.compile(optimizer=tf.optimizers.Adam(0.1),loss=bceloss_fn)\nnet_bce.fit(X,y,epochs=1,batch_size=N)\n\n1/1 [==============================] - 0s 114ms/step - loss: 1.0827\n\n\n&lt;keras.callbacks.History at 0x7fa303477850&gt;\n\n\n\nnet_mse.get_weights(), net_bce.get_weights()\n\n([array([[ 0.61489564],\n         [-1.2362169 ]], dtype=float32)],\n [array([[ 0.89960575],\n         [-0.63551056]], dtype=float32)])\n\n\n\nnet_mse.set_weights([tnp.array([[-10.0 ],[ -1.0]],dtype=tf.float32)])\nnet_bce.set_weights([tnp.array([[-10.0 ],[ -1.0]],dtype=tf.float32)])\n\n\nnet_mse.get_weights(), net_bce.get_weights()\n\n([array([[-10.],\n         [ -1.]], dtype=float32)],\n [array([[-10.],\n         [ -1.]], dtype=float32)])\n\n\n\n학습과정기록: 15에폭마다 기록\n\n\nWhat_mse = tnp.array([[-10.0 ],[ -1.0]],dtype=tf.float32)\nWhat_bce = tnp.array([[-10.0 ],[ -1.0]],dtype=tf.float32)\n\n\nfor k in range(29):\n    net_mse.fit(X,y,epochs=15,batch_size=N,verbose=0)\n    net_bce.fit(X,y,epochs=15,batch_size=N,verbose=0)\n    What_mse = tf.concat([What_mse,net_mse.weights[0]],axis=1)\n    What_bce = tf.concat([What_bce,net_bce.weights[0]],axis=1)\n\n\n시각화\n\n\nfrom matplotlib import animation\nplt.rcParams[\"animation.html\"] = \"jshtml\"\n\n\nfig = plt.figure()\nfig.set_figwidth(6)\nfig.set_figheight(6)\nfig.suptitle(\"Adam, Winit=(-10,-1)\")\nax1=fig.add_subplot(2,2,1,projection='3d')\nax2=fig.add_subplot(2,2,2,projection='3d')\nax1.elev=15;ax2.elev=15;ax1.azim=75;ax2.azim=75\nax3=fig.add_subplot(2,2,3)\nax4=fig.add_subplot(2,2,4)\n\nax1.scatter(w0,w1,loss1,s=0.1);ax1.scatter(-1,5,loss_fn1(-1,5),color='red',marker='*',s=200)\nax2.scatter(w0,w1,loss2,s=0.1);ax2.scatter(-1,5,loss_fn2(-1,5),color='red',marker='*',s=200)\n\nax3.plot(x,y,','); ax3.plot(x,v,'--r');\nline3, = ax3.plot(x,1/(1+np.exp(-X@What_mse[:,0])),'--b')\nax4.plot(x,y,','); ax4.plot(x,v,'--r')\nline4, = ax4.plot(x,1/(1+np.exp(-X@What_bce[:,0])),'--b')\n\ndef animate(i):\n    _w0_mse,_w1_mse = What_mse[:,i]\n    _w0_bce,_w1_bce = What_bce[:,i]\n    ax1.scatter(_w0_mse, _w1_mse, loss_fn1(_w0_mse, _w1_mse),color='gray')\n    ax2.scatter(_w0_bce, _w1_bce, loss_fn2(_w0_bce, _w1_bce),color='gray')\n    line3.set_ydata(1/(1+np.exp(-X@What_mse[:,i])))\n    line4.set_ydata(1/(1+np.exp(-X@What_bce[:,i])))\n\nani = animation.FuncAnimation(fig, animate, frames=30)\nplt.close()\nani\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n아무리 아담이라고 해도 이건 힘듬\n\n- discussion\n\nmse_loss는 경우에 따라서 엄청 수렴속도가 느릴수도 있음.\n근본적인 문제점: mse_loss일 경우 loss function의 곡면이 예쁘지 않음. (전문용어로 convex가 아니라고 말함)\n좋은 옵티마지어를 이용하면 mse_loss일 경우에도 수렴속도를 올릴 수 있음 (학습과정 시각화예시2). 그렇지만 이는 근본적인 해결책은 아님. (학습과정 시각화예시3)\n\n- 요약: 왜 logistic regression에서 mse loss를 쓰면 안되는가?\n\nmse loss를 사용하면 손실함수가 convex하지 않으니까!\n그리고 bce loss를 사용하면 손실함수가 convex하니까!"
  },
  {
    "objectID": "posts/2022_03_21_(3주차)_3월21일.html",
    "href": "posts/2022_03_21_(3주차)_3월21일.html",
    "title": "[STBDA] 03wk: tensorflow_Variable, 미분",
    "section": "",
    "text": "해당 강의노트는 전북대학교 최규빈교수님 STBDA2022 자료임\n\n\nimports\n\nimport tensorflow as tf\nimport numpy as np\n\n2023-06-16 17:03:31.644114: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n\n\n\ntf.config.experimental.list_physical_devices('GPU')\n\n[]\n\n\n\n\n지난강의 보충\n- max, min, sum, mean\n\na= tf.constant([1.0,2.0,3.0,4.0])\na\n\n&lt;tf.Tensor: shape=(4,), dtype=float32, numpy=array([1., 2., 3., 4.], dtype=float32)&gt;\n\n\n\ntf.reduce_mean(a)\n\n&lt;tf.Tensor: shape=(), dtype=float32, numpy=2.5&gt;\n\n\n\nconcat, stack\n- 예제: (2,3,4,5) stack (2,3,4,5) -&gt; (?,?,?,?,?)\n\na = tf.reshape(tf.constant(range(2*3*4*5)),(2,3,4,5))\nb = -a\n\ncase1 (1,2,3,4,5) stack (1,2,3,4,5) –&gt; (2,2,3,4,5) # axis=0\n\ntf.stack([a,b],axis=0)\n\n&lt;tf.Tensor: shape=(2, 2, 3, 4, 5), dtype=int32, numpy=\narray([[[[[   0,    1,    2,    3,    4],\n          [   5,    6,    7,    8,    9],\n          [  10,   11,   12,   13,   14],\n          [  15,   16,   17,   18,   19]],\n\n         [[  20,   21,   22,   23,   24],\n          [  25,   26,   27,   28,   29],\n          [  30,   31,   32,   33,   34],\n          [  35,   36,   37,   38,   39]],\n\n         [[  40,   41,   42,   43,   44],\n          [  45,   46,   47,   48,   49],\n          [  50,   51,   52,   53,   54],\n          [  55,   56,   57,   58,   59]]],\n\n\n        [[[  60,   61,   62,   63,   64],\n          [  65,   66,   67,   68,   69],\n          [  70,   71,   72,   73,   74],\n          [  75,   76,   77,   78,   79]],\n\n         [[  80,   81,   82,   83,   84],\n          [  85,   86,   87,   88,   89],\n          [  90,   91,   92,   93,   94],\n          [  95,   96,   97,   98,   99]],\n\n         [[ 100,  101,  102,  103,  104],\n          [ 105,  106,  107,  108,  109],\n          [ 110,  111,  112,  113,  114],\n          [ 115,  116,  117,  118,  119]]]],\n\n\n\n       [[[[   0,   -1,   -2,   -3,   -4],\n          [  -5,   -6,   -7,   -8,   -9],\n          [ -10,  -11,  -12,  -13,  -14],\n          [ -15,  -16,  -17,  -18,  -19]],\n\n         [[ -20,  -21,  -22,  -23,  -24],\n          [ -25,  -26,  -27,  -28,  -29],\n          [ -30,  -31,  -32,  -33,  -34],\n          [ -35,  -36,  -37,  -38,  -39]],\n\n         [[ -40,  -41,  -42,  -43,  -44],\n          [ -45,  -46,  -47,  -48,  -49],\n          [ -50,  -51,  -52,  -53,  -54],\n          [ -55,  -56,  -57,  -58,  -59]]],\n\n\n        [[[ -60,  -61,  -62,  -63,  -64],\n          [ -65,  -66,  -67,  -68,  -69],\n          [ -70,  -71,  -72,  -73,  -74],\n          [ -75,  -76,  -77,  -78,  -79]],\n\n         [[ -80,  -81,  -82,  -83,  -84],\n          [ -85,  -86,  -87,  -88,  -89],\n          [ -90,  -91,  -92,  -93,  -94],\n          [ -95,  -96,  -97,  -98,  -99]],\n\n         [[-100, -101, -102, -103, -104],\n          [-105, -106, -107, -108, -109],\n          [-110, -111, -112, -113, -114],\n          [-115, -116, -117, -118, -119]]]]], dtype=int32)&gt;\n\n\ncase2 (2,1,3,4,5) stack (2,1,3,4,5) –&gt; (2,2,3,4,5) # axis=1\n\ntf.stack([a,b],axis=1)\n\n&lt;tf.Tensor: shape=(2, 2, 3, 4, 5), dtype=int32, numpy=\narray([[[[[   0,    1,    2,    3,    4],\n          [   5,    6,    7,    8,    9],\n          [  10,   11,   12,   13,   14],\n          [  15,   16,   17,   18,   19]],\n\n         [[  20,   21,   22,   23,   24],\n          [  25,   26,   27,   28,   29],\n          [  30,   31,   32,   33,   34],\n          [  35,   36,   37,   38,   39]],\n\n         [[  40,   41,   42,   43,   44],\n          [  45,   46,   47,   48,   49],\n          [  50,   51,   52,   53,   54],\n          [  55,   56,   57,   58,   59]]],\n\n\n        [[[   0,   -1,   -2,   -3,   -4],\n          [  -5,   -6,   -7,   -8,   -9],\n          [ -10,  -11,  -12,  -13,  -14],\n          [ -15,  -16,  -17,  -18,  -19]],\n\n         [[ -20,  -21,  -22,  -23,  -24],\n          [ -25,  -26,  -27,  -28,  -29],\n          [ -30,  -31,  -32,  -33,  -34],\n          [ -35,  -36,  -37,  -38,  -39]],\n\n         [[ -40,  -41,  -42,  -43,  -44],\n          [ -45,  -46,  -47,  -48,  -49],\n          [ -50,  -51,  -52,  -53,  -54],\n          [ -55,  -56,  -57,  -58,  -59]]]],\n\n\n\n       [[[[  60,   61,   62,   63,   64],\n          [  65,   66,   67,   68,   69],\n          [  70,   71,   72,   73,   74],\n          [  75,   76,   77,   78,   79]],\n\n         [[  80,   81,   82,   83,   84],\n          [  85,   86,   87,   88,   89],\n          [  90,   91,   92,   93,   94],\n          [  95,   96,   97,   98,   99]],\n\n         [[ 100,  101,  102,  103,  104],\n          [ 105,  106,  107,  108,  109],\n          [ 110,  111,  112,  113,  114],\n          [ 115,  116,  117,  118,  119]]],\n\n\n        [[[ -60,  -61,  -62,  -63,  -64],\n          [ -65,  -66,  -67,  -68,  -69],\n          [ -70,  -71,  -72,  -73,  -74],\n          [ -75,  -76,  -77,  -78,  -79]],\n\n         [[ -80,  -81,  -82,  -83,  -84],\n          [ -85,  -86,  -87,  -88,  -89],\n          [ -90,  -91,  -92,  -93,  -94],\n          [ -95,  -96,  -97,  -98,  -99]],\n\n         [[-100, -101, -102, -103, -104],\n          [-105, -106, -107, -108, -109],\n          [-110, -111, -112, -113, -114],\n          [-115, -116, -117, -118, -119]]]]], dtype=int32)&gt;\n\n\ncase3 (2,3,1,4,5) stack (2,3,1,4,5) –&gt; (2,3,2,4,5) # axis=2\n\ntf.stack([a,b],axis=2)\n\n&lt;tf.Tensor: shape=(2, 3, 2, 4, 5), dtype=int32, numpy=\narray([[[[[   0,    1,    2,    3,    4],\n          [   5,    6,    7,    8,    9],\n          [  10,   11,   12,   13,   14],\n          [  15,   16,   17,   18,   19]],\n\n         [[   0,   -1,   -2,   -3,   -4],\n          [  -5,   -6,   -7,   -8,   -9],\n          [ -10,  -11,  -12,  -13,  -14],\n          [ -15,  -16,  -17,  -18,  -19]]],\n\n\n        [[[  20,   21,   22,   23,   24],\n          [  25,   26,   27,   28,   29],\n          [  30,   31,   32,   33,   34],\n          [  35,   36,   37,   38,   39]],\n\n         [[ -20,  -21,  -22,  -23,  -24],\n          [ -25,  -26,  -27,  -28,  -29],\n          [ -30,  -31,  -32,  -33,  -34],\n          [ -35,  -36,  -37,  -38,  -39]]],\n\n\n        [[[  40,   41,   42,   43,   44],\n          [  45,   46,   47,   48,   49],\n          [  50,   51,   52,   53,   54],\n          [  55,   56,   57,   58,   59]],\n\n         [[ -40,  -41,  -42,  -43,  -44],\n          [ -45,  -46,  -47,  -48,  -49],\n          [ -50,  -51,  -52,  -53,  -54],\n          [ -55,  -56,  -57,  -58,  -59]]]],\n\n\n\n       [[[[  60,   61,   62,   63,   64],\n          [  65,   66,   67,   68,   69],\n          [  70,   71,   72,   73,   74],\n          [  75,   76,   77,   78,   79]],\n\n         [[ -60,  -61,  -62,  -63,  -64],\n          [ -65,  -66,  -67,  -68,  -69],\n          [ -70,  -71,  -72,  -73,  -74],\n          [ -75,  -76,  -77,  -78,  -79]]],\n\n\n        [[[  80,   81,   82,   83,   84],\n          [  85,   86,   87,   88,   89],\n          [  90,   91,   92,   93,   94],\n          [  95,   96,   97,   98,   99]],\n\n         [[ -80,  -81,  -82,  -83,  -84],\n          [ -85,  -86,  -87,  -88,  -89],\n          [ -90,  -91,  -92,  -93,  -94],\n          [ -95,  -96,  -97,  -98,  -99]]],\n\n\n        [[[ 100,  101,  102,  103,  104],\n          [ 105,  106,  107,  108,  109],\n          [ 110,  111,  112,  113,  114],\n          [ 115,  116,  117,  118,  119]],\n\n         [[-100, -101, -102, -103, -104],\n          [-105, -106, -107, -108, -109],\n          [-110, -111, -112, -113, -114],\n          [-115, -116, -117, -118, -119]]]]], dtype=int32)&gt;\n\n\ncase4 (2,3,4,1,5) stack (2,3,4,1,5) –&gt; (2,3,4,2,5) # axis=3\n\ntf.stack([a,b],axis=-2)\n\n&lt;tf.Tensor: shape=(2, 3, 4, 2, 5), dtype=int32, numpy=\narray([[[[[   0,    1,    2,    3,    4],\n          [   0,   -1,   -2,   -3,   -4]],\n\n         [[   5,    6,    7,    8,    9],\n          [  -5,   -6,   -7,   -8,   -9]],\n\n         [[  10,   11,   12,   13,   14],\n          [ -10,  -11,  -12,  -13,  -14]],\n\n         [[  15,   16,   17,   18,   19],\n          [ -15,  -16,  -17,  -18,  -19]]],\n\n\n        [[[  20,   21,   22,   23,   24],\n          [ -20,  -21,  -22,  -23,  -24]],\n\n         [[  25,   26,   27,   28,   29],\n          [ -25,  -26,  -27,  -28,  -29]],\n\n         [[  30,   31,   32,   33,   34],\n          [ -30,  -31,  -32,  -33,  -34]],\n\n         [[  35,   36,   37,   38,   39],\n          [ -35,  -36,  -37,  -38,  -39]]],\n\n\n        [[[  40,   41,   42,   43,   44],\n          [ -40,  -41,  -42,  -43,  -44]],\n\n         [[  45,   46,   47,   48,   49],\n          [ -45,  -46,  -47,  -48,  -49]],\n\n         [[  50,   51,   52,   53,   54],\n          [ -50,  -51,  -52,  -53,  -54]],\n\n         [[  55,   56,   57,   58,   59],\n          [ -55,  -56,  -57,  -58,  -59]]]],\n\n\n\n       [[[[  60,   61,   62,   63,   64],\n          [ -60,  -61,  -62,  -63,  -64]],\n\n         [[  65,   66,   67,   68,   69],\n          [ -65,  -66,  -67,  -68,  -69]],\n\n         [[  70,   71,   72,   73,   74],\n          [ -70,  -71,  -72,  -73,  -74]],\n\n         [[  75,   76,   77,   78,   79],\n          [ -75,  -76,  -77,  -78,  -79]]],\n\n\n        [[[  80,   81,   82,   83,   84],\n          [ -80,  -81,  -82,  -83,  -84]],\n\n         [[  85,   86,   87,   88,   89],\n          [ -85,  -86,  -87,  -88,  -89]],\n\n         [[  90,   91,   92,   93,   94],\n          [ -90,  -91,  -92,  -93,  -94]],\n\n         [[  95,   96,   97,   98,   99],\n          [ -95,  -96,  -97,  -98,  -99]]],\n\n\n        [[[ 100,  101,  102,  103,  104],\n          [-100, -101, -102, -103, -104]],\n\n         [[ 105,  106,  107,  108,  109],\n          [-105, -106, -107, -108, -109]],\n\n         [[ 110,  111,  112,  113,  114],\n          [-110, -111, -112, -113, -114]],\n\n         [[ 115,  116,  117,  118,  119],\n          [-115, -116, -117, -118, -119]]]]], dtype=int32)&gt;\n\n\ncase5 (2,3,4,5,1) stack (2,3,4,5,1) –&gt; (2,3,4,5,2) # axis=4\n\ntf.stack([a,b],axis=-1)\n\n&lt;tf.Tensor: shape=(2, 3, 4, 5, 2), dtype=int32, numpy=\narray([[[[[   0,    0],\n          [   1,   -1],\n          [   2,   -2],\n          [   3,   -3],\n          [   4,   -4]],\n\n         [[   5,   -5],\n          [   6,   -6],\n          [   7,   -7],\n          [   8,   -8],\n          [   9,   -9]],\n\n         [[  10,  -10],\n          [  11,  -11],\n          [  12,  -12],\n          [  13,  -13],\n          [  14,  -14]],\n\n         [[  15,  -15],\n          [  16,  -16],\n          [  17,  -17],\n          [  18,  -18],\n          [  19,  -19]]],\n\n\n        [[[  20,  -20],\n          [  21,  -21],\n          [  22,  -22],\n          [  23,  -23],\n          [  24,  -24]],\n\n         [[  25,  -25],\n          [  26,  -26],\n          [  27,  -27],\n          [  28,  -28],\n          [  29,  -29]],\n\n         [[  30,  -30],\n          [  31,  -31],\n          [  32,  -32],\n          [  33,  -33],\n          [  34,  -34]],\n\n         [[  35,  -35],\n          [  36,  -36],\n          [  37,  -37],\n          [  38,  -38],\n          [  39,  -39]]],\n\n\n        [[[  40,  -40],\n          [  41,  -41],\n          [  42,  -42],\n          [  43,  -43],\n          [  44,  -44]],\n\n         [[  45,  -45],\n          [  46,  -46],\n          [  47,  -47],\n          [  48,  -48],\n          [  49,  -49]],\n\n         [[  50,  -50],\n          [  51,  -51],\n          [  52,  -52],\n          [  53,  -53],\n          [  54,  -54]],\n\n         [[  55,  -55],\n          [  56,  -56],\n          [  57,  -57],\n          [  58,  -58],\n          [  59,  -59]]]],\n\n\n\n       [[[[  60,  -60],\n          [  61,  -61],\n          [  62,  -62],\n          [  63,  -63],\n          [  64,  -64]],\n\n         [[  65,  -65],\n          [  66,  -66],\n          [  67,  -67],\n          [  68,  -68],\n          [  69,  -69]],\n\n         [[  70,  -70],\n          [  71,  -71],\n          [  72,  -72],\n          [  73,  -73],\n          [  74,  -74]],\n\n         [[  75,  -75],\n          [  76,  -76],\n          [  77,  -77],\n          [  78,  -78],\n          [  79,  -79]]],\n\n\n        [[[  80,  -80],\n          [  81,  -81],\n          [  82,  -82],\n          [  83,  -83],\n          [  84,  -84]],\n\n         [[  85,  -85],\n          [  86,  -86],\n          [  87,  -87],\n          [  88,  -88],\n          [  89,  -89]],\n\n         [[  90,  -90],\n          [  91,  -91],\n          [  92,  -92],\n          [  93,  -93],\n          [  94,  -94]],\n\n         [[  95,  -95],\n          [  96,  -96],\n          [  97,  -97],\n          [  98,  -98],\n          [  99,  -99]]],\n\n\n        [[[ 100, -100],\n          [ 101, -101],\n          [ 102, -102],\n          [ 103, -103],\n          [ 104, -104]],\n\n         [[ 105, -105],\n          [ 106, -106],\n          [ 107, -107],\n          [ 108, -108],\n          [ 109, -109]],\n\n         [[ 110, -110],\n          [ 111, -111],\n          [ 112, -112],\n          [ 113, -113],\n          [ 114, -114]],\n\n         [[ 115, -115],\n          [ 116, -116],\n          [ 117, -117],\n          [ 118, -118],\n          [ 119, -119]]]]], dtype=int32)&gt;\n\n\n- 예제: (2,3,4), (2,3,4), (2,3,4)\n\na= tf.reshape(tf.constant(range(2*3*4)),(2,3,4))\nb= -a\nc= 2*a\n\n(예시1) (2,3,4), (2,3,4), (2,3,4) \\(\\to\\) (6,3,4)\n\ntf.concat([a,b,c],axis=0)\n\n&lt;tf.Tensor: shape=(6, 3, 4), dtype=int32, numpy=\narray([[[  0,   1,   2,   3],\n        [  4,   5,   6,   7],\n        [  8,   9,  10,  11]],\n\n       [[ 12,  13,  14,  15],\n        [ 16,  17,  18,  19],\n        [ 20,  21,  22,  23]],\n\n       [[  0,  -1,  -2,  -3],\n        [ -4,  -5,  -6,  -7],\n        [ -8,  -9, -10, -11]],\n\n       [[-12, -13, -14, -15],\n        [-16, -17, -18, -19],\n        [-20, -21, -22, -23]],\n\n       [[  0,   2,   4,   6],\n        [  8,  10,  12,  14],\n        [ 16,  18,  20,  22]],\n\n       [[ 24,  26,  28,  30],\n        [ 32,  34,  36,  38],\n        [ 40,  42,  44,  46]]], dtype=int32)&gt;\n\n\n(예시2) (2,3,4), (2,3,4), (2,3,4) \\(\\to\\) (2,9,4)\n\ntf.concat([a,b,c],axis=1)\n\n&lt;tf.Tensor: shape=(2, 9, 4), dtype=int32, numpy=\narray([[[  0,   1,   2,   3],\n        [  4,   5,   6,   7],\n        [  8,   9,  10,  11],\n        [  0,  -1,  -2,  -3],\n        [ -4,  -5,  -6,  -7],\n        [ -8,  -9, -10, -11],\n        [  0,   2,   4,   6],\n        [  8,  10,  12,  14],\n        [ 16,  18,  20,  22]],\n\n       [[ 12,  13,  14,  15],\n        [ 16,  17,  18,  19],\n        [ 20,  21,  22,  23],\n        [-12, -13, -14, -15],\n        [-16, -17, -18, -19],\n        [-20, -21, -22, -23],\n        [ 24,  26,  28,  30],\n        [ 32,  34,  36,  38],\n        [ 40,  42,  44,  46]]], dtype=int32)&gt;\n\n\n(예시3) (2,3,4), (2,3,4), (2,3,4) \\(\\to\\) (2,3,12)\n\ntf.concat([a,b,c],axis=-1)\n\n&lt;tf.Tensor: shape=(2, 3, 12), dtype=int32, numpy=\narray([[[  0,   1,   2,   3,   0,  -1,  -2,  -3,   0,   2,   4,   6],\n        [  4,   5,   6,   7,  -4,  -5,  -6,  -7,   8,  10,  12,  14],\n        [  8,   9,  10,  11,  -8,  -9, -10, -11,  16,  18,  20,  22]],\n\n       [[ 12,  13,  14,  15, -12, -13, -14, -15,  24,  26,  28,  30],\n        [ 16,  17,  18,  19, -16, -17, -18, -19,  32,  34,  36,  38],\n        [ 20,  21,  22,  23, -20, -21, -22, -23,  40,  42,  44,  46]]],\n      dtype=int32)&gt;\n\n\n(예시4) (2,3,4), (2,3,4), (2,3,4) \\(\\to\\) (3,2,3,4)\n\ntf.stack([a,b,c],axis=0)\n\n&lt;tf.Tensor: shape=(3, 2, 3, 4), dtype=int32, numpy=\narray([[[[  0,   1,   2,   3],\n         [  4,   5,   6,   7],\n         [  8,   9,  10,  11]],\n\n        [[ 12,  13,  14,  15],\n         [ 16,  17,  18,  19],\n         [ 20,  21,  22,  23]]],\n\n\n       [[[  0,  -1,  -2,  -3],\n         [ -4,  -5,  -6,  -7],\n         [ -8,  -9, -10, -11]],\n\n        [[-12, -13, -14, -15],\n         [-16, -17, -18, -19],\n         [-20, -21, -22, -23]]],\n\n\n       [[[  0,   2,   4,   6],\n         [  8,  10,  12,  14],\n         [ 16,  18,  20,  22]],\n\n        [[ 24,  26,  28,  30],\n         [ 32,  34,  36,  38],\n         [ 40,  42,  44,  46]]]], dtype=int32)&gt;\n\n\n(예시5) (2,3,4), (2,3,4), (2,3,4) \\(\\to\\) (2,3,3,4)\n\ntf.stack([a,b,c],axis=1)\n\n&lt;tf.Tensor: shape=(2, 3, 3, 4), dtype=int32, numpy=\narray([[[[  0,   1,   2,   3],\n         [  4,   5,   6,   7],\n         [  8,   9,  10,  11]],\n\n        [[  0,  -1,  -2,  -3],\n         [ -4,  -5,  -6,  -7],\n         [ -8,  -9, -10, -11]],\n\n        [[  0,   2,   4,   6],\n         [  8,  10,  12,  14],\n         [ 16,  18,  20,  22]]],\n\n\n       [[[ 12,  13,  14,  15],\n         [ 16,  17,  18,  19],\n         [ 20,  21,  22,  23]],\n\n        [[-12, -13, -14, -15],\n         [-16, -17, -18, -19],\n         [-20, -21, -22, -23]],\n\n        [[ 24,  26,  28,  30],\n         [ 32,  34,  36,  38],\n         [ 40,  42,  44,  46]]]], dtype=int32)&gt;\n\n\n(예시6) (2,3,4), (2,3,4), (2,3,4) \\(\\to\\) (2,3,3,4)\n\ntf.stack([a,b,c],axis=2)\n\n&lt;tf.Tensor: shape=(2, 3, 3, 4), dtype=int32, numpy=\narray([[[[  0,   1,   2,   3],\n         [  0,  -1,  -2,  -3],\n         [  0,   2,   4,   6]],\n\n        [[  4,   5,   6,   7],\n         [ -4,  -5,  -6,  -7],\n         [  8,  10,  12,  14]],\n\n        [[  8,   9,  10,  11],\n         [ -8,  -9, -10, -11],\n         [ 16,  18,  20,  22]]],\n\n\n       [[[ 12,  13,  14,  15],\n         [-12, -13, -14, -15],\n         [ 24,  26,  28,  30]],\n\n        [[ 16,  17,  18,  19],\n         [-16, -17, -18, -19],\n         [ 32,  34,  36,  38]],\n\n        [[ 20,  21,  22,  23],\n         [-20, -21, -22, -23],\n         [ 40,  42,  44,  46]]]], dtype=int32)&gt;\n\n\n(예시7) (2,3,4), (2,3,4), (2,3,4) \\(\\to\\) (2,3,4,3)\n\ntf.stack([a,b,c],axis=-1)\n\n&lt;tf.Tensor: shape=(2, 3, 4, 3), dtype=int32, numpy=\narray([[[[  0,   0,   0],\n         [  1,  -1,   2],\n         [  2,  -2,   4],\n         [  3,  -3,   6]],\n\n        [[  4,  -4,   8],\n         [  5,  -5,  10],\n         [  6,  -6,  12],\n         [  7,  -7,  14]],\n\n        [[  8,  -8,  16],\n         [  9,  -9,  18],\n         [ 10, -10,  20],\n         [ 11, -11,  22]]],\n\n\n       [[[ 12, -12,  24],\n         [ 13, -13,  26],\n         [ 14, -14,  28],\n         [ 15, -15,  30]],\n\n        [[ 16, -16,  32],\n         [ 17, -17,  34],\n         [ 18, -18,  36],\n         [ 19, -19,  38]],\n\n        [[ 20, -20,  40],\n         [ 21, -21,  42],\n         [ 22, -22,  44],\n         [ 23, -23,  46]]]], dtype=int32)&gt;\n\n\n- 예제: (2,3,4) (4,3,4) \\(\\to\\) (6,3,4)\n\na=tf.reshape(tf.constant(range(2*3*4)),(2,3,4))\nb=tf.reshape(-tf.constant(range(4*3*4)),(4,3,4))\n\n\ntf.concat([a,b],axis=0)\n\n&lt;tf.Tensor: shape=(6, 3, 4), dtype=int32, numpy=\narray([[[  0,   1,   2,   3],\n        [  4,   5,   6,   7],\n        [  8,   9,  10,  11]],\n\n       [[ 12,  13,  14,  15],\n        [ 16,  17,  18,  19],\n        [ 20,  21,  22,  23]],\n\n       [[  0,  -1,  -2,  -3],\n        [ -4,  -5,  -6,  -7],\n        [ -8,  -9, -10, -11]],\n\n       [[-12, -13, -14, -15],\n        [-16, -17, -18, -19],\n        [-20, -21, -22, -23]],\n\n       [[-24, -25, -26, -27],\n        [-28, -29, -30, -31],\n        [-32, -33, -34, -35]],\n\n       [[-36, -37, -38, -39],\n        [-40, -41, -42, -43],\n        [-44, -45, -46, -47]]], dtype=int32)&gt;\n\n\n\ntf.concat([a,b],axis=1)\n\nInvalidArgumentError: {{function_node __wrapped__ConcatV2_N_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} ConcatOp : Dimension 0 in both shapes must be equal: shape[0] = [2,3,4] vs. shape[1] = [4,3,4] [Op:ConcatV2] name: concat\n\n\n\ndimension이 달라서 안된다.\n\n\ntf.concat([a,b],axis=2)\n\nInvalidArgumentError: {{function_node __wrapped__ConcatV2_N_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} ConcatOp : Dimension 0 in both shapes must be equal: shape[0] = [2,3,4] vs. shape[1] = [4,3,4] [Op:ConcatV2] name: concat\n\n\n- (2,2) @ (2,) 의 연산?\nnumpy\n\nnp.array([77,-88])\n\narray([ 77, -88])\n\n\n\n길이가 2인 벡터\n\n\nnp.array([[1,0],[0,1]]) @ np.array([77,-88])\n\narray([ 77, -88])\n\n\n\n오 하지만 계산이 된다!?\n\n\nnp.array([77,-88]) @ np.array([[1,0],[0,1]])\n\narray([ 77, -88])\n\n\n\nnp.array([[1,0],[0,1]]) @ np.array([77,-88]).reshape(2,1)\n\narray([[ 77],\n       [-88]])\n\n\n\nnp.array([77,-88]).reshape(2,1) @ np.array([[1,0],[0,1]])\n\nValueError: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)-&gt;(n?,m?) (size 2 is different from 1)\n\n\n\n(2x1) @ (2x2) 가 되서 dimension이 달라서 안됨\n\n\nnp.array([77,-88]).reshape(1,2) @ np.array([[1,0],[0,1]])\n\narray([[ 77, -88]])\n\n\ntensorflow\n\nI = tf.constant([[1.0,0.0],[0.0,1.0]])\nx = tf.constant([77.0,-88.0])\n\n\nI @ x\n\nInvalidArgumentError: {{function_node __wrapped__MatMul_device_/job:localhost/replica:0/task:0/device:CPU:0}} In[0] and In[1] has different ndims: [2,2] vs. [2] [Op:MatMul]\n\n\n\n(2x2) 랑 길이가2인 벡터의 행렬곲 안됨.\n\n\nx @ I\n\nInvalidArgumentError: {{function_node __wrapped__MatMul_device_/job:localhost/replica:0/task:0/device:CPU:0}} In[0] and In[1] has different ndims: [2] vs. [2,2] [Op:MatMul]\n\n\n\nI @ tf.reshape(x,(2,1))\n\n&lt;tf.Tensor: shape=(2, 1), dtype=float32, numpy=\narray([[ 77.],\n       [-88.]], dtype=float32)&gt;\n\n\n\ntf.reshape(x,(1,2)) @ I\n\n&lt;tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[ 77., -88.]], dtype=float32)&gt;\n\n\n\n\n\n\ntf.Variable\n\n선언\n- tf.Variable()로 선언\n\ntf.Variable([1,2,3,4])\n\n&lt;tf.Variable 'Variable:0' shape=(4,) dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)&gt;\n\n\n\ntf.Variable([1.0,2.0,3.0,4.0])\n\n&lt;tf.Variable 'Variable:0' shape=(4,) dtype=float32, numpy=array([1., 2., 3., 4.], dtype=float32)&gt;\n\n\n- tf.constant() 선언후 변환\n\ntf.Variable(tf.constant([1,2,3,4]))\n\n&lt;tf.Variable 'Variable:0' shape=(4,) dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)&gt;\n\n\n- np 등으로 선언후 변환\n\ntf.Variable(np.array([1,2,3,4]))\n\n&lt;tf.Variable 'Variable:0' shape=(4,) dtype=int64, numpy=array([1, 2, 3, 4])&gt;\n\n\n\n\n타입\n\ntype(tf.Variable([1,2,3,4]))\n\ntensorflow.python.ops.resource_variable_ops.ResourceVariable\n\n\n\n\n인덱싱\n\na=tf.Variable([1,2,3,4])\na\n\n&lt;tf.Variable 'Variable:0' shape=(4,) dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)&gt;\n\n\n\na[:2]\n\n&lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 2], dtype=int32)&gt;\n\n\n\n\n연산가능\n\na=tf.Variable([1,2,3,4])\nb=tf.Variable([-1,-2,-3,-4])\n\n\ntype(a)\n\ntensorflow.python.ops.resource_variable_ops.ResourceVariable\n\n\n\ntype(b)\n\ntensorflow.python.ops.resource_variable_ops.ResourceVariable\n\n\n\na+b\n\n&lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([0, 0, 0, 0], dtype=int32)&gt;\n\n\n\ntype(a+b)\n\ntensorflow.python.framework.ops.EagerTensor\n\n\n\nb = -a\n\n\ntype(b)\n\ntensorflow.python.framework.ops.EagerTensor\n\n\n\n\ntf.Variable도 쓰기 불편함\n\ntf.Variable([1,2])+tf.Variable([3.14,3.14])\n\nInvalidArgumentError: cannot compute AddV2 as input #1(zero-based) was expected to be a int32 tensor but is a float tensor [Op:AddV2]\n\n\n\n\ntnp의 은총도 일부만 가능\n\nimport tensorflow.experimental.numpy as tnp\ntnp.experimental_enable_numpy_behavior()\n\n- 알아서 형 변환\n\ntf.Variable([1,2])+tf.Variable([3.14,3.14])\n\n&lt;tf.Tensor: shape=(2,), dtype=float64, numpy=array([4.1400001, 5.1400001])&gt;\n\n\n- .reshape 메소드\n\ntf.Variable([1,2,3,4]).reshape(2,2)\n\nAttributeError: 'ResourceVariable' object has no attribute 'reshape'\n\n\n\n\n대부분의 동작은 tf.constant랑 큰 차이를 모르겠음\n- tf.concat\n\na= tf.Variable([[1,2],[3,4]])\nb= tf.Variable([[-1,-2],[-3,-4]])\ntf.concat([a,b],axis=0)\n\n&lt;tf.Tensor: shape=(4, 2), dtype=int32, numpy=\narray([[ 1,  2],\n       [ 3,  4],\n       [-1, -2],\n       [-3, -4]], dtype=int32)&gt;\n\n\n- tf.stack\n\na= tf.Variable([[1,2],[3,4]])\nb= tf.Variable([[-1,-2],[-3,-4]])\ntf.stack([a,b],axis=0)\n\n&lt;tf.Tensor: shape=(2, 2, 2), dtype=int32, numpy=\narray([[[ 1,  2],\n        [ 3,  4]],\n\n       [[-1, -2],\n        [-3, -4]]], dtype=int32)&gt;\n\n\n\n\n변수값변경가능(?)\n\na= tf.Variable([1,2,3,4])\nid(a)\n\n139819184398048\n\n\n\na\n\n&lt;tf.Variable 'Variable:0' shape=(4,) dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)&gt;\n\n\n\na.assign_add([-1,-2,-3,-4])\nid(a)\n\n139819184398048\n\n\n\na\n\n&lt;tf.Variable 'Variable:0' shape=(4,) dtype=int32, numpy=array([0, 0, 0, 0], dtype=int32)&gt;\n\n\n\n\n요약\n- tf.Variable()로 만들어야 하는 뚜렷한 차이는 모르겠음.\n- 애써 tf.Variable()로 만들어도 간단한연산을 하면 그 결과는 tf.constant()로 만든 오브젝트와 동일해짐.\n\n\n\n미분\n\n모티브\n- 예제: 컴퓨터를 이용하여 \\(x=2\\)에서 \\(y=3x^2\\)의 접선의 기울기를 구해보자.\n(손풀이)\n\\[\\frac{dy}{dx}=6x\\]\n이므로 \\(x=2\\)를 대입하면 12이다.\n(컴퓨터를 이용한 풀이)\n단계1\n\nx1=2\ny1= 3*x1**2\n\n\nx2=2+0.000000001\ny2= 3*x2**2\n\n\n(y2-y1)/(x2-x1)\n\n12.0\n\n\n단계2\n\ndef f(x):\n    return(3*x**2)\n\n\nf(3)\n\n27\n\n\n\ndef d(f,x):\n    return (f(x+0.000000001)-f(x))/0.000000001\n\n\nd(f,2)\n\n12.000000992884452\n\n\n단계3\n\nd(lambda x: 3*x**2 ,2)\n\n12.000000992884452\n\n\n\nd(lambda x: x**2 ,0)\n\n1e-09\n\n\n단계4\n\\[f(x,y)= x^2 +3y\\]\n\ndef f(x,y):\n    return(x**2 +3*y)\n\n\nd(f,(2,3))\n\nTypeError: can only concatenate tuple (not \"float\") to tuple\n\n\ny에대한 미분 정의를 안해줬으. 오류.\n\n\ntf.GradientTape() 사용방법\n- 예제1: \\(x=2\\)에서 \\(y=3x^2\\)의 도함수값을 구하라.\n\nx=tf.Variable(2.0) #\na=tf.constant(3.0)\n\n\n미분할 것을 Varialbe로\n\n\nmytape=tf.GradientTape()\nmytape\n\n&lt;tensorflow.python.eager.backprop.GradientTape at 0x7f2a30cc9820&gt;\n\n\n\n실행 결과가 0x7f2a30cc9820 이 오브젝트주소 안에\n\n\ndir(mytape)\n\n['__class__',\n '__delattr__',\n '__dict__',\n '__dir__',\n '__doc__',\n '__enter__',\n '__eq__',\n '__exit__',\n '__format__',\n '__ge__',\n '__getattribute__',\n '__gt__',\n '__hash__',\n '__init__',\n '__init_subclass__',\n '__le__',\n '__lt__',\n '__module__',\n '__ne__',\n '__new__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__setattr__',\n '__sizeof__',\n '__str__',\n '__subclasshook__',\n '__weakref__',\n '_ensure_recording',\n '_persistent',\n '_pop_tape',\n '_push_tape',\n '_recording',\n '_tape',\n '_tf_api_names',\n '_tf_api_names_v1',\n '_watch_accessed_variables',\n '_watched_variables',\n 'batch_jacobian',\n 'gradient',\n 'jacobian',\n 'reset',\n 'stop_recording',\n 'watch',\n 'watched_variables']\n\n\n\nmytape.__enter__() # 기록 시작\ny=a*x**2 # y=ax^2 = 3x^2\nmytape.__exit__(None,None,None) # 기록 끝\n\n\nmytape.gradient(y,x) # y를 x로 미분하라.\n\n&lt;tf.Tensor: shape=(), dtype=float32, numpy=12.0&gt;\n\n\n- 예제2: 조금 다른예제\n\nx=tf.Variable(2.0)\n#a=tf.constant(3.0)\n\nmytape=tf.GradientTape()\nmytape.__enter__() # 기록 시작\na=(x/2)*3 ## a=(3/2)x\ny=a*x**2  ## y=ax^2 = (3/2)x^3\nmytape.__exit__(None,None,None) # 기록 끝\n\nmytape.gradient(y,x) # y를 x로 미분하라.\n\n&lt;tf.Tensor: shape=(), dtype=float32, numpy=18.0&gt;\n\n\n\n왜 12가 안나오고 18이 나올까? 아래 식을 살펴보자.\n\n\\[a=\\frac{3}{2}x\\] \\[y=ax^2=\\frac{3}{2}x^3\\]\n\\[\\frac{dy}{dx}=\\frac{3}{2} 3x^2\\]\n\n3/2*3*4\n\n18.0\n\n\n- 테이프의 개념 (\\(\\star\\))\n(상황)\n우리가 어려운 미분계산을 컴퓨터에게 부탁하는 상황임. (예를들면 \\(y=3x^2\\)) 컴퓨터에게 부탁을 하기 위해서는 연습장(=테이프)에 \\(y=3x^2\\)이라는 수식을 써서 보여줘야하는데 이때 컴퓨터에게 target이 무엇인지 그리고 무엇으로 미분하고 싶은 것인지를 명시해야함.\n\nmytape = tf.GradientTape(): tf.GradientTape()는 연습장을 만드는 명령어, 만들어진 연습장을 mytape라고 이름을 붙인다.\nmytape.__enter__(): 만들어진 공책을 연다 (=기록할수 있는 상태로 만든다)\na=x/2*3; y=a*x**2: 컴퓨터에게 전달할 수식을 쓴다\nmytape.__exit__(None,None,None): 공책을 닫는다.\nmytape.gradient(y,x): \\(y\\)를 \\(x\\)로 미분하라는 메모를 남기고 컴퓨터에게 전달한다.\n\n- 예제3: 연습장을 언제 열고 닫을지 결정하는건 중요하다.\n\nx=tf.Variable(2.0)\na=(x/2)*3 ## a=(3/2)x\n\nmytape=tf.GradientTape()\nmytape.__enter__() # 기록 시작\ny=a*x**2  ## y=ax^2 = (3/2)x^3\nmytape.__exit__(None,None,None) # 기록 끝\n\nmytape.gradient(y,x) # y를 x로 미분하라.\n\n&lt;tf.Tensor: shape=(), dtype=float32, numpy=12.0&gt;\n\n\n- 예제4: with문과 함께 쓰는 tf.GradientTape()\n\nx=tf.Variable(2.0)\na=(x/2)*3\n\n\nwith tf.GradientTape() as mytape:\n    ## with문 시작\n    y=a*x**2\n    ## with문 끝\n\n\nmytape.gradient(y,x) # y를 x로 미분하라.\n\n&lt;tf.Tensor: shape=(), dtype=float32, numpy=12.0&gt;\n\n\n(문법해설)\n아래와 같이 쓴다.\nwith expression as myname:\n    ## with문 시작: myname.__enter__()\n    blabla ~\n    yadiyadi !!\n    ## with문 끝: myname.__exit__()\n\nexpression 의 실행결과 오브젝트가 생성, 생성된 오브젝트는 myname라고 이름붙임. 이 오브젝트는 .__enter__()와 .__exit__()를 숨겨진 기능으로 포함해야 한다.\nwith문이 시작되면서 myname.__enter__()이 실행된다.\n블라블라와 야디야디가 실행된다.\nwith문이 종료되면서 myname.__exit__()이 실행된다.\n\n- 예제5: 예제2를 with문과 함께 구현\n\nx=tf.Variable(2.0)\n\nwith tf.GradientTape() as mytape:\n    a=(x/2)*3 ## a=(3/2)x\n    y=a*x**2  ## y=ax^2 = (3/2)x^3\n\nmytape.gradient(y,x) # y를 x로 미분하라.\n\n&lt;tf.Tensor: shape=(), dtype=float32, numpy=18.0&gt;\n\n\n- 예제6: persistent = True\n(관찰1)\n\nx=tf.Variable(2.0)\n\nwith tf.GradientTape() as mytape:\n    a=(x/2)*3 ## a=(3/2)x\n    y=a*x**2  ## y=ax^2 = (3/2)x^3\n\n\nmytape.gradient(y,x) # 2번이상 실행해서 에러를 관측하라\n\nRuntimeError: A non-persistent GradientTape can only be used to compute one set of gradients (or jacobians)\n\n\n(관찰2)\n\nx=tf.Variable(2.0)\n\nwith tf.GradientTape(persistent=True) as mytape:\n    a=(x/2)*3 ## a=(3/2)x\n    y=a*x**2  ## y=ax^2 = (3/2)x^3\n\n\nmytape.gradient(y,x) # 2번이상실행해도 에러가 나지않음\n\n&lt;tf.Tensor: shape=(), dtype=float32, numpy=18.0&gt;\n\n\n- 예제7: watch\n(관찰1)\nx를 Variable이 아니라 constant\n\nx=tf.constant(2.0)\n\nwith tf.GradientTape(persistent=True) as mytape:\n    a=(x/2)*3 ## a=(3/2)x\n    y=a*x**2  ## y=ax^2 = (3/2)x^3\n\n\nprint(mytape.gradient(y,x))\n\nNone\n\n\n(관찰2)\n\nx=tf.constant(2.0)\nwith tf.GradientTape(persistent=True) as mytape:\n    mytape.watch(x) # 수동감시\n    a=(x/2)*3 ## a=(3/2)x\n    y=a*x**2  ## y=ax^2 = (3/2)x^3\n\n\nprint(mytape.gradient(y,x))\n\ntf.Tensor(18.0, shape=(), dtype=float32)\n\n\n(관찰3)\n\nx=tf.Variable(2.0)\nwith tf.GradientTape(persistent=True,watch_accessed_variables=False) as mytape: # 자동감시 모드 해제\n    a=(x/2)*3 ## a=(3/2)x\n    y=a*x**2  ## y=ax^2 = (3/2)x^3\n\n\nprint(mytape.gradient(y,x))\n\nNone\n\n\n(관찰4)\n\nx=tf.Variable(2.0)\nwith tf.GradientTape(persistent=True,watch_accessed_variables=False) as mytape: # 자동감시 모드 해제\n    mytape.watch(x)\n    a=(x/2)*3 ## a=(3/2)x\n    y=a*x**2  ## y=ax^2 = (3/2)x^3\n\n\nprint(mytape.gradient(y,x))\n\ntf.Tensor(18.0, shape=(), dtype=float32)\n\n\n(관찰5)\n\nx=tf.Variable(2.0)\nwith tf.GradientTape(persistent=True) as mytape:\n    mytape.watch(x)\n    a=(x/2)*3 ## a=(3/2)x\n    y=a*x**2  ## y=ax^2 = (3/2)x^3\n\n\nprint(mytape.gradient(y,x))\n\ntf.Tensor(18.0, shape=(), dtype=float32)\n\n\n\n자동감시모드를 해제한다고 말한적이 없고, 감시를 하라고 하면 걍..\n\n- 예제9: 카페예제로 돌아오자.\n- 예제10: 카페예제의 매트릭스 버전\n- 예제11: 위의 예제에서 이론적인 \\(\\boldsymbol{\\beta}\\)의 최적값을 찾아보고 (즉 \\(\\hat{\\boldsymbol{\\beta}}\\)을 찾고) 그곳에서 loss의 미분을 구하라. 구한결과가 \\(\\begin{bmatrix}0 \\\\ 0 \\end{bmatrix}\\) 임을 확인하라."
  },
  {
    "objectID": "posts/2022_04_04_(5주차)_4월4일.html",
    "href": "posts/2022_04_04_(5주차)_4월4일.html",
    "title": "[STBDA] 05wk: tensorflow,keras",
    "section": "",
    "text": "해당 강의노트는 전북대학교 최규빈교수님 STBDA2022 자료임\n\n\nimports\n\n# conda install -c conda-forge python-graphviz -y\n\n\nimport tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n2023-06-20 13:43:31.300590: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n\n\n\nimport tensorflow.experimental.numpy as tnp\n\n\ntnp.experimental_enable_numpy_behavior()\n\n\n\n최적화의 문제\n- \\(loss=(\\frac{1}{2}\\beta-1)^2\\)\n- 기존에 했던 방법은 수식을 알고 있어야 한다는 단점이 있음\n\n\ntf.keras.optimizers를 이용한 최적화방법\n\n방법1: opt.apply_gradients()를 이용\n\nalpha= 0.01/6\n\n\nbeta= tf.Variable(-10.0)\n\n\nopt = tf.keras.optimizers.SGD(alpha)\n\n\ntf.keras.optimizers = tp.optimizers 다 똑같은것.\n\n- iter1\n\nwith tf.GradientTape() as tape:\n    tape.watch(beta)\n    loss=(beta/2-1)**2\nslope = tape.gradient(loss,beta)\n\n\nopt.apply_gradients([(slope,beta)]) # beta.assign_sub(slope * alpha)\nbeta\n\n&lt;tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-9.99&gt;\n\n\n- iter2\n\nwith tf.GradientTape() as tape:\n    tape.watch(beta)\n    loss=(beta/2-1)**2\nslope = tape.gradient(loss,beta)\nopt.apply_gradients([(slope,beta)]) # beta.assign_sub(slope * alpha)\nbeta\n\n&lt;tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-9.980008&gt;\n\n\n- for문으로 정리\n\nalpha= 0.01/6\nbeta= tf.Variable(-10.0)\nopt = tf.keras.optimizers.SGD(alpha)\n\n\nfor epoc in range(10000):\n    with tf.GradientTape() as tape:\n        tape.watch(beta)\n        loss=(beta/2-1)**2\n    slope = tape.gradient(loss,beta)\n    opt.apply_gradients([(slope,beta)]) # beta.assign_sub(slope * alpha)\n    beta\n\n\nbeta\n\n&lt;tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.9971251&gt;\n\n\n\nopt.apply_gradients()의 입력은 pair 의 list\n\n\n\n방법2: opt.minimize()\n\nalpha= 0.01/6\nbeta= tf.Variable(-10.0)\nopt = tf.keras.optimizers.SGD(alpha)\n\n\nloss_fn = lambda: (beta/2-1)**2\n\n\nlambda x: x**2 &lt;=&gt; lambda(x)=x^2\nlambda x,y: x+y &lt;=&gt; lambda(x,y)=x+y\nlambda: y &lt;=&gt; lambda()=y, 입력이 없으며 출력은 항상 y인 함수\n\n\nloss_fn() # 입력은 없고 출력은 뭔가 계산되는 함수\n\n&lt;tf.Tensor: shape=(), dtype=float32, numpy=36.0&gt;\n\n\n- iter 1\n\n오류난당..\n\n\nopt.minimize?\n\n\nSignature: opt.minimize(loss, var_list, tape=None)\nDocstring:\nMinimize `loss` by updating `var_list`.\nThis method simply computes gradient using `tf.GradientTape` and calls\n`apply_gradients()`. If you want to process the gradient before applying\nthen call `tf.GradientTape` and `apply_gradients()` explicitly instead\nof using this function.\nArgs:\n  loss: `Tensor` or callable. If a callable, `loss` should take no\n    arguments and return the value to minimize.\n  var_list: list or tuple of `Variable` objects to update to minimize\n    `loss`, or a callable returning the list or tuple of `Variable`\n    objects.  Use callable when the variable list would otherwise be\n    incomplete before `minimize` since the variables are created at the\n    first time `loss` is called.\n  tape: (Optional) `tf.GradientTape`.\nReturns:\n  None\nFile:      ~/anaconda3/envs/py38/lib/python3.8/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\nType:      method\n\n\n\n\nopt.minimize(loss_fn, beta)\n\nTypeError: Cannot iterate over a scalar tensor.\n\n\n\nbeta\n\n&lt;tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-10.0&gt;\n\n\n- iter2\n\nopt.minimize(loss_fn, beta)\nbeta\n\nTypeError: Cannot iterate over a scalar tensor.\n\n\n- for문으로 정리하면\n\nalpha= 0.01/6\nbeta= tf.Variable(-10.0)\nopt = tf.keras.optimizers.SGD(alpha)\nloss_fn = lambda: (beta/2-1)**2\nfor epoc in range(10000):\n    opt.minimize(loss_fn, beta)\nbeta\n\nTypeError: Cannot iterate over a scalar tensor.\n\n\n\n\n\n회귀분석 문제\n- \\({\\bf y} \\approx 2.5 + 4.0 {\\bf x}\\)\n\ntnp.random.seed(43052)\nN = 200\nx = tnp.linspace(0,1,N)\nepsilon = tnp.random.randn(N)*0.5\ny = 2.5+4*x + epsilon\ny_true = 2.5+4*x\n\n\nplt.plot(x,y,'.')\nplt.plot(x,y_true,'r--')\n\n\n\n\n\n\n이론적 풀이\n\n풀이1: 스칼라버전\n- 포인트 - \\(S_{xx}=\\), \\(S_{xy}=\\) - \\(\\hat{\\beta}_0=\\), \\(\\hat{\\beta}_1=\\)\n- 풀이\n\nSxx = sum((x-x.mean())**2)\nSxy = sum((x-x.mean())*(y-y.mean()))\n\n\nbeta1_hat = Sxy/Sxx\nbeta1_hat\n\n&lt;tf.Tensor: shape=(), dtype=float64, numpy=3.9330345167331697&gt;\n\n\n\nbeta0_hat = y.mean() - x.mean()*beta1_hat\nbeta0_hat\n\n&lt;tf.Tensor: shape=(), dtype=float64, numpy=2.583667211565867&gt;\n\n\n\n\n풀이2: 벡터버전\n- 포인트 - \\(\\hat{\\beta}=(X'X)^{-1}X'y\\)\n- 풀이\n\ny=y.reshape(N,1)\nX=tf.stack([tf.ones(N,dtype=tf.float64),x],axis=1)\ny.shape,X.shape\n\n(TensorShape([200, 1]), TensorShape([200, 2]))\n\n\n\ntf.linalg.inv(X.T @ X ) @ X.T @ y\n\n&lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy=\narray([[2.58366721],\n       [3.93303452]])&gt;\n\n\n\n\n풀이3: 벡터버전, 손실함수의 도함수이용\n- 포인트 - \\(loss'(\\beta)=-2X'y +2X'X\\beta\\) - \\(\\beta_{new} = \\beta_{old} - \\alpha \\times loss'(\\beta_{old})\\)\n- 풀이\n\ny=y.reshape(N,1)\ny.shape,X.shape\n\n(TensorShape([200, 1]), TensorShape([200, 2]))\n\n\n\nbeta_hat = tnp.array([-5,10]).reshape(2,1)\nbeta_hat\n\n&lt;tf.Tensor: shape=(2, 1), dtype=int64, numpy=\narray([[-5],\n       [10]])&gt;\n\n\n\nslope = (-2*X.T @ y + 2*X.T @ X @ beta_hat) / N\nslope\n\n&lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy=\narray([[-9.10036894],\n       [-3.52886113]])&gt;\n\n\n\nalpha= 0.1\n\n\nstep = slope*alpha\nstep\n\n&lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy=\narray([[-0.91003689],\n       [-0.35288611]])&gt;\n\n\n\nfor epoc in range(1000):\n    slope = (-2*X.T @ y + 2*X.T @ X @ beta_hat)/N\n    beta_hat = beta_hat - alpha* slope\n\n\nbeta_hat\n\n&lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy=\narray([[2.58366061],\n       [3.93304684]])&gt;\n\n\n\n\n\nGradientTape를 이용\n\n풀이1: 벡터버전\n- 포인트\n## 포인트코드1: 그레디언트 테입\nwith tf.GradientTape() as tape:\n    loss =\n## 포인트코드2: 미분\nslope = tape.gradient(loss,beta_hat)\n## 포인트코드3: update\nbeta_hat.assign_sub(slope*alph)\n- 풀이\n\ny=y.reshape(N,1)\ny.shape,X.shape\n\n(TensorShape([200, 1]), TensorShape([200, 2]))\n\n\n\nbeta_hat = tf.Variable(tnp.array([-5.0,10.0]).reshape(2,1))\nbeta_hat\n\n&lt;tf.Variable 'Variable:0' shape=(2, 1) dtype=float64, numpy=\narray([[-5.],\n       [10.]])&gt;\n\n\n\nalpha=0.1\n\n\nfor epoc in range(1000):\n    with tf.GradientTape() as tape:\n        yhat= X@beta_hat\n        loss= (y-yhat).T @ (y-yhat) / N\n    slope = tape.gradient(loss,beta_hat)\n    beta_hat.assign_sub(alpha*slope)\n\n\nbeta_hat\n\n&lt;tf.Variable 'Variable:0' shape=(2, 1) dtype=float64, numpy=\narray([[2.58366061],\n       [3.93304684]])&gt;\n\n\n\n\n풀이2: 스칼라버전\n- 포인트\n## 포인트코드: 미분\nslope0,slope1 = tape.gradient(loss,[beta0_hat,beta1_hat])\n- 풀이\n\ny=y.reshape(-1)\ny.shape,x.shape\n\n(TensorShape([200]), TensorShape([200]))\n\n\n\nbeta0_hat = tf.Variable(-5.0)\nbeta1_hat = tf.Variable(10.0)\n\n\nalpha=0.1\n\n\nfor epoc in range(1000):\n    with tf.GradientTape() as tape:\n        yhat= beta0_hat + x*beta1_hat\n        loss= tf.reduce_sum((y-yhat)**2)/N #loss= sum((y-yhat)**2)/N (이거로하면 좀 느림)\n    slope0,slope1 = tape.gradient(loss,[beta0_hat,beta1_hat])\n    beta0_hat.assign_sub(alpha*slope0)\n    beta1_hat.assign_sub(alpha*slope1)\n\n\nbeta0_hat,beta1_hat\n\n(&lt;tf.Variable 'Variable:0' shape=() dtype=float32, numpy=2.58366&gt;,\n &lt;tf.Variable 'Variable:0' shape=() dtype=float32, numpy=3.933048&gt;)\n\n\n\n\n\nGradientTape + opt.apply_gradients\n\n풀이1: 벡터버전\n- 포인트\n## 포인트코드: 업데이트\nopt.apply_gradients([(slope,beta_hat)])  ## pair의 list가 입력\n- 풀이\n\ny=y.reshape(N,1)\ny.shape,X.shape\n\n(TensorShape([200, 1]), TensorShape([200, 2]))\n\n\n\nbeta_hat = tf.Variable(tnp.array([-5.0,10.0]).reshape(2,1))\nbeta_hat\n\n&lt;tf.Variable 'Variable:0' shape=(2, 1) dtype=float64, numpy=\narray([[-5.],\n       [10.]])&gt;\n\n\n\nalpha=0.1\nopt = tf.optimizers.SGD(alpha)\n\n\nfor epoc in range(1000):\n    with tf.GradientTape() as tape:\n        yhat= X@beta_hat\n        loss= (y-yhat).T @ (y-yhat) / N\n    slope = tape.gradient(loss,beta_hat)\n    opt.apply_gradients([(slope,beta_hat)])\n    #beta_hat.assign_sub(alpha*slope)\n\n\nbeta_hat\n\n&lt;tf.Variable 'Variable:0' shape=(2, 1) dtype=float64, numpy=\narray([[2.58366061],\n       [3.93304684]])&gt;\n\n\n\n\n풀이2: 스칼라버전\n- 포인트\n## 포인트코드: 업데이트\nopt.apply_gradients([(slope0,beta0_hat),(slope1,beta1_hat)]) ## pair의 list가 입력\n- 풀이\n\ny=y.reshape(-1)\ny.shape,x.shape\n\n(TensorShape([200]), TensorShape([200]))\n\n\n\nbeta0_hat = tf.Variable(-5.0)\nbeta1_hat = tf.Variable(10.0)\n\n\nalpha=0.1\nopt = tf.optimizers.SGD(alpha)\n\n\nfor epoc in range(1000):\n    with tf.GradientTape() as tape:\n        yhat= beta0_hat + beta1_hat*x #X@beta_hat\n        loss= tf.reduce_sum((y-yhat)**2) / N\n    slope0,slope1 = tape.gradient(loss,[beta0_hat,beta1_hat])\n    opt.apply_gradients([(slope0,beta0_hat),(slope1,beta1_hat)])\n\n\nbeta0_hat,beta1_hat\n\n(&lt;tf.Variable 'Variable:0' shape=() dtype=float32, numpy=2.58366&gt;,\n &lt;tf.Variable 'Variable:0' shape=() dtype=float32, numpy=3.933048&gt;)\n\n\n\n\n\nopt.minimize\n\n풀이1: 벡터버전, 사용자정의 손실함수 with lambda\n- 풀이\n\ny=y.reshape(N,1)\ny.shape,X.shape\n\n(TensorShape([200, 1]), TensorShape([200, 2]))\n\n\n\nbeta_hat = tf.Variable(tnp.array([-5.0,10.0]).reshape(2,1))\nbeta_hat\n\n&lt;tf.Variable 'Variable:0' shape=(2, 1) dtype=float64, numpy=\narray([[-5.],\n       [10.]])&gt;\n\n\n\nloss_fn = lambda: (y-X@beta_hat).T @ (y-X@beta_hat) / N\n\n\nalpha=0.1\nopt = tf.optimizers.SGD(alpha)\n\n\nfor epoc in range(1000):\n    opt.minimize(loss_fn,beta_hat)\n\nAttributeError: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute '_unique_id'\n\n\n\nbeta_hat\n\n&lt;tf.Variable 'Variable:0' shape=(2, 1) dtype=float64, numpy=\narray([[-5.],\n       [10.]])&gt;\n\n\n\n\n풀이2: 스칼라버전, 사용자정의 손실함수 with lambda\n- 포인트\n## 포인트코드: 미분 & 업데이트 = minimize\nopt.minimize(loss_fn,[beta0_hat,beta1_hat])\n- 풀이\n\ny=y.reshape(-1)\ny.shape,x.shape\n\n(TensorShape([200]), TensorShape([200]))\n\n\n\nbeta0_hat = tf.Variable(-5.0)\nbeta1_hat = tf.Variable(10.0)\n\n\nloss_fn = lambda: tf.reduce_sum((y-beta0_hat-beta1_hat*x )**2) / N\n\n\nalpha=0.1\nopt = tf.optimizers.SGD(alpha)\n\n\nfor epoc in range(1000):\n    opt.minimize(loss_fn,[beta0_hat,beta1_hat])\n\n\nbeta0_hat,beta1_hat\n\n(&lt;tf.Variable 'Variable:0' shape=() dtype=float32, numpy=2.58366&gt;,\n &lt;tf.Variable 'Variable:0' shape=() dtype=float32, numpy=3.933048&gt;)\n\n\n\n\n풀이3: 벡터버전, 사용자정의 (짧은) 손실함수\n- 포인트\n## 포인트코드: 손실함수정의\ndef loss_fn():\n    return ??\n- 풀이\n\ny=y.reshape(N,1)\ny.shape,X.shape\n\n(TensorShape([200, 1]), TensorShape([200, 2]))\n\n\n\nbeta_hat = tf.Variable(tnp.array([-5.0,10.0]).reshape(2,1))\nbeta_hat\n\n&lt;tf.Variable 'Variable:0' shape=(2, 1) dtype=float64, numpy=\narray([[-5.],\n       [10.]])&gt;\n\n\n\ndef loss_fn():\n    return (y-X@beta_hat).T @ (y-X@beta_hat) / N\n\n\nalpha=0.1\nopt = tf.optimizers.SGD(alpha)\n\n\nfor epoc in range(1000):\n    opt.minimize(loss_fn,beta_hat)\n\nAttributeError: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute '_unique_id'\n\n\n\nbeta_hat\n\n&lt;tf.Variable 'Variable:0' shape=(2, 1) dtype=float64, numpy=\narray([[-5.],\n       [10.]])&gt;\n\n\n\n\n풀이4: 벡터버전, 사용자정의 (긴) 손실함수\n- 포인트\n## 포인트코드: 손실함수정의\ndef loss_fn():\n    ??\n    ??\n    return ??\n- 풀이\n\ny=y.reshape(N,1)\ny.shape,X.shape\n\n(TensorShape([200, 1]), TensorShape([200, 2]))\n\n\n\nbeta_hat = tf.Variable(tnp.array([-5.0,10.0]).reshape(2,1))\nbeta_hat\n\n&lt;tf.Variable 'Variable:0' shape=(2, 1) dtype=float64, numpy=\narray([[-5.],\n       [10.]])&gt;\n\n\n\ndef loss_fn():\n    yhat= X@beta_hat # 컴퓨터한테 전달할 수식1\n    loss = (y-yhat).T @ (y-yhat) / N # 컴퓨터한테 전달할 수식 2\n    return loss # tape.gradient(loss,beta_hat) 에서의 미분당하는애\n\n\nalpha=0.1\nopt = tf.optimizers.SGD(alpha)\n\n\nfor epoc in range(1000):\n    opt.minimize(loss_fn,beta_hat)\n\nAttributeError: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute '_unique_id'\n\n\n\nbeta_hat\n\n&lt;tf.Variable 'Variable:0' shape=(2, 1) dtype=float64, numpy=\narray([[-5.],\n       [10.]])&gt;\n\n\n\n\n풀이5: 벡터버전, 사용자정의 손실함수 &lt;- tf.losses.MSE\n- 포인트\n## 포인트코드: 미리구현되어있는 손실함수 이용\ntf.losses.MSE(y,yhat)\n- 풀이\n\ny=y.reshape(N,1)\ny.shape,X.shape\n\n(TensorShape([200, 1]), TensorShape([200, 2]))\n\n\n\nbeta_hat = tf.Variable(tnp.array([-5.0,10.0]).reshape(2,1))\nbeta_hat\n\n&lt;tf.Variable 'Variable:0' shape=(2, 1) dtype=float64, numpy=\narray([[-5.],\n       [10.]])&gt;\n\n\n\ndef loss_fn():\n    yhat= X@beta_hat # 컴퓨터한테 전달할 수식1\n    loss = tf.keras.losses.MSE(y.reshape(-1),yhat.reshape(-1)) # 컴퓨터한테 전달할 수식 2\n    return loss # tape.gradient(loss,beta_hat) 에서의 미분당하는애\n\n\nalpha=0.1\nopt = tf.optimizers.SGD(alpha)\n\n\nfor epoc in range(1000):\n    opt.minimize(loss_fn,beta_hat)\n\nAttributeError: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute '_unique_id'\n\n\n\nbeta_hat\n\n&lt;tf.Variable 'Variable:0' shape=(2, 1) dtype=float64, numpy=\narray([[-5.],\n       [10.]])&gt;\n\n\n\n\n풀이6: 벡터버전, 사용자정의 손실함수 &lt;- tf.losses.MeaSquaredError\n- 포인트\n## 포인트코드: 클래스로부터 손실함수 오브젝트 생성 (함수를 찍어내는 클래스)\nmse_fn = tf.losses.MeanSquaredError()\nmse_fn(y,yhat)\n- 풀이\n\nmseloss_fn = tf.losses.MeanSquaredError()\n\n\nmseloss_fn = tf.keras.losses.MSE 라고 보면된다.\n\n\ny=y.reshape(N,1)\ny.shape,X.shape\n\n(TensorShape([200, 1]), TensorShape([200, 2]))\n\n\n\nbeta_hat = tf.Variable(tnp.array([-5.0,10.0]).reshape(2,1))\nbeta_hat\n\n&lt;tf.Variable 'Variable:0' shape=(2, 1) dtype=float64, numpy=\narray([[-5.],\n       [10.]])&gt;\n\n\n\ndef loss_fn():\n    yhat= X@beta_hat # 컴퓨터한테 전달할 수식1\n    loss = mseloss_fn(y.reshape(-1),yhat.reshape(-1)) # 컴퓨터한테 전달할 수식 2\n    return loss # tape.gradient(loss,beta_hat) 에서의 미분당하는애\n\n\nalpha=0.1\nopt = tf.optimizers.SGD(alpha)\n\n\nfor epoc in range(1000):\n    opt.minimize(loss_fn,beta_hat)\n\nAttributeError: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute '_unique_id'\n\n\n\nbeta_hat\n\n&lt;tf.Variable 'Variable:0' shape=(2, 1) dtype=float64, numpy=\narray([[-5.],\n       [10.]])&gt;\n\n\n\n\n\ntf.keras.Sequential\n- \\(\\hat{y}_i=\\hat{\\beta}_0+\\hat{\\beta}_1x_i\\) 의 서로다른 표현\n\nimport graphviz\ndef gv(s): return graphviz.Source('digraph G{ rankdir=\"LR\"'+s + '; }')\n\n\ngv('''\n    \"1\" -&gt; \"beta0_hat + x*beta1_hat,    bias=False\"[label=\"* beta0_hat\"]\n    \"x\" -&gt; \"beta0_hat + x*beta1_hat,    bias=False\"[label=\"* beta1_hat\"]\n    \"beta0_hat + x*beta1_hat,    bias=False\" -&gt; \"yhat\"[label=\"indentity\"]\n    ''')\n\n\n\n\n\ngv('''\n\"x\" -&gt; \"x*beta1_hat,    bias=True\"[label=\"*beta1_hat\"] ;\n\"x*beta1_hat,    bias=True\" -&gt; \"yhat\"[label=\"indentity\"] ''')\n\n\n\n\n\ngv('''\n\"X=[1 x]\" -&gt; \"X@beta_hat,    bias=False\"[label=\"@beta_hat\"] ;\n\"X@beta_hat,    bias=False\" -&gt; \"yhat\"[label=\"indentity\"] ''')\n\n\n\n\n\n풀이1: 벡터버전, 사용자정의 손실함수\n- 포인트\n## 포인트코드1: 네트워크 생성\nnet = tf.keras.Sequential()\n\n## 포인트코드2: 네트워크의 아키텍처 설계\nnet.add(tf.keras.layers.Dense(1,input_shape=(2,),use_bias=False))\n\n## 포인트코드3: 네트워크 컴파일 = 아키텍처 + 손실함수 + 옵티마이저\nnet.compile(opt,loss=loss_fn2)\n\n## 포인트코드4: 미분 & update\nnet.fit(X,y,epochs=1000,verbose=0,batch_size=N)\n- 풀이\n\nnet = tf.keras.Sequential()\n\n\nnet.add(tf.keras.layers.Dense(units=1,input_shape=(2,),use_bias=False)) ## yhat을 구하는 방법정의 = 아키텍처가 설계\n\n\nunits는 layer의 출력의 차원, 이 경우는 yhat의 차원, yhat은 (200,1) 이므로 1임.\ninput_shape는 layer의 입력의 차원, 이 경우는 X의 차원, X는 (200,2) 이므로 2임.\n\n\ndef loss_fn2(y,yhat):\n    return (y-yhat).T @ (y-yhat) / N\n\n\nalpha=0.1\nopt =tf.optimizers.SGD(alpha)\n\n\n[np.array([[-5.0],[10.0]],dtype=np.float32)]\n\n[array([[-5.],\n        [10.]], dtype=float32)]\n\n\n\nnet.set_weights([np.array([[-5.0],[10.0]],dtype=np.float32)])\n\n\nnet.weights\n\n[&lt;tf.Variable 'dense/kernel:0' shape=(2, 1) dtype=float32, numpy=\n array([[-5.],\n        [10.]], dtype=float32)&gt;]\n\n\n\nnet.compile(opt,loss=tf.losses.MSE)\n# 아키텍처 + 손실함수 + 옵티마이저 =&gt; 네트워크에 다 합치자 =&gt; 네트워크를 컴파일한다.\n\n\nnet.fit(X,y,epochs=1000,batch_size=N,verbose=0) # 미분 + 파라메터업데이트 = net.fit\n\n&lt;keras.callbacks.History at 0x7fc7f48f5520&gt;\n\n\n\nverbose=0 하면 옵션 도는거 안보여짐.\n\n\nnet.weights\n\n[&lt;tf.Variable 'dense/kernel:0' shape=(2, 1) dtype=float32, numpy=\n array([[2.58366 ],\n        [3.933048]], dtype=float32)&gt;]"
  },
  {
    "objectID": "posts/2022_05_30_(13주차)_5월30일.html",
    "href": "posts/2022_05_30_(13주차)_5월30일.html",
    "title": "[STBDA] 13wk: 오버피팅, 학습과정분석",
    "section": "",
    "text": "해당 강의노트는 전북대학교 최규빈교수님 STBDA2022 자료임\n\n\nimports\n\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow.experimental.numpy as tnp\n\n\ntnp.experimental_enable_numpy_behavior()\n\n\nimport matplotlib.pyplot as plt\n\n\n%load_ext tensorboard\n\n\n\n오버피팅\n\n오버피팅으로 착각하기 쉬운 상황\n3-(1) 아래와 같은 모형을 고려하자.\n\\[y_i= \\beta_0 + \\sum_{k=1}^{5} \\beta_k \\cos(k t_i)+\\epsilon_i\\]\n여기에서 \\(t=(t_1,\\dots,t_{1000})=\\) np.linspace(0,5,1000) 이다. 그리고 \\(\\epsilon_i \\sim i.i.d~ N(0,\\sigma^2)\\), 즉 서로 독립인 표준정규분포에서 추출된 샘플이다. 위의 모형에서 아래와 같은 데이터를 관측했다고 가정하자.\n\nnp.random.seed(43052)\nt= np.linspace(0,5,1000)\ny = -2+ 3*np.cos(t) + 1*np.cos(2*t) + 0.5*np.cos(5*t) + np.random.randn(1000)*0.2\nplt.plot(t,y,'.',alpha=0.1)\n\n\n\n\ntf.keras를 이용하여 \\(\\beta_0,\\dots,\\beta_5\\)를 추정하라. (\\(\\beta_0,\\dots,\\beta_5\\)의 참값은 각각 -2,3,1,0,0,0.5 이다)\n(풀이)\n- 다시 풀어보자\n\ny = y.reshape(1000,1)\nx1 = np.cos(t)\nx2 = np.cos(2*t)\nx3 = np.cos(3*t)\nx4 = np.cos(4*t)\nx5 = np.cos(5*t)\nX = tf.stack([x1,x2,x3,x4,x5],axis=1)\n\n\n#collapse_output\n!rm -rf logs\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(1))\nnet.compile(loss='mse',optimizer='adam')\nnet.fit(X,y,epochs=500,batch_size=100, validation_split=0.45, callbacks=tf.keras.callbacks.TensorBoard())\n# 텐서보드를 이용한 시각화기능 추가\n# validation_split 이용\n\nEpoch 1/500\n6/6 [==============================] - 0s 15ms/step - loss: 10.0487 - val_loss: 15.8383\nEpoch 2/500\n6/6 [==============================] - 0s 3ms/step - loss: 9.9808 - val_loss: 15.7613\nEpoch 3/500\n6/6 [==============================] - 0s 3ms/step - loss: 9.9136 - val_loss: 15.6839\nEpoch 4/500\n6/6 [==============================] - 0s 3ms/step - loss: 9.8457 - val_loss: 15.6076\nEpoch 5/500\n6/6 [==============================] - 0s 3ms/step - loss: 9.7800 - val_loss: 15.5323\nEpoch 6/500\n6/6 [==============================] - 0s 3ms/step - loss: 9.7149 - val_loss: 15.4564\nEpoch 7/500\n6/6 [==============================] - 0s 3ms/step - loss: 9.6470 - val_loss: 15.3823\nEpoch 8/500\n6/6 [==============================] - 0s 3ms/step - loss: 9.5832 - val_loss: 15.3080\nEpoch 9/500\n6/6 [==============================] - 0s 3ms/step - loss: 9.5175 - val_loss: 15.2351\nEpoch 10/500\n6/6 [==============================] - 0s 3ms/step - loss: 9.4528 - val_loss: 15.1618\nEpoch 11/500\n6/6 [==============================] - 0s 3ms/step - loss: 9.3887 - val_loss: 15.0892\nEpoch 12/500\n6/6 [==============================] - 0s 3ms/step - loss: 9.3250 - val_loss: 15.0169\nEpoch 13/500\n6/6 [==============================] - 0s 3ms/step - loss: 9.2629 - val_loss: 14.9442\nEpoch 14/500\n6/6 [==============================] - 0s 3ms/step - loss: 9.1997 - val_loss: 14.8727\nEpoch 15/500\n6/6 [==============================] - 0s 3ms/step - loss: 9.1368 - val_loss: 14.8024\nEpoch 16/500\n6/6 [==============================] - 0s 3ms/step - loss: 9.0745 - val_loss: 14.7324\nEpoch 17/500\n6/6 [==============================] - 0s 3ms/step - loss: 9.0141 - val_loss: 14.6615\nEpoch 18/500\n6/6 [==============================] - 0s 3ms/step - loss: 8.9506 - val_loss: 14.5931\nEpoch 19/500\n6/6 [==============================] - 0s 3ms/step - loss: 8.8915 - val_loss: 14.5229\nEpoch 20/500\n6/6 [==============================] - 0s 3ms/step - loss: 8.8304 - val_loss: 14.4547\nEpoch 21/500\n6/6 [==============================] - 0s 3ms/step - loss: 8.7697 - val_loss: 14.3877\nEpoch 22/500\n6/6 [==============================] - 0s 3ms/step - loss: 8.7119 - val_loss: 14.3198\nEpoch 23/500\n6/6 [==============================] - 0s 3ms/step - loss: 8.6528 - val_loss: 14.2532\nEpoch 24/500\n6/6 [==============================] - 0s 3ms/step - loss: 8.5944 - val_loss: 14.1877\nEpoch 25/500\n6/6 [==============================] - 0s 3ms/step - loss: 8.5363 - val_loss: 14.1203\nEpoch 26/500\n6/6 [==============================] - 0s 3ms/step - loss: 8.4776 - val_loss: 14.0529\nEpoch 27/500\n6/6 [==============================] - 0s 3ms/step - loss: 8.4195 - val_loss: 13.9861\nEpoch 28/500\n6/6 [==============================] - 0s 3ms/step - loss: 8.3615 - val_loss: 13.9192\nEpoch 29/500\n6/6 [==============================] - 0s 3ms/step - loss: 8.3032 - val_loss: 13.8530\nEpoch 30/500\n6/6 [==============================] - 0s 3ms/step - loss: 8.2451 - val_loss: 13.7870\nEpoch 31/500\n6/6 [==============================] - 0s 3ms/step - loss: 8.1885 - val_loss: 13.7208\nEpoch 32/500\n6/6 [==============================] - 0s 3ms/step - loss: 8.1307 - val_loss: 13.6562\nEpoch 33/500\n6/6 [==============================] - 0s 3ms/step - loss: 8.0741 - val_loss: 13.5914\nEpoch 34/500\n6/6 [==============================] - 0s 3ms/step - loss: 8.0184 - val_loss: 13.5276\nEpoch 35/500\n6/6 [==============================] - 0s 3ms/step - loss: 7.9615 - val_loss: 13.4644\nEpoch 36/500\n6/6 [==============================] - 0s 3ms/step - loss: 7.9072 - val_loss: 13.4002\nEpoch 37/500\n6/6 [==============================] - 0s 3ms/step - loss: 7.8514 - val_loss: 13.3375\nEpoch 38/500\n6/6 [==============================] - 0s 3ms/step - loss: 7.7970 - val_loss: 13.2736\nEpoch 39/500\n6/6 [==============================] - 0s 3ms/step - loss: 7.7420 - val_loss: 13.2120\nEpoch 40/500\n6/6 [==============================] - 0s 3ms/step - loss: 7.6874 - val_loss: 13.1517\nEpoch 41/500\n6/6 [==============================] - 0s 3ms/step - loss: 7.6349 - val_loss: 13.0910\nEpoch 42/500\n6/6 [==============================] - 0s 3ms/step - loss: 7.5832 - val_loss: 13.0298\nEpoch 43/500\n6/6 [==============================] - 0s 3ms/step - loss: 7.5296 - val_loss: 12.9690\nEpoch 44/500\n6/6 [==============================] - 0s 3ms/step - loss: 7.4760 - val_loss: 12.9096\nEpoch 45/500\n6/6 [==============================] - 0s 3ms/step - loss: 7.4249 - val_loss: 12.8495\nEpoch 46/500\n6/6 [==============================] - 0s 3ms/step - loss: 7.3716 - val_loss: 12.7897\nEpoch 47/500\n6/6 [==============================] - 0s 3ms/step - loss: 7.3208 - val_loss: 12.7301\nEpoch 48/500\n6/6 [==============================] - 0s 3ms/step - loss: 7.2692 - val_loss: 12.6713\nEpoch 49/500\n6/6 [==============================] - 0s 3ms/step - loss: 7.2180 - val_loss: 12.6127\nEpoch 50/500\n6/6 [==============================] - 0s 3ms/step - loss: 7.1679 - val_loss: 12.5545\nEpoch 51/500\n6/6 [==============================] - 0s 3ms/step - loss: 7.1167 - val_loss: 12.4963\nEpoch 52/500\n6/6 [==============================] - 0s 3ms/step - loss: 7.0665 - val_loss: 12.4383\nEpoch 53/500\n6/6 [==============================] - 0s 3ms/step - loss: 7.0168 - val_loss: 12.3811\nEpoch 54/500\n6/6 [==============================] - 0s 3ms/step - loss: 6.9674 - val_loss: 12.3251\nEpoch 55/500\n6/6 [==============================] - 0s 3ms/step - loss: 6.9188 - val_loss: 12.2691\nEpoch 56/500\n6/6 [==============================] - 0s 3ms/step - loss: 6.8708 - val_loss: 12.2134\nEpoch 57/500\n6/6 [==============================] - 0s 3ms/step - loss: 6.8225 - val_loss: 12.1605\nEpoch 58/500\n6/6 [==============================] - 0s 3ms/step - loss: 6.7754 - val_loss: 12.1054\nEpoch 59/500\n6/6 [==============================] - 0s 3ms/step - loss: 6.7278 - val_loss: 12.0510\nEpoch 60/500\n6/6 [==============================] - 0s 3ms/step - loss: 6.6801 - val_loss: 11.9977\nEpoch 61/500\n6/6 [==============================] - 0s 3ms/step - loss: 6.6331 - val_loss: 11.9442\nEpoch 62/500\n6/6 [==============================] - 0s 3ms/step - loss: 6.5862 - val_loss: 11.8903\nEpoch 63/500\n6/6 [==============================] - 0s 3ms/step - loss: 6.5398 - val_loss: 11.8362\nEpoch 64/500\n6/6 [==============================] - 0s 3ms/step - loss: 6.4929 - val_loss: 11.7816\nEpoch 65/500\n6/6 [==============================] - 0s 3ms/step - loss: 6.4466 - val_loss: 11.7289\nEpoch 66/500\n6/6 [==============================] - 0s 3ms/step - loss: 6.4013 - val_loss: 11.6761\nEpoch 67/500\n6/6 [==============================] - 0s 3ms/step - loss: 6.3541 - val_loss: 11.6248\nEpoch 68/500\n6/6 [==============================] - 0s 3ms/step - loss: 6.3095 - val_loss: 11.5717\nEpoch 69/500\n6/6 [==============================] - 0s 3ms/step - loss: 6.2640 - val_loss: 11.5200\nEpoch 70/500\n6/6 [==============================] - 0s 3ms/step - loss: 6.2195 - val_loss: 11.4698\nEpoch 71/500\n6/6 [==============================] - 0s 3ms/step - loss: 6.1750 - val_loss: 11.4190\nEpoch 72/500\n6/6 [==============================] - 0s 3ms/step - loss: 6.1316 - val_loss: 11.3683\nEpoch 73/500\n6/6 [==============================] - 0s 3ms/step - loss: 6.0864 - val_loss: 11.3183\nEpoch 74/500\n6/6 [==============================] - 0s 3ms/step - loss: 6.0436 - val_loss: 11.2677\nEpoch 75/500\n6/6 [==============================] - 0s 3ms/step - loss: 6.0002 - val_loss: 11.2183\nEpoch 76/500\n6/6 [==============================] - 0s 3ms/step - loss: 5.9566 - val_loss: 11.1697\nEpoch 77/500\n6/6 [==============================] - 0s 3ms/step - loss: 5.9146 - val_loss: 11.1207\nEpoch 78/500\n6/6 [==============================] - 0s 3ms/step - loss: 5.8710 - val_loss: 11.0724\nEpoch 79/500\n6/6 [==============================] - 0s 3ms/step - loss: 5.8293 - val_loss: 11.0248\nEpoch 80/500\n6/6 [==============================] - 0s 3ms/step - loss: 5.7878 - val_loss: 10.9772\nEpoch 81/500\n6/6 [==============================] - 0s 3ms/step - loss: 5.7454 - val_loss: 10.9306\nEpoch 82/500\n6/6 [==============================] - 0s 3ms/step - loss: 5.7051 - val_loss: 10.8836\nEpoch 83/500\n6/6 [==============================] - 0s 3ms/step - loss: 5.6627 - val_loss: 10.8379\nEpoch 84/500\n6/6 [==============================] - 0s 3ms/step - loss: 5.6231 - val_loss: 10.7918\nEpoch 85/500\n6/6 [==============================] - 0s 3ms/step - loss: 5.5823 - val_loss: 10.7444\nEpoch 86/500\n6/6 [==============================] - 0s 3ms/step - loss: 5.5416 - val_loss: 10.6978\nEpoch 87/500\n6/6 [==============================] - 0s 3ms/step - loss: 5.5011 - val_loss: 10.6519\nEpoch 88/500\n6/6 [==============================] - 0s 3ms/step - loss: 5.4614 - val_loss: 10.6062\nEpoch 89/500\n6/6 [==============================] - 0s 3ms/step - loss: 5.4226 - val_loss: 10.5597\nEpoch 90/500\n6/6 [==============================] - 0s 3ms/step - loss: 5.3823 - val_loss: 10.5143\nEpoch 91/500\n6/6 [==============================] - 0s 3ms/step - loss: 5.3418 - val_loss: 10.4700\nEpoch 92/500\n6/6 [==============================] - 0s 3ms/step - loss: 5.3026 - val_loss: 10.4259\nEpoch 93/500\n6/6 [==============================] - 0s 3ms/step - loss: 5.2642 - val_loss: 10.3821\nEpoch 94/500\n6/6 [==============================] - 0s 3ms/step - loss: 5.2252 - val_loss: 10.3366\nEpoch 95/500\n6/6 [==============================] - 0s 3ms/step - loss: 5.1874 - val_loss: 10.2914\nEpoch 96/500\n6/6 [==============================] - 0s 3ms/step - loss: 5.1478 - val_loss: 10.2481\nEpoch 97/500\n6/6 [==============================] - 0s 3ms/step - loss: 5.1097 - val_loss: 10.2052\nEpoch 98/500\n6/6 [==============================] - 0s 3ms/step - loss: 5.0721 - val_loss: 10.1616\nEpoch 99/500\n6/6 [==============================] - 0s 3ms/step - loss: 5.0340 - val_loss: 10.1195\nEpoch 100/500\n6/6 [==============================] - 0s 3ms/step - loss: 4.9971 - val_loss: 10.0780\nEpoch 101/500\n6/6 [==============================] - 0s 3ms/step - loss: 4.9603 - val_loss: 10.0364\nEpoch 102/500\n6/6 [==============================] - 0s 3ms/step - loss: 4.9244 - val_loss: 9.9949\nEpoch 103/500\n6/6 [==============================] - 0s 3ms/step - loss: 4.8881 - val_loss: 9.9536\nEpoch 104/500\n6/6 [==============================] - 0s 3ms/step - loss: 4.8513 - val_loss: 9.9115\nEpoch 105/500\n6/6 [==============================] - 0s 3ms/step - loss: 4.8154 - val_loss: 9.8702\nEpoch 106/500\n6/6 [==============================] - 0s 3ms/step - loss: 4.7797 - val_loss: 9.8285\nEpoch 107/500\n6/6 [==============================] - 0s 3ms/step - loss: 4.7442 - val_loss: 9.7873\nEpoch 108/500\n6/6 [==============================] - 0s 3ms/step - loss: 4.7093 - val_loss: 9.7462\nEpoch 109/500\n6/6 [==============================] - 0s 3ms/step - loss: 4.6727 - val_loss: 9.7073\nEpoch 110/500\n6/6 [==============================] - 0s 3ms/step - loss: 4.6381 - val_loss: 9.6680\nEpoch 111/500\n6/6 [==============================] - 0s 3ms/step - loss: 4.6034 - val_loss: 9.6283\nEpoch 112/500\n6/6 [==============================] - 0s 3ms/step - loss: 4.5691 - val_loss: 9.5892\nEpoch 113/500\n6/6 [==============================] - 0s 3ms/step - loss: 4.5350 - val_loss: 9.5507\nEpoch 114/500\n6/6 [==============================] - 0s 3ms/step - loss: 4.5010 - val_loss: 9.5124\nEpoch 115/500\n6/6 [==============================] - 0s 3ms/step - loss: 4.4672 - val_loss: 9.4737\nEpoch 116/500\n6/6 [==============================] - 0s 3ms/step - loss: 4.4336 - val_loss: 9.4360\nEpoch 117/500\n6/6 [==============================] - 0s 3ms/step - loss: 4.4013 - val_loss: 9.3994\nEpoch 118/500\n6/6 [==============================] - 0s 3ms/step - loss: 4.3676 - val_loss: 9.3625\nEpoch 119/500\n6/6 [==============================] - 0s 3ms/step - loss: 4.3355 - val_loss: 9.3243\nEpoch 120/500\n6/6 [==============================] - 0s 3ms/step - loss: 4.3032 - val_loss: 9.2870\nEpoch 121/500\n6/6 [==============================] - 0s 3ms/step - loss: 4.2703 - val_loss: 9.2500\nEpoch 122/500\n6/6 [==============================] - 0s 3ms/step - loss: 4.2374 - val_loss: 9.2131\nEpoch 123/500\n6/6 [==============================] - 0s 3ms/step - loss: 4.2062 - val_loss: 9.1757\nEpoch 124/500\n6/6 [==============================] - 0s 3ms/step - loss: 4.1737 - val_loss: 9.1397\nEpoch 125/500\n6/6 [==============================] - 0s 3ms/step - loss: 4.1425 - val_loss: 9.1044\nEpoch 126/500\n6/6 [==============================] - 0s 3ms/step - loss: 4.1111 - val_loss: 9.0689\nEpoch 127/500\n6/6 [==============================] - 0s 3ms/step - loss: 4.0799 - val_loss: 9.0347\nEpoch 128/500\n6/6 [==============================] - 0s 3ms/step - loss: 4.0494 - val_loss: 9.0004\nEpoch 129/500\n6/6 [==============================] - 0s 3ms/step - loss: 4.0190 - val_loss: 8.9654\nEpoch 130/500\n6/6 [==============================] - 0s 3ms/step - loss: 3.9886 - val_loss: 8.9311\nEpoch 131/500\n6/6 [==============================] - 0s 3ms/step - loss: 3.9588 - val_loss: 8.8964\nEpoch 132/500\n6/6 [==============================] - 0s 3ms/step - loss: 3.9282 - val_loss: 8.8630\nEpoch 133/500\n6/6 [==============================] - 0s 3ms/step - loss: 3.8986 - val_loss: 8.8296\nEpoch 134/500\n6/6 [==============================] - 0s 3ms/step - loss: 3.8687 - val_loss: 8.7960\nEpoch 135/500\n6/6 [==============================] - 0s 3ms/step - loss: 3.8394 - val_loss: 8.7626\nEpoch 136/500\n6/6 [==============================] - 0s 3ms/step - loss: 3.8101 - val_loss: 8.7300\nEpoch 137/500\n6/6 [==============================] - 0s 3ms/step - loss: 3.7806 - val_loss: 8.6978\nEpoch 138/500\n6/6 [==============================] - 0s 3ms/step - loss: 3.7521 - val_loss: 8.6654\nEpoch 139/500\n6/6 [==============================] - 0s 3ms/step - loss: 3.7234 - val_loss: 8.6329\nEpoch 140/500\n6/6 [==============================] - 0s 3ms/step - loss: 3.6953 - val_loss: 8.6003\nEpoch 141/500\n6/6 [==============================] - 0s 3ms/step - loss: 3.6663 - val_loss: 8.5690\nEpoch 142/500\n6/6 [==============================] - 0s 3ms/step - loss: 3.6383 - val_loss: 8.5376\nEpoch 143/500\n6/6 [==============================] - 0s 3ms/step - loss: 3.6108 - val_loss: 8.5056\nEpoch 144/500\n6/6 [==============================] - 0s 3ms/step - loss: 3.5826 - val_loss: 8.4748\nEpoch 145/500\n6/6 [==============================] - 0s 3ms/step - loss: 3.5552 - val_loss: 8.4438\nEpoch 146/500\n6/6 [==============================] - 0s 3ms/step - loss: 3.5281 - val_loss: 8.4131\nEpoch 147/500\n6/6 [==============================] - 0s 3ms/step - loss: 3.5013 - val_loss: 8.3834\nEpoch 148/500\n6/6 [==============================] - 0s 3ms/step - loss: 3.4747 - val_loss: 8.3532\nEpoch 149/500\n6/6 [==============================] - 0s 3ms/step - loss: 3.4481 - val_loss: 8.3232\nEpoch 150/500\n6/6 [==============================] - 0s 3ms/step - loss: 3.4222 - val_loss: 8.2925\nEpoch 151/500\n6/6 [==============================] - 0s 3ms/step - loss: 3.3947 - val_loss: 8.2636\nEpoch 152/500\n6/6 [==============================] - 0s 3ms/step - loss: 3.3694 - val_loss: 8.2343\nEpoch 153/500\n6/6 [==============================] - 0s 3ms/step - loss: 3.3435 - val_loss: 8.2049\nEpoch 154/500\n6/6 [==============================] - 0s 3ms/step - loss: 3.3172 - val_loss: 8.1763\nEpoch 155/500\n6/6 [==============================] - 0s 3ms/step - loss: 3.2920 - val_loss: 8.1478\nEpoch 156/500\n6/6 [==============================] - 0s 3ms/step - loss: 3.2668 - val_loss: 8.1193\nEpoch 157/500\n6/6 [==============================] - 0s 3ms/step - loss: 3.2412 - val_loss: 8.0910\nEpoch 158/500\n6/6 [==============================] - 0s 3ms/step - loss: 3.2159 - val_loss: 8.0635\nEpoch 159/500\n6/6 [==============================] - 0s 3ms/step - loss: 3.1910 - val_loss: 8.0352\nEpoch 160/500\n6/6 [==============================] - 0s 3ms/step - loss: 3.1666 - val_loss: 8.0075\nEpoch 161/500\n6/6 [==============================] - 0s 3ms/step - loss: 3.1421 - val_loss: 7.9793\nEpoch 162/500\n6/6 [==============================] - 0s 3ms/step - loss: 3.1169 - val_loss: 7.9525\nEpoch 163/500\n6/6 [==============================] - 0s 3ms/step - loss: 3.0936 - val_loss: 7.9252\nEpoch 164/500\n6/6 [==============================] - 0s 3ms/step - loss: 3.0692 - val_loss: 7.8983\nEpoch 165/500\n6/6 [==============================] - 0s 3ms/step - loss: 3.0451 - val_loss: 7.8712\nEpoch 166/500\n6/6 [==============================] - 0s 3ms/step - loss: 3.0212 - val_loss: 7.8450\nEpoch 167/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.9977 - val_loss: 7.8189\nEpoch 168/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.9736 - val_loss: 7.7929\nEpoch 169/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.9508 - val_loss: 7.7665\nEpoch 170/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.9267 - val_loss: 7.7403\nEpoch 171/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.9040 - val_loss: 7.7142\nEpoch 172/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.8809 - val_loss: 7.6879\nEpoch 173/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.8584 - val_loss: 7.6619\nEpoch 174/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.8357 - val_loss: 7.6359\nEpoch 175/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.8135 - val_loss: 7.6107\nEpoch 176/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.7908 - val_loss: 7.5844\nEpoch 177/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.7691 - val_loss: 7.5588\nEpoch 178/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.7469 - val_loss: 7.5328\nEpoch 179/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.7248 - val_loss: 7.5070\nEpoch 180/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.7035 - val_loss: 7.4820\nEpoch 181/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.6819 - val_loss: 7.4576\nEpoch 182/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.6604 - val_loss: 7.4337\nEpoch 183/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.6396 - val_loss: 7.4101\nEpoch 184/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.6184 - val_loss: 7.3862\nEpoch 185/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.5976 - val_loss: 7.3626\nEpoch 186/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.5768 - val_loss: 7.3386\nEpoch 187/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.5565 - val_loss: 7.3143\nEpoch 188/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.5358 - val_loss: 7.2904\nEpoch 189/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.5156 - val_loss: 7.2661\nEpoch 190/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.4954 - val_loss: 7.2430\nEpoch 191/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.4753 - val_loss: 7.2203\nEpoch 192/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.4555 - val_loss: 7.1977\nEpoch 193/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.4362 - val_loss: 7.1751\nEpoch 194/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.4165 - val_loss: 7.1535\nEpoch 195/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.3973 - val_loss: 7.1316\nEpoch 196/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.3781 - val_loss: 7.1091\nEpoch 197/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.3592 - val_loss: 7.0877\nEpoch 198/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.3398 - val_loss: 7.0654\nEpoch 199/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.3213 - val_loss: 7.0436\nEpoch 200/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.3022 - val_loss: 7.0223\nEpoch 201/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.2834 - val_loss: 7.0009\nEpoch 202/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.2655 - val_loss: 6.9792\nEpoch 203/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.2465 - val_loss: 6.9575\nEpoch 204/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.2286 - val_loss: 6.9357\nEpoch 205/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.2101 - val_loss: 6.9149\nEpoch 206/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.1925 - val_loss: 6.8939\nEpoch 207/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.1748 - val_loss: 6.8716\nEpoch 208/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.1572 - val_loss: 6.8519\nEpoch 209/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.1397 - val_loss: 6.8315\nEpoch 210/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.1222 - val_loss: 6.8120\nEpoch 211/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.1054 - val_loss: 6.7916\nEpoch 212/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.0880 - val_loss: 6.7723\nEpoch 213/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.0713 - val_loss: 6.7535\nEpoch 214/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.0546 - val_loss: 6.7346\nEpoch 215/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.0382 - val_loss: 6.7144\nEpoch 216/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.0216 - val_loss: 6.6951\nEpoch 217/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.0050 - val_loss: 6.6763\nEpoch 218/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.9890 - val_loss: 6.6575\nEpoch 219/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.9726 - val_loss: 6.6393\nEpoch 220/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.9566 - val_loss: 6.6206\nEpoch 221/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.9409 - val_loss: 6.6020\nEpoch 222/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.9249 - val_loss: 6.5835\nEpoch 223/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.9096 - val_loss: 6.5654\nEpoch 224/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.8937 - val_loss: 6.5476\nEpoch 225/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.8787 - val_loss: 6.5296\nEpoch 226/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.8632 - val_loss: 6.5118\nEpoch 227/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.8480 - val_loss: 6.4938\nEpoch 228/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.8331 - val_loss: 6.4762\nEpoch 229/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.8179 - val_loss: 6.4591\nEpoch 230/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.8032 - val_loss: 6.4418\nEpoch 231/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.7886 - val_loss: 6.4246\nEpoch 232/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.7738 - val_loss: 6.4079\nEpoch 233/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.7595 - val_loss: 6.3905\nEpoch 234/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.7451 - val_loss: 6.3733\nEpoch 235/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.7305 - val_loss: 6.3561\nEpoch 236/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.7165 - val_loss: 6.3390\nEpoch 237/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.7024 - val_loss: 6.3212\nEpoch 238/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.6882 - val_loss: 6.3043\nEpoch 239/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.6745 - val_loss: 6.2869\nEpoch 240/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.6606 - val_loss: 6.2703\nEpoch 241/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.6470 - val_loss: 6.2547\nEpoch 242/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.6335 - val_loss: 6.2384\nEpoch 243/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.6200 - val_loss: 6.2223\nEpoch 244/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.6067 - val_loss: 6.2055\nEpoch 245/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.5935 - val_loss: 6.1894\nEpoch 246/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.5804 - val_loss: 6.1735\nEpoch 247/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.5673 - val_loss: 6.1573\nEpoch 248/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.5544 - val_loss: 6.1409\nEpoch 249/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.5416 - val_loss: 6.1249\nEpoch 250/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.5288 - val_loss: 6.1100\nEpoch 251/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.5162 - val_loss: 6.0949\nEpoch 252/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.5039 - val_loss: 6.0796\nEpoch 253/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.4913 - val_loss: 6.0641\nEpoch 254/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.4792 - val_loss: 6.0482\nEpoch 255/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.4668 - val_loss: 6.0327\nEpoch 256/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.4547 - val_loss: 6.0176\nEpoch 257/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.4426 - val_loss: 6.0025\nEpoch 258/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.4305 - val_loss: 5.9874\nEpoch 259/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.4188 - val_loss: 5.9716\nEpoch 260/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.4071 - val_loss: 5.9556\nEpoch 261/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.3954 - val_loss: 5.9407\nEpoch 262/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.3838 - val_loss: 5.9261\nEpoch 263/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.3722 - val_loss: 5.9119\nEpoch 264/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.3610 - val_loss: 5.8977\nEpoch 265/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.3497 - val_loss: 5.8836\nEpoch 266/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.3386 - val_loss: 5.8688\nEpoch 267/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.3275 - val_loss: 5.8544\nEpoch 268/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.3168 - val_loss: 5.8399\nEpoch 269/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.3055 - val_loss: 5.8263\nEpoch 270/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.2949 - val_loss: 5.8124\nEpoch 271/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.2842 - val_loss: 5.7990\nEpoch 272/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.2734 - val_loss: 5.7858\nEpoch 273/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.2629 - val_loss: 5.7731\nEpoch 274/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.2525 - val_loss: 5.7597\nEpoch 275/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.2421 - val_loss: 5.7456\nEpoch 276/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.2318 - val_loss: 5.7320\nEpoch 277/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.2215 - val_loss: 5.7185\nEpoch 278/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.2114 - val_loss: 5.7054\nEpoch 279/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.2014 - val_loss: 5.6918\nEpoch 280/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.1915 - val_loss: 5.6785\nEpoch 281/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.1815 - val_loss: 5.6653\nEpoch 282/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.1716 - val_loss: 5.6520\nEpoch 283/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.1618 - val_loss: 5.6389\nEpoch 284/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.1522 - val_loss: 5.6260\nEpoch 285/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.1424 - val_loss: 5.6137\nEpoch 286/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.1329 - val_loss: 5.6008\nEpoch 287/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.1234 - val_loss: 5.5881\nEpoch 288/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.1142 - val_loss: 5.5759\nEpoch 289/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.1049 - val_loss: 5.5638\nEpoch 290/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.0959 - val_loss: 5.5512\nEpoch 291/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.0868 - val_loss: 5.5395\nEpoch 292/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.0778 - val_loss: 5.5273\nEpoch 293/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.0689 - val_loss: 5.5155\nEpoch 294/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.0600 - val_loss: 5.5036\nEpoch 295/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.0514 - val_loss: 5.4916\nEpoch 296/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.0425 - val_loss: 5.4804\nEpoch 297/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.0342 - val_loss: 5.4684\nEpoch 298/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.0254 - val_loss: 5.4567\nEpoch 299/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.0169 - val_loss: 5.4454\nEpoch 300/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.0087 - val_loss: 5.4336\nEpoch 301/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.0003 - val_loss: 5.4223\nEpoch 302/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.9922 - val_loss: 5.4112\nEpoch 303/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.9840 - val_loss: 5.3997\nEpoch 304/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.9758 - val_loss: 5.3889\nEpoch 305/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.9679 - val_loss: 5.3780\nEpoch 306/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.9600 - val_loss: 5.3667\nEpoch 307/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.9521 - val_loss: 5.3556\nEpoch 308/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.9442 - val_loss: 5.3442\nEpoch 309/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.9365 - val_loss: 5.3335\nEpoch 310/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.9288 - val_loss: 5.3230\nEpoch 311/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.9211 - val_loss: 5.3118\nEpoch 312/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.9135 - val_loss: 5.3006\nEpoch 313/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.9061 - val_loss: 5.2896\nEpoch 314/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.8987 - val_loss: 5.2785\nEpoch 315/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.8912 - val_loss: 5.2678\nEpoch 316/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.8840 - val_loss: 5.2566\nEpoch 317/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.8768 - val_loss: 5.2458\nEpoch 318/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.8696 - val_loss: 5.2346\nEpoch 319/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.8624 - val_loss: 5.2238\nEpoch 320/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.8552 - val_loss: 5.2129\nEpoch 321/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.8482 - val_loss: 5.2022\nEpoch 322/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.8412 - val_loss: 5.1910\nEpoch 323/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.8343 - val_loss: 5.1802\nEpoch 324/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.8275 - val_loss: 5.1693\nEpoch 325/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.8206 - val_loss: 5.1592\nEpoch 326/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.8140 - val_loss: 5.1487\nEpoch 327/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.8073 - val_loss: 5.1384\nEpoch 328/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.8008 - val_loss: 5.1287\nEpoch 329/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.7943 - val_loss: 5.1186\nEpoch 330/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.7879 - val_loss: 5.1079\nEpoch 331/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.7814 - val_loss: 5.0977\nEpoch 332/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.7750 - val_loss: 5.0873\nEpoch 333/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.7687 - val_loss: 5.0768\nEpoch 334/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.7625 - val_loss: 5.0666\nEpoch 335/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.7562 - val_loss: 5.0569\nEpoch 336/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.7502 - val_loss: 5.0470\nEpoch 337/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.7440 - val_loss: 5.0377\nEpoch 338/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.7381 - val_loss: 5.0282\nEpoch 339/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.7321 - val_loss: 5.0186\nEpoch 340/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.7262 - val_loss: 5.0086\nEpoch 341/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.7204 - val_loss: 4.9980\nEpoch 342/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.7146 - val_loss: 4.9882\nEpoch 343/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.7089 - val_loss: 4.9786\nEpoch 344/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.7032 - val_loss: 4.9691\nEpoch 345/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.6975 - val_loss: 4.9597\nEpoch 346/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.6919 - val_loss: 4.9500\nEpoch 347/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.6864 - val_loss: 4.9407\nEpoch 348/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.6809 - val_loss: 4.9313\nEpoch 349/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.6755 - val_loss: 4.9218\nEpoch 350/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.6702 - val_loss: 4.9126\nEpoch 351/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.6647 - val_loss: 4.9039\nEpoch 352/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.6595 - val_loss: 4.8949\nEpoch 353/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.6543 - val_loss: 4.8853\nEpoch 354/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.6491 - val_loss: 4.8757\nEpoch 355/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.6440 - val_loss: 4.8663\nEpoch 356/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.6390 - val_loss: 4.8570\nEpoch 357/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.6337 - val_loss: 4.8484\nEpoch 358/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.6289 - val_loss: 4.8392\nEpoch 359/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.6239 - val_loss: 4.8301\nEpoch 360/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.6191 - val_loss: 4.8209\nEpoch 361/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.6142 - val_loss: 4.8123\nEpoch 362/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.6094 - val_loss: 4.8037\nEpoch 363/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.6046 - val_loss: 4.7953\nEpoch 364/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.5999 - val_loss: 4.7865\nEpoch 365/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.5952 - val_loss: 4.7778\nEpoch 366/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.5905 - val_loss: 4.7687\nEpoch 367/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.5859 - val_loss: 4.7598\nEpoch 368/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.5813 - val_loss: 4.7509\nEpoch 369/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.5768 - val_loss: 4.7417\nEpoch 370/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.5722 - val_loss: 4.7330\nEpoch 371/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.5678 - val_loss: 4.7244\nEpoch 372/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.5634 - val_loss: 4.7154\nEpoch 373/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.5590 - val_loss: 4.7066\nEpoch 374/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.5547 - val_loss: 4.6978\nEpoch 375/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.5505 - val_loss: 4.6892\nEpoch 376/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.5463 - val_loss: 4.6808\nEpoch 377/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.5421 - val_loss: 4.6723\nEpoch 378/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.5379 - val_loss: 4.6636\nEpoch 379/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.5338 - val_loss: 4.6550\nEpoch 380/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.5297 - val_loss: 4.6467\nEpoch 381/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.5257 - val_loss: 4.6378\nEpoch 382/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.5216 - val_loss: 4.6290\nEpoch 383/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.5176 - val_loss: 4.6204\nEpoch 384/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.5137 - val_loss: 4.6120\nEpoch 385/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.5098 - val_loss: 4.6034\nEpoch 386/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.5059 - val_loss: 4.5948\nEpoch 387/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.5021 - val_loss: 4.5862\nEpoch 388/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4984 - val_loss: 4.5775\nEpoch 389/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4946 - val_loss: 4.5692\nEpoch 390/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4909 - val_loss: 4.5607\nEpoch 391/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4872 - val_loss: 4.5523\nEpoch 392/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4836 - val_loss: 4.5439\nEpoch 393/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4799 - val_loss: 4.5359\nEpoch 394/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4765 - val_loss: 4.5276\nEpoch 395/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4729 - val_loss: 4.5197\nEpoch 396/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4694 - val_loss: 4.5114\nEpoch 397/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4659 - val_loss: 4.5028\nEpoch 398/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4625 - val_loss: 4.4946\nEpoch 399/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4591 - val_loss: 4.4868\nEpoch 400/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4557 - val_loss: 4.4793\nEpoch 401/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4524 - val_loss: 4.4713\nEpoch 402/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4491 - val_loss: 4.4627\nEpoch 403/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4458 - val_loss: 4.4547\nEpoch 404/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4426 - val_loss: 4.4467\nEpoch 405/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4394 - val_loss: 4.4384\nEpoch 406/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4362 - val_loss: 4.4304\nEpoch 407/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4331 - val_loss: 4.4218\nEpoch 408/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4299 - val_loss: 4.4138\nEpoch 409/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4269 - val_loss: 4.4057\nEpoch 410/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4238 - val_loss: 4.3977\nEpoch 411/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4207 - val_loss: 4.3896\nEpoch 412/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4177 - val_loss: 4.3816\nEpoch 413/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4147 - val_loss: 4.3740\nEpoch 414/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4118 - val_loss: 4.3656\nEpoch 415/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4088 - val_loss: 4.3573\nEpoch 416/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4059 - val_loss: 4.3496\nEpoch 417/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4031 - val_loss: 4.3416\nEpoch 418/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4003 - val_loss: 4.3333\nEpoch 419/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3973 - val_loss: 4.3256\nEpoch 420/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3946 - val_loss: 4.3177\nEpoch 421/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3918 - val_loss: 4.3101\nEpoch 422/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3891 - val_loss: 4.3026\nEpoch 423/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3865 - val_loss: 4.2948\nEpoch 424/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3837 - val_loss: 4.2870\nEpoch 425/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3812 - val_loss: 4.2786\nEpoch 426/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3785 - val_loss: 4.2707\nEpoch 427/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3759 - val_loss: 4.2626\nEpoch 428/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3734 - val_loss: 4.2545\nEpoch 429/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3708 - val_loss: 4.2463\nEpoch 430/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3683 - val_loss: 4.2384\nEpoch 431/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3657 - val_loss: 4.2311\nEpoch 432/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3632 - val_loss: 4.2237\nEpoch 433/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3608 - val_loss: 4.2161\nEpoch 434/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3584 - val_loss: 4.2083\nEpoch 435/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3561 - val_loss: 4.2007\nEpoch 436/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3536 - val_loss: 4.1931\nEpoch 437/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3513 - val_loss: 4.1860\nEpoch 438/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3490 - val_loss: 4.1785\nEpoch 439/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3467 - val_loss: 4.1713\nEpoch 440/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3444 - val_loss: 4.1635\nEpoch 441/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3422 - val_loss: 4.1556\nEpoch 442/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3399 - val_loss: 4.1478\nEpoch 443/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3377 - val_loss: 4.1404\nEpoch 444/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3356 - val_loss: 4.1324\nEpoch 445/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3334 - val_loss: 4.1243\nEpoch 446/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3312 - val_loss: 4.1167\nEpoch 447/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3291 - val_loss: 4.1091\nEpoch 448/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3270 - val_loss: 4.1015\nEpoch 449/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3250 - val_loss: 4.0939\nEpoch 450/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3229 - val_loss: 4.0859\nEpoch 451/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3209 - val_loss: 4.0782\nEpoch 452/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3189 - val_loss: 4.0702\nEpoch 453/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3169 - val_loss: 4.0625\nEpoch 454/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3149 - val_loss: 4.0550\nEpoch 455/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3129 - val_loss: 4.0475\nEpoch 456/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3110 - val_loss: 4.0401\nEpoch 457/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3091 - val_loss: 4.0320\nEpoch 458/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3072 - val_loss: 4.0240\nEpoch 459/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3052 - val_loss: 4.0166\nEpoch 460/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3034 - val_loss: 4.0087\nEpoch 461/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3016 - val_loss: 4.0012\nEpoch 462/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2998 - val_loss: 3.9929\nEpoch 463/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2980 - val_loss: 3.9848\nEpoch 464/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2962 - val_loss: 3.9770\nEpoch 465/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2944 - val_loss: 3.9695\nEpoch 466/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2927 - val_loss: 3.9618\nEpoch 467/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2910 - val_loss: 3.9542\nEpoch 468/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2893 - val_loss: 3.9468\nEpoch 469/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2876 - val_loss: 3.9392\nEpoch 470/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2859 - val_loss: 3.9318\nEpoch 471/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2843 - val_loss: 3.9241\nEpoch 472/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2826 - val_loss: 3.9166\nEpoch 473/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2811 - val_loss: 3.9085\nEpoch 474/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2794 - val_loss: 3.9008\nEpoch 475/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2778 - val_loss: 3.8931\nEpoch 476/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2763 - val_loss: 3.8855\nEpoch 477/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2747 - val_loss: 3.8773\nEpoch 478/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2732 - val_loss: 3.8694\nEpoch 479/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2716 - val_loss: 3.8618\nEpoch 480/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2701 - val_loss: 3.8538\nEpoch 481/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2686 - val_loss: 3.8460\nEpoch 482/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2671 - val_loss: 3.8385\nEpoch 483/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2656 - val_loss: 3.8310\nEpoch 484/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2642 - val_loss: 3.8234\nEpoch 485/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2628 - val_loss: 3.8157\nEpoch 486/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2614 - val_loss: 3.8074\nEpoch 487/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2599 - val_loss: 3.8000\nEpoch 488/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2585 - val_loss: 3.7923\nEpoch 489/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2572 - val_loss: 3.7849\nEpoch 490/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2558 - val_loss: 3.7775\nEpoch 491/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2544 - val_loss: 3.7700\nEpoch 492/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2531 - val_loss: 3.7623\nEpoch 493/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2517 - val_loss: 3.7545\nEpoch 494/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2504 - val_loss: 3.7468\nEpoch 495/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2491 - val_loss: 3.7394\nEpoch 496/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2479 - val_loss: 3.7318\nEpoch 497/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2465 - val_loss: 3.7241\nEpoch 498/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2453 - val_loss: 3.7170\nEpoch 499/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2440 - val_loss: 3.7097\nEpoch 500/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2428 - val_loss: 3.7022\n\n\n&lt;keras.callbacks.History at 0x7f38a7ff58e0&gt;\n\n\n- 결과시각화\n\nplt.plot(y,'.',alpha=0.1)\nplt.plot(net(X),'--')\n\n\n\n\n- 보여준 데이터에서는 잘 맞추는것 같지만 validation에서는 엉망이다. -&gt; 오버피팅인가? -&gt; 텐서보드로 확인\n\n#\n#%tensorboard --logdir logs --host 0.0.0.0\n\n\n확인결과: 에폭마다 val_loss 가 줄어들고 있기는 하다 (늦게 줄어들뿐) -&gt; 오버피팅이라기보다 val_set에 들어있는 자료를 예측하기에는 보여준 데이터가 불충분하다라고 해석하는것이 더 옳음 (모형자체의 문제는 아님)\n\n- 해결하는 방법? 그냥 더 학습시키면된다.\n\n#collapse_output\n!rm -rf logs\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(1))\nnet.compile(loss='mse',optimizer='adam')\nnet.fit(X,y,epochs=2000,batch_size=100, validation_split=0.45, callbacks=tf.keras.callbacks.TensorBoard())\n# 텐서보드를 이용한 시각화기능 추가\n# validation_split 이용\n\nEpoch 1/2000\n6/6 [==============================] - 0s 9ms/step - loss: 7.9563 - val_loss: 17.0027\nEpoch 2/2000\n6/6 [==============================] - 0s 4ms/step - loss: 7.9013 - val_loss: 16.9412\nEpoch 3/2000\n6/6 [==============================] - 0s 3ms/step - loss: 7.8463 - val_loss: 16.8810\nEpoch 4/2000\n6/6 [==============================] - 0s 3ms/step - loss: 7.7913 - val_loss: 16.8207\nEpoch 5/2000\n6/6 [==============================] - 0s 3ms/step - loss: 7.7380 - val_loss: 16.7617\nEpoch 6/2000\n6/6 [==============================] - 0s 3ms/step - loss: 7.6829 - val_loss: 16.7032\nEpoch 7/2000\n6/6 [==============================] - 0s 3ms/step - loss: 7.6308 - val_loss: 16.6446\nEpoch 8/2000\n6/6 [==============================] - 0s 3ms/step - loss: 7.5769 - val_loss: 16.5886\nEpoch 9/2000\n6/6 [==============================] - 0s 3ms/step - loss: 7.5245 - val_loss: 16.5321\nEpoch 10/2000\n6/6 [==============================] - 0s 3ms/step - loss: 7.4717 - val_loss: 16.4760\nEpoch 11/2000\n6/6 [==============================] - 0s 3ms/step - loss: 7.4201 - val_loss: 16.4201\nEpoch 12/2000\n6/6 [==============================] - 0s 4ms/step - loss: 7.3686 - val_loss: 16.3636\nEpoch 13/2000\n6/6 [==============================] - 0s 3ms/step - loss: 7.3162 - val_loss: 16.3079\nEpoch 14/2000\n6/6 [==============================] - 0s 3ms/step - loss: 7.2647 - val_loss: 16.2527\nEpoch 15/2000\n6/6 [==============================] - 0s 3ms/step - loss: 7.2148 - val_loss: 16.1978\nEpoch 16/2000\n6/6 [==============================] - 0s 3ms/step - loss: 7.1634 - val_loss: 16.1427\nEpoch 17/2000\n6/6 [==============================] - 0s 3ms/step - loss: 7.1142 - val_loss: 16.0877\nEpoch 18/2000\n6/6 [==============================] - 0s 3ms/step - loss: 7.0632 - val_loss: 16.0335\nEpoch 19/2000\n6/6 [==============================] - 0s 3ms/step - loss: 7.0148 - val_loss: 15.9786\nEpoch 20/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.9651 - val_loss: 15.9238\nEpoch 21/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.9164 - val_loss: 15.8699\nEpoch 22/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.8670 - val_loss: 15.8155\nEpoch 23/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.8191 - val_loss: 15.7617\nEpoch 24/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.7714 - val_loss: 15.7093\nEpoch 25/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.7231 - val_loss: 15.6568\nEpoch 26/2000\n6/6 [==============================] - 0s 4ms/step - loss: 6.6762 - val_loss: 15.6048\nEpoch 27/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.6286 - val_loss: 15.5528\nEpoch 28/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.5816 - val_loss: 15.5012\nEpoch 29/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.5349 - val_loss: 15.4495\nEpoch 30/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.4885 - val_loss: 15.3982\nEpoch 31/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.4417 - val_loss: 15.3463\nEpoch 32/2000\n6/6 [==============================] - 0s 4ms/step - loss: 6.3956 - val_loss: 15.2957\nEpoch 33/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.3508 - val_loss: 15.2442\nEpoch 34/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.3053 - val_loss: 15.1941\nEpoch 35/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.2593 - val_loss: 15.1447\nEpoch 36/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.2142 - val_loss: 15.0943\nEpoch 37/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.1706 - val_loss: 15.0446\nEpoch 38/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.1261 - val_loss: 14.9966\nEpoch 39/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.0825 - val_loss: 14.9488\nEpoch 40/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.0394 - val_loss: 14.9001\nEpoch 41/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.9965 - val_loss: 14.8519\nEpoch 42/2000\n6/6 [==============================] - 0s 4ms/step - loss: 5.9530 - val_loss: 14.8035\nEpoch 43/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.9106 - val_loss: 14.7552\nEpoch 44/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.8678 - val_loss: 14.7074\nEpoch 45/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.8262 - val_loss: 14.6600\nEpoch 46/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.7841 - val_loss: 14.6128\nEpoch 47/2000\n6/6 [==============================] - 0s 4ms/step - loss: 5.7422 - val_loss: 14.5659\nEpoch 48/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.7007 - val_loss: 14.5194\nEpoch 49/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.6598 - val_loss: 14.4737\nEpoch 50/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.6183 - val_loss: 14.4275\nEpoch 51/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.5779 - val_loss: 14.3826\nEpoch 52/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.5376 - val_loss: 14.3368\nEpoch 53/2000\n6/6 [==============================] - 0s 4ms/step - loss: 5.4973 - val_loss: 14.2902\nEpoch 54/2000\n6/6 [==============================] - 0s 4ms/step - loss: 5.4572 - val_loss: 14.2453\nEpoch 55/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.4176 - val_loss: 14.2002\nEpoch 56/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.3785 - val_loss: 14.1551\nEpoch 57/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.3387 - val_loss: 14.1109\nEpoch 58/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.2996 - val_loss: 14.0679\nEpoch 59/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.2613 - val_loss: 14.0241\nEpoch 60/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.2221 - val_loss: 13.9811\nEpoch 61/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.1839 - val_loss: 13.9382\nEpoch 62/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.1467 - val_loss: 13.8949\nEpoch 63/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.1086 - val_loss: 13.8506\nEpoch 64/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.0710 - val_loss: 13.8065\nEpoch 65/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.0336 - val_loss: 13.7637\nEpoch 66/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.9956 - val_loss: 13.7212\nEpoch 67/2000\n6/6 [==============================] - 0s 4ms/step - loss: 4.9589 - val_loss: 13.6790\nEpoch 68/2000\n6/6 [==============================] - 0s 2ms/step - loss: 4.9218 - val_loss: 13.6360\nEpoch 69/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.8858 - val_loss: 13.5936\nEpoch 70/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.8495 - val_loss: 13.5514\nEpoch 71/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.8133 - val_loss: 13.5099\nEpoch 72/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.7778 - val_loss: 13.4699\nEpoch 73/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.7419 - val_loss: 13.4284\nEpoch 74/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.7071 - val_loss: 13.3875\nEpoch 75/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.6717 - val_loss: 13.3462\nEpoch 76/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.6372 - val_loss: 13.3066\nEpoch 77/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.6033 - val_loss: 13.2665\nEpoch 78/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.5686 - val_loss: 13.2260\nEpoch 79/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.5349 - val_loss: 13.1872\nEpoch 80/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.5012 - val_loss: 13.1474\nEpoch 81/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.4672 - val_loss: 13.1093\nEpoch 82/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.4336 - val_loss: 13.0714\nEpoch 83/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.4009 - val_loss: 13.0337\nEpoch 84/2000\n6/6 [==============================] - 0s 4ms/step - loss: 4.3675 - val_loss: 12.9957\nEpoch 85/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.3348 - val_loss: 12.9574\nEpoch 86/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.3025 - val_loss: 12.9182\nEpoch 87/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.2700 - val_loss: 12.8800\nEpoch 88/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.2382 - val_loss: 12.8425\nEpoch 89/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.2063 - val_loss: 12.8047\nEpoch 90/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.1745 - val_loss: 12.7663\nEpoch 91/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.1428 - val_loss: 12.7295\nEpoch 92/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.1119 - val_loss: 12.6919\nEpoch 93/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.0804 - val_loss: 12.6557\nEpoch 94/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.0493 - val_loss: 12.6196\nEpoch 95/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.0188 - val_loss: 12.5834\nEpoch 96/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.9878 - val_loss: 12.5464\nEpoch 97/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.9580 - val_loss: 12.5099\nEpoch 98/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.9274 - val_loss: 12.4748\nEpoch 99/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.8977 - val_loss: 12.4391\nEpoch 100/2000\n6/6 [==============================] - 0s 4ms/step - loss: 3.8678 - val_loss: 12.4017\nEpoch 101/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.8381 - val_loss: 12.3662\nEpoch 102/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.8084 - val_loss: 12.3317\nEpoch 103/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.7793 - val_loss: 12.2967\nEpoch 104/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.7504 - val_loss: 12.2616\nEpoch 105/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.7214 - val_loss: 12.2255\nEpoch 106/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.6928 - val_loss: 12.1913\nEpoch 107/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.6639 - val_loss: 12.1572\nEpoch 108/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.6359 - val_loss: 12.1229\nEpoch 109/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.6079 - val_loss: 12.0890\nEpoch 110/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.5798 - val_loss: 12.0553\nEpoch 111/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.5520 - val_loss: 12.0209\nEpoch 112/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.5245 - val_loss: 11.9872\nEpoch 113/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.4968 - val_loss: 11.9544\nEpoch 114/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.4703 - val_loss: 11.9212\nEpoch 115/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.4430 - val_loss: 11.8894\nEpoch 116/2000\n6/6 [==============================] - 0s 4ms/step - loss: 3.4164 - val_loss: 11.8567\nEpoch 117/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.3899 - val_loss: 11.8250\nEpoch 118/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.3633 - val_loss: 11.7929\nEpoch 119/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.3368 - val_loss: 11.7604\nEpoch 120/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.3108 - val_loss: 11.7281\nEpoch 121/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.2852 - val_loss: 11.6949\nEpoch 122/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.2593 - val_loss: 11.6615\nEpoch 123/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.2339 - val_loss: 11.6295\nEpoch 124/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.2084 - val_loss: 11.5968\nEpoch 125/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.1835 - val_loss: 11.5650\nEpoch 126/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.1585 - val_loss: 11.5338\nEpoch 127/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.1335 - val_loss: 11.5018\nEpoch 128/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.1093 - val_loss: 11.4705\nEpoch 129/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.0847 - val_loss: 11.4392\nEpoch 130/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.0604 - val_loss: 11.4076\nEpoch 131/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.0360 - val_loss: 11.3776\nEpoch 132/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.0123 - val_loss: 11.3460\nEpoch 133/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.9884 - val_loss: 11.3158\nEpoch 134/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.9648 - val_loss: 11.2869\nEpoch 135/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.9414 - val_loss: 11.2564\nEpoch 136/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.9179 - val_loss: 11.2282\nEpoch 137/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.8950 - val_loss: 11.1989\nEpoch 138/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.8720 - val_loss: 11.1695\nEpoch 139/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.8492 - val_loss: 11.1400\nEpoch 140/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.8265 - val_loss: 11.1121\nEpoch 141/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.8047 - val_loss: 11.0833\nEpoch 142/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.7819 - val_loss: 11.0539\nEpoch 143/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.7598 - val_loss: 11.0244\nEpoch 144/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.7381 - val_loss: 10.9954\nEpoch 145/2000\n6/6 [==============================] - 0s 4ms/step - loss: 2.7157 - val_loss: 10.9659\nEpoch 146/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.6944 - val_loss: 10.9374\nEpoch 147/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.6728 - val_loss: 10.9100\nEpoch 148/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.6517 - val_loss: 10.8820\nEpoch 149/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.6303 - val_loss: 10.8545\nEpoch 150/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.6098 - val_loss: 10.8258\nEpoch 151/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.5886 - val_loss: 10.7982\nEpoch 152/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.5681 - val_loss: 10.7702\nEpoch 153/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.5477 - val_loss: 10.7426\nEpoch 154/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.5268 - val_loss: 10.7159\nEpoch 155/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.5070 - val_loss: 10.6879\nEpoch 156/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.4865 - val_loss: 10.6615\nEpoch 157/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.4665 - val_loss: 10.6347\nEpoch 158/2000\n6/6 [==============================] - 0s 4ms/step - loss: 2.4471 - val_loss: 10.6074\nEpoch 159/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.4269 - val_loss: 10.5801\nEpoch 160/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.4074 - val_loss: 10.5535\nEpoch 161/2000\n6/6 [==============================] - 0s 4ms/step - loss: 2.3880 - val_loss: 10.5256\nEpoch 162/2000\n6/6 [==============================] - 0s 4ms/step - loss: 2.3687 - val_loss: 10.4981\nEpoch 163/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.3497 - val_loss: 10.4711\nEpoch 164/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.3308 - val_loss: 10.4447\nEpoch 165/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.3117 - val_loss: 10.4198\nEpoch 166/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.2933 - val_loss: 10.3949\nEpoch 167/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.2749 - val_loss: 10.3696\nEpoch 168/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.2567 - val_loss: 10.3437\nEpoch 169/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.2385 - val_loss: 10.3177\nEpoch 170/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.2204 - val_loss: 10.2909\nEpoch 171/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.2022 - val_loss: 10.2653\nEpoch 172/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.1842 - val_loss: 10.2391\nEpoch 173/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.1669 - val_loss: 10.2139\nEpoch 174/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.1493 - val_loss: 10.1893\nEpoch 175/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.1320 - val_loss: 10.1643\nEpoch 176/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.1146 - val_loss: 10.1393\nEpoch 177/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.0978 - val_loss: 10.1148\nEpoch 178/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.0802 - val_loss: 10.0900\nEpoch 179/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.0637 - val_loss: 10.0662\nEpoch 180/2000\n6/6 [==============================] - 0s 4ms/step - loss: 2.0470 - val_loss: 10.0430\nEpoch 181/2000\n6/6 [==============================] - 0s 4ms/step - loss: 2.0304 - val_loss: 10.0192\nEpoch 182/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.0140 - val_loss: 9.9947\nEpoch 183/2000\n6/6 [==============================] - 0s 4ms/step - loss: 1.9974 - val_loss: 9.9698\nEpoch 184/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.9812 - val_loss: 9.9468\nEpoch 185/2000\n6/6 [==============================] - 0s 4ms/step - loss: 1.9653 - val_loss: 9.9227\nEpoch 186/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.9492 - val_loss: 9.8996\nEpoch 187/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.9333 - val_loss: 9.8767\nEpoch 188/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.9175 - val_loss: 9.8542\nEpoch 189/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.9018 - val_loss: 9.8322\nEpoch 190/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.8864 - val_loss: 9.8097\nEpoch 191/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.8710 - val_loss: 9.7886\nEpoch 192/2000\n6/6 [==============================] - 0s 4ms/step - loss: 1.8560 - val_loss: 9.7661\nEpoch 193/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.8408 - val_loss: 9.7440\nEpoch 194/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.8261 - val_loss: 9.7216\nEpoch 195/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.8109 - val_loss: 9.6998\nEpoch 196/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.7963 - val_loss: 9.6770\nEpoch 197/2000\n6/6 [==============================] - 0s 4ms/step - loss: 1.7817 - val_loss: 9.6547\nEpoch 198/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.7671 - val_loss: 9.6326\nEpoch 199/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.7529 - val_loss: 9.6101\nEpoch 200/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.7385 - val_loss: 9.5889\nEpoch 201/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.7247 - val_loss: 9.5673\nEpoch 202/2000\n6/6 [==============================] - 0s 4ms/step - loss: 1.7103 - val_loss: 9.5462\nEpoch 203/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.6965 - val_loss: 9.5249\nEpoch 204/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.6832 - val_loss: 9.5034\nEpoch 205/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.6688 - val_loss: 9.4818\nEpoch 206/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.6554 - val_loss: 9.4604\nEpoch 207/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.6420 - val_loss: 9.4388\nEpoch 208/2000\n6/6 [==============================] - 0s 4ms/step - loss: 1.6284 - val_loss: 9.4175\nEpoch 209/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.6153 - val_loss: 9.3953\nEpoch 210/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.6020 - val_loss: 9.3753\nEpoch 211/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.5890 - val_loss: 9.3550\nEpoch 212/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.5760 - val_loss: 9.3347\nEpoch 213/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.5630 - val_loss: 9.3146\nEpoch 214/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.5504 - val_loss: 9.2938\nEpoch 215/2000\n6/6 [==============================] - 0s 4ms/step - loss: 1.5378 - val_loss: 9.2736\nEpoch 216/2000\n6/6 [==============================] - 0s 4ms/step - loss: 1.5254 - val_loss: 9.2528\nEpoch 217/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.5129 - val_loss: 9.2324\nEpoch 218/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.5007 - val_loss: 9.2127\nEpoch 219/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.4883 - val_loss: 9.1932\nEpoch 220/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.4761 - val_loss: 9.1740\nEpoch 221/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.4642 - val_loss: 9.1550\nEpoch 222/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.4522 - val_loss: 9.1361\nEpoch 223/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.4407 - val_loss: 9.1162\nEpoch 224/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.4289 - val_loss: 9.0964\nEpoch 225/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.4172 - val_loss: 9.0773\nEpoch 226/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.4058 - val_loss: 9.0589\nEpoch 227/2000\n6/6 [==============================] - 0s 4ms/step - loss: 1.3944 - val_loss: 9.0388\nEpoch 228/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.3833 - val_loss: 9.0196\nEpoch 229/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.3721 - val_loss: 9.0016\nEpoch 230/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.3607 - val_loss: 8.9826\nEpoch 231/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.3498 - val_loss: 8.9648\nEpoch 232/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.3390 - val_loss: 8.9451\nEpoch 233/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.3282 - val_loss: 8.9263\nEpoch 234/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.3173 - val_loss: 8.9072\nEpoch 235/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.3065 - val_loss: 8.8890\nEpoch 236/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.2960 - val_loss: 8.8710\nEpoch 237/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.2857 - val_loss: 8.8527\nEpoch 238/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.2753 - val_loss: 8.8343\nEpoch 239/2000\n6/6 [==============================] - 0s 2ms/step - loss: 1.2650 - val_loss: 8.8167\nEpoch 240/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.2549 - val_loss: 8.7986\nEpoch 241/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.2446 - val_loss: 8.7807\nEpoch 242/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.2348 - val_loss: 8.7634\nEpoch 243/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.2248 - val_loss: 8.7452\nEpoch 244/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.2149 - val_loss: 8.7272\nEpoch 245/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.2051 - val_loss: 8.7098\nEpoch 246/2000\n6/6 [==============================] - 0s 4ms/step - loss: 1.1953 - val_loss: 8.6919\nEpoch 247/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.1859 - val_loss: 8.6734\nEpoch 248/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.1761 - val_loss: 8.6563\nEpoch 249/2000\n6/6 [==============================] - 0s 4ms/step - loss: 1.1668 - val_loss: 8.6384\nEpoch 250/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.1575 - val_loss: 8.6211\nEpoch 251/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.1482 - val_loss: 8.6034\nEpoch 252/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.1391 - val_loss: 8.5858\nEpoch 253/2000\n6/6 [==============================] - 0s 4ms/step - loss: 1.1299 - val_loss: 8.5696\nEpoch 254/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.1210 - val_loss: 8.5526\nEpoch 255/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.1121 - val_loss: 8.5356\nEpoch 256/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.1033 - val_loss: 8.5179\nEpoch 257/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0945 - val_loss: 8.5020\nEpoch 258/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0857 - val_loss: 8.4856\nEpoch 259/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0772 - val_loss: 8.4692\nEpoch 260/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0686 - val_loss: 8.4531\nEpoch 261/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0601 - val_loss: 8.4360\nEpoch 262/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0518 - val_loss: 8.4194\nEpoch 263/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0433 - val_loss: 8.4037\nEpoch 264/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0351 - val_loss: 8.3865\nEpoch 265/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0271 - val_loss: 8.3690\nEpoch 266/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0190 - val_loss: 8.3524\nEpoch 267/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0108 - val_loss: 8.3361\nEpoch 268/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0029 - val_loss: 8.3203\nEpoch 269/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.9951 - val_loss: 8.3041\nEpoch 270/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9873 - val_loss: 8.2878\nEpoch 271/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9796 - val_loss: 8.2707\nEpoch 272/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9719 - val_loss: 8.2545\nEpoch 273/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.9642 - val_loss: 8.2390\nEpoch 274/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.9568 - val_loss: 8.2218\nEpoch 275/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9494 - val_loss: 8.2058\nEpoch 276/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9420 - val_loss: 8.1896\nEpoch 277/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9347 - val_loss: 8.1732\nEpoch 278/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9275 - val_loss: 8.1561\nEpoch 279/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9202 - val_loss: 8.1401\nEpoch 280/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9131 - val_loss: 8.1239\nEpoch 281/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9060 - val_loss: 8.1088\nEpoch 282/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8990 - val_loss: 8.0931\nEpoch 283/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8922 - val_loss: 8.0769\nEpoch 284/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8853 - val_loss: 8.0603\nEpoch 285/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8784 - val_loss: 8.0448\nEpoch 286/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8718 - val_loss: 8.0294\nEpoch 287/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8652 - val_loss: 8.0138\nEpoch 288/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8584 - val_loss: 7.9985\nEpoch 289/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8520 - val_loss: 7.9826\nEpoch 290/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.8454 - val_loss: 7.9671\nEpoch 291/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.8390 - val_loss: 7.9519\nEpoch 292/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8326 - val_loss: 7.9362\nEpoch 293/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8263 - val_loss: 7.9212\nEpoch 294/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8201 - val_loss: 7.9061\nEpoch 295/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8138 - val_loss: 7.8905\nEpoch 296/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8079 - val_loss: 7.8750\nEpoch 297/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8017 - val_loss: 7.8607\nEpoch 298/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7958 - val_loss: 7.8459\nEpoch 299/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7899 - val_loss: 7.8306\nEpoch 300/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7840 - val_loss: 7.8159\nEpoch 301/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7783 - val_loss: 7.8007\nEpoch 302/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7725 - val_loss: 7.7864\nEpoch 303/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.7669 - val_loss: 7.7719\nEpoch 304/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7611 - val_loss: 7.7579\nEpoch 305/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7556 - val_loss: 7.7428\nEpoch 306/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7500 - val_loss: 7.7285\nEpoch 307/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7445 - val_loss: 7.7141\nEpoch 308/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7391 - val_loss: 7.6997\nEpoch 309/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7337 - val_loss: 7.6853\nEpoch 310/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7283 - val_loss: 7.6718\nEpoch 311/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7231 - val_loss: 7.6580\nEpoch 312/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7179 - val_loss: 7.6447\nEpoch 313/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7128 - val_loss: 7.6302\nEpoch 314/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7077 - val_loss: 7.6174\nEpoch 315/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7026 - val_loss: 7.6038\nEpoch 316/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6975 - val_loss: 7.5901\nEpoch 317/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6925 - val_loss: 7.5768\nEpoch 318/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6876 - val_loss: 7.5627\nEpoch 319/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6827 - val_loss: 7.5483\nEpoch 320/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6779 - val_loss: 7.5341\nEpoch 321/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6731 - val_loss: 7.5198\nEpoch 322/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6683 - val_loss: 7.5051\nEpoch 323/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6636 - val_loss: 7.4905\nEpoch 324/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6588 - val_loss: 7.4768\nEpoch 325/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6543 - val_loss: 7.4629\nEpoch 326/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6497 - val_loss: 7.4488\nEpoch 327/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6452 - val_loss: 7.4349\nEpoch 328/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6407 - val_loss: 7.4209\nEpoch 329/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6361 - val_loss: 7.4075\nEpoch 330/2000\n6/6 [==============================] - 0s 2ms/step - loss: 0.6318 - val_loss: 7.3936\nEpoch 331/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6273 - val_loss: 7.3804\nEpoch 332/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6230 - val_loss: 7.3673\nEpoch 333/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6187 - val_loss: 7.3541\nEpoch 334/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6146 - val_loss: 7.3411\nEpoch 335/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6104 - val_loss: 7.3282\nEpoch 336/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6063 - val_loss: 7.3151\nEpoch 337/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.6021 - val_loss: 7.3019\nEpoch 338/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5982 - val_loss: 7.2878\nEpoch 339/2000\n6/6 [==============================] - 0s 2ms/step - loss: 0.5941 - val_loss: 7.2744\nEpoch 340/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5901 - val_loss: 7.2609\nEpoch 341/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5862 - val_loss: 7.2473\nEpoch 342/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5822 - val_loss: 7.2341\nEpoch 343/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5785 - val_loss: 7.2205\nEpoch 344/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5746 - val_loss: 7.2082\nEpoch 345/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5709 - val_loss: 7.1954\nEpoch 346/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5672 - val_loss: 7.1823\nEpoch 347/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5635 - val_loss: 7.1690\nEpoch 348/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5599 - val_loss: 7.1551\nEpoch 349/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5561 - val_loss: 7.1426\nEpoch 350/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5527 - val_loss: 7.1290\nEpoch 351/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5491 - val_loss: 7.1156\nEpoch 352/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5456 - val_loss: 7.1031\nEpoch 353/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.5421 - val_loss: 7.0905\nEpoch 354/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5387 - val_loss: 7.0773\nEpoch 355/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5352 - val_loss: 7.0639\nEpoch 356/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5319 - val_loss: 7.0513\nEpoch 357/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5285 - val_loss: 7.0387\nEpoch 358/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.5251 - val_loss: 7.0263\nEpoch 359/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5218 - val_loss: 7.0130\nEpoch 360/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5186 - val_loss: 7.0000\nEpoch 361/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.5154 - val_loss: 6.9864\nEpoch 362/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5121 - val_loss: 6.9732\nEpoch 363/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5090 - val_loss: 6.9603\nEpoch 364/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5057 - val_loss: 6.9482\nEpoch 365/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5027 - val_loss: 6.9357\nEpoch 366/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4997 - val_loss: 6.9228\nEpoch 367/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4966 - val_loss: 6.9100\nEpoch 368/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4936 - val_loss: 6.8972\nEpoch 369/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4906 - val_loss: 6.8840\nEpoch 370/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.4876 - val_loss: 6.8711\nEpoch 371/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4847 - val_loss: 6.8580\nEpoch 372/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.4818 - val_loss: 6.8453\nEpoch 373/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.4790 - val_loss: 6.8319\nEpoch 374/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4760 - val_loss: 6.8195\nEpoch 375/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4732 - val_loss: 6.8068\nEpoch 376/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4704 - val_loss: 6.7937\nEpoch 377/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4676 - val_loss: 6.7809\nEpoch 378/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4649 - val_loss: 6.7677\nEpoch 379/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4622 - val_loss: 6.7545\nEpoch 380/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4595 - val_loss: 6.7420\nEpoch 381/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4569 - val_loss: 6.7292\nEpoch 382/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4543 - val_loss: 6.7164\nEpoch 383/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4517 - val_loss: 6.7039\nEpoch 384/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4492 - val_loss: 6.6910\nEpoch 385/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.4467 - val_loss: 6.6781\nEpoch 386/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4441 - val_loss: 6.6659\nEpoch 387/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4416 - val_loss: 6.6535\nEpoch 388/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4392 - val_loss: 6.6404\nEpoch 389/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4367 - val_loss: 6.6274\nEpoch 390/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.4342 - val_loss: 6.6145\nEpoch 391/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4318 - val_loss: 6.6011\nEpoch 392/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4295 - val_loss: 6.5875\nEpoch 393/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4270 - val_loss: 6.5747\nEpoch 394/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4248 - val_loss: 6.5615\nEpoch 395/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4224 - val_loss: 6.5482\nEpoch 396/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4202 - val_loss: 6.5350\nEpoch 397/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4178 - val_loss: 6.5221\nEpoch 398/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4156 - val_loss: 6.5091\nEpoch 399/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4134 - val_loss: 6.4958\nEpoch 400/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4112 - val_loss: 6.4832\nEpoch 401/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4090 - val_loss: 6.4701\nEpoch 402/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4068 - val_loss: 6.4580\nEpoch 403/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.4047 - val_loss: 6.4458\nEpoch 404/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4026 - val_loss: 6.4328\nEpoch 405/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4005 - val_loss: 6.4201\nEpoch 406/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3984 - val_loss: 6.4078\nEpoch 407/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3964 - val_loss: 6.3951\nEpoch 408/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3943 - val_loss: 6.3820\nEpoch 409/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3923 - val_loss: 6.3687\nEpoch 410/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3904 - val_loss: 6.3556\nEpoch 411/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3884 - val_loss: 6.3432\nEpoch 412/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3864 - val_loss: 6.3316\nEpoch 413/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.3845 - val_loss: 6.3191\nEpoch 414/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.3826 - val_loss: 6.3064\nEpoch 415/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3807 - val_loss: 6.2937\nEpoch 416/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3788 - val_loss: 6.2801\nEpoch 417/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3770 - val_loss: 6.2673\nEpoch 418/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3751 - val_loss: 6.2552\nEpoch 419/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3734 - val_loss: 6.2428\nEpoch 420/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3716 - val_loss: 6.2297\nEpoch 421/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3697 - val_loss: 6.2176\nEpoch 422/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3680 - val_loss: 6.2058\nEpoch 423/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3662 - val_loss: 6.1930\nEpoch 424/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3645 - val_loss: 6.1798\nEpoch 425/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3628 - val_loss: 6.1656\nEpoch 426/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3611 - val_loss: 6.1524\nEpoch 427/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3593 - val_loss: 6.1409\nEpoch 428/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3577 - val_loss: 6.1283\nEpoch 429/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3560 - val_loss: 6.1157\nEpoch 430/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3543 - val_loss: 6.1029\nEpoch 431/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3527 - val_loss: 6.0910\nEpoch 432/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3511 - val_loss: 6.0784\nEpoch 433/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3495 - val_loss: 6.0659\nEpoch 434/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3479 - val_loss: 6.0532\nEpoch 435/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3463 - val_loss: 6.0409\nEpoch 436/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3449 - val_loss: 6.0271\nEpoch 437/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3433 - val_loss: 6.0139\nEpoch 438/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.3417 - val_loss: 6.0021\nEpoch 439/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3403 - val_loss: 5.9891\nEpoch 440/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3387 - val_loss: 5.9761\nEpoch 441/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.3373 - val_loss: 5.9631\nEpoch 442/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3358 - val_loss: 5.9510\nEpoch 443/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3343 - val_loss: 5.9384\nEpoch 444/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3329 - val_loss: 5.9251\nEpoch 445/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3315 - val_loss: 5.9124\nEpoch 446/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3300 - val_loss: 5.9006\nEpoch 447/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.3286 - val_loss: 5.8884\nEpoch 448/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.3273 - val_loss: 5.8753\nEpoch 449/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3259 - val_loss: 5.8638\nEpoch 450/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3246 - val_loss: 5.8502\nEpoch 451/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3232 - val_loss: 5.8370\nEpoch 452/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3219 - val_loss: 5.8248\nEpoch 453/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3205 - val_loss: 5.8125\nEpoch 454/2000\n6/6 [==============================] - 0s 2ms/step - loss: 0.3192 - val_loss: 5.8001\nEpoch 455/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3179 - val_loss: 5.7872\nEpoch 456/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3166 - val_loss: 5.7746\nEpoch 457/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.3153 - val_loss: 5.7613\nEpoch 458/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3141 - val_loss: 5.7485\nEpoch 459/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3128 - val_loss: 5.7363\nEpoch 460/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3115 - val_loss: 5.7242\nEpoch 461/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3102 - val_loss: 5.7113\nEpoch 462/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.3091 - val_loss: 5.6971\nEpoch 463/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3078 - val_loss: 5.6843\nEpoch 464/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3066 - val_loss: 5.6719\nEpoch 465/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3054 - val_loss: 5.6590\nEpoch 466/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.3042 - val_loss: 5.6457\nEpoch 467/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3030 - val_loss: 5.6332\nEpoch 468/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3018 - val_loss: 5.6205\nEpoch 469/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3007 - val_loss: 5.6081\nEpoch 470/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2995 - val_loss: 5.5948\nEpoch 471/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2984 - val_loss: 5.5822\nEpoch 472/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2973 - val_loss: 5.5698\nEpoch 473/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2962 - val_loss: 5.5573\nEpoch 474/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2950 - val_loss: 5.5454\nEpoch 475/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2939 - val_loss: 5.5330\nEpoch 476/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2929 - val_loss: 5.5206\nEpoch 477/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2918 - val_loss: 5.5077\nEpoch 478/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2907 - val_loss: 5.4955\nEpoch 479/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2897 - val_loss: 5.4826\nEpoch 480/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2886 - val_loss: 5.4703\nEpoch 481/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2875 - val_loss: 5.4572\nEpoch 482/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2865 - val_loss: 5.4437\nEpoch 483/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2854 - val_loss: 5.4312\nEpoch 484/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2844 - val_loss: 5.4180\nEpoch 485/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2833 - val_loss: 5.4055\nEpoch 486/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2823 - val_loss: 5.3928\nEpoch 487/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2812 - val_loss: 5.3801\nEpoch 488/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2803 - val_loss: 5.3680\nEpoch 489/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2793 - val_loss: 5.3553\nEpoch 490/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2784 - val_loss: 5.3432\nEpoch 491/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2774 - val_loss: 5.3311\nEpoch 492/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2764 - val_loss: 5.3187\nEpoch 493/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2755 - val_loss: 5.3060\nEpoch 494/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2745 - val_loss: 5.2937\nEpoch 495/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2735 - val_loss: 5.2808\nEpoch 496/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2726 - val_loss: 5.2676\nEpoch 497/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2717 - val_loss: 5.2543\nEpoch 498/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2707 - val_loss: 5.2419\nEpoch 499/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2697 - val_loss: 5.2294\nEpoch 500/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2688 - val_loss: 5.2162\nEpoch 501/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2679 - val_loss: 5.2035\nEpoch 502/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2670 - val_loss: 5.1906\nEpoch 503/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2661 - val_loss: 5.1787\nEpoch 504/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2653 - val_loss: 5.1656\nEpoch 505/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2643 - val_loss: 5.1536\nEpoch 506/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2635 - val_loss: 5.1414\nEpoch 507/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2626 - val_loss: 5.1286\nEpoch 508/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2618 - val_loss: 5.1159\nEpoch 509/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2609 - val_loss: 5.1031\nEpoch 510/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2600 - val_loss: 5.0902\nEpoch 511/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2592 - val_loss: 5.0771\nEpoch 512/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2583 - val_loss: 5.0634\nEpoch 513/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2574 - val_loss: 5.0508\nEpoch 514/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2566 - val_loss: 5.0378\nEpoch 515/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2558 - val_loss: 5.0251\nEpoch 516/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2549 - val_loss: 5.0124\nEpoch 517/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2541 - val_loss: 5.0009\nEpoch 518/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2533 - val_loss: 4.9884\nEpoch 519/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2524 - val_loss: 4.9745\nEpoch 520/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2516 - val_loss: 4.9611\nEpoch 521/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2508 - val_loss: 4.9477\nEpoch 522/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2500 - val_loss: 4.9343\nEpoch 523/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2492 - val_loss: 4.9215\nEpoch 524/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2483 - val_loss: 4.9094\nEpoch 525/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2476 - val_loss: 4.8964\nEpoch 526/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2468 - val_loss: 4.8839\nEpoch 527/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2460 - val_loss: 4.8708\nEpoch 528/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2452 - val_loss: 4.8576\nEpoch 529/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2444 - val_loss: 4.8448\nEpoch 530/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2437 - val_loss: 4.8320\nEpoch 531/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2429 - val_loss: 4.8194\nEpoch 532/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2421 - val_loss: 4.8058\nEpoch 533/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2413 - val_loss: 4.7929\nEpoch 534/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2406 - val_loss: 4.7801\nEpoch 535/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2398 - val_loss: 4.7671\nEpoch 536/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2390 - val_loss: 4.7535\nEpoch 537/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2382 - val_loss: 4.7403\nEpoch 538/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2375 - val_loss: 4.7273\nEpoch 539/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2367 - val_loss: 4.7137\nEpoch 540/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2360 - val_loss: 4.7006\nEpoch 541/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2352 - val_loss: 4.6876\nEpoch 542/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2345 - val_loss: 4.6743\nEpoch 543/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2337 - val_loss: 4.6610\nEpoch 544/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2330 - val_loss: 4.6474\nEpoch 545/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2323 - val_loss: 4.6331\nEpoch 546/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2316 - val_loss: 4.6199\nEpoch 547/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2308 - val_loss: 4.6071\nEpoch 548/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2301 - val_loss: 4.5937\nEpoch 549/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2294 - val_loss: 4.5801\nEpoch 550/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2287 - val_loss: 4.5669\nEpoch 551/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2280 - val_loss: 4.5541\nEpoch 552/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2273 - val_loss: 4.5418\nEpoch 553/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2266 - val_loss: 4.5291\nEpoch 554/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2259 - val_loss: 4.5153\nEpoch 555/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2252 - val_loss: 4.5024\nEpoch 556/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2245 - val_loss: 4.4886\nEpoch 557/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2238 - val_loss: 4.4760\nEpoch 558/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2231 - val_loss: 4.4632\nEpoch 559/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2225 - val_loss: 4.4498\nEpoch 560/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2218 - val_loss: 4.4371\nEpoch 561/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2211 - val_loss: 4.4243\nEpoch 562/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2205 - val_loss: 4.4102\nEpoch 563/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2198 - val_loss: 4.3965\nEpoch 564/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2191 - val_loss: 4.3836\nEpoch 565/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2184 - val_loss: 4.3712\nEpoch 566/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2178 - val_loss: 4.3576\nEpoch 567/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2171 - val_loss: 4.3441\nEpoch 568/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2164 - val_loss: 4.3317\nEpoch 569/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2158 - val_loss: 4.3184\nEpoch 570/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2151 - val_loss: 4.3051\nEpoch 571/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2145 - val_loss: 4.2917\nEpoch 572/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2138 - val_loss: 4.2786\nEpoch 573/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2132 - val_loss: 4.2653\nEpoch 574/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2125 - val_loss: 4.2523\nEpoch 575/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2119 - val_loss: 4.2398\nEpoch 576/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2112 - val_loss: 4.2273\nEpoch 577/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2107 - val_loss: 4.2141\nEpoch 578/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2100 - val_loss: 4.2012\nEpoch 579/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2094 - val_loss: 4.1880\nEpoch 580/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2087 - val_loss: 4.1755\nEpoch 581/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2081 - val_loss: 4.1625\nEpoch 582/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2075 - val_loss: 4.1492\nEpoch 583/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2068 - val_loss: 4.1359\nEpoch 584/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2062 - val_loss: 4.1222\nEpoch 585/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2055 - val_loss: 4.1091\nEpoch 586/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2050 - val_loss: 4.0958\nEpoch 587/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2043 - val_loss: 4.0840\nEpoch 588/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2037 - val_loss: 4.0713\nEpoch 589/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2031 - val_loss: 4.0589\nEpoch 590/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2025 - val_loss: 4.0462\nEpoch 591/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2018 - val_loss: 4.0341\nEpoch 592/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2013 - val_loss: 4.0212\nEpoch 593/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2007 - val_loss: 4.0088\nEpoch 594/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2001 - val_loss: 3.9966\nEpoch 595/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1995 - val_loss: 3.9836\nEpoch 596/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1989 - val_loss: 3.9706\nEpoch 597/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1982 - val_loss: 3.9582\nEpoch 598/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1977 - val_loss: 3.9448\nEpoch 599/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1971 - val_loss: 3.9314\nEpoch 600/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1964 - val_loss: 3.9187\nEpoch 601/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1959 - val_loss: 3.9060\nEpoch 602/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1953 - val_loss: 3.8930\nEpoch 603/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1947 - val_loss: 3.8804\nEpoch 604/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1941 - val_loss: 3.8667\nEpoch 605/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1935 - val_loss: 3.8544\nEpoch 606/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1929 - val_loss: 3.8421\nEpoch 607/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1924 - val_loss: 3.8289\nEpoch 608/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1917 - val_loss: 3.8162\nEpoch 609/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1912 - val_loss: 3.8026\nEpoch 610/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1906 - val_loss: 3.7897\nEpoch 611/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1900 - val_loss: 3.7766\nEpoch 612/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1894 - val_loss: 3.7639\nEpoch 613/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1888 - val_loss: 3.7507\nEpoch 614/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1882 - val_loss: 3.7374\nEpoch 615/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1877 - val_loss: 3.7244\nEpoch 616/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1871 - val_loss: 3.7116\nEpoch 617/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1865 - val_loss: 3.6990\nEpoch 618/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1859 - val_loss: 3.6861\nEpoch 619/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1854 - val_loss: 3.6728\nEpoch 620/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1847 - val_loss: 3.6605\nEpoch 621/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1842 - val_loss: 3.6471\nEpoch 622/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1836 - val_loss: 3.6345\nEpoch 623/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1830 - val_loss: 3.6220\nEpoch 624/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1825 - val_loss: 3.6088\nEpoch 625/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1819 - val_loss: 3.5953\nEpoch 626/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1813 - val_loss: 3.5819\nEpoch 627/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1807 - val_loss: 3.5693\nEpoch 628/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1802 - val_loss: 3.5565\nEpoch 629/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1796 - val_loss: 3.5439\nEpoch 630/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1791 - val_loss: 3.5319\nEpoch 631/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1785 - val_loss: 3.5202\nEpoch 632/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1780 - val_loss: 3.5072\nEpoch 633/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1774 - val_loss: 3.4947\nEpoch 634/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1768 - val_loss: 3.4815\nEpoch 635/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1763 - val_loss: 3.4685\nEpoch 636/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1757 - val_loss: 3.4563\nEpoch 637/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1751 - val_loss: 3.4440\nEpoch 638/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1746 - val_loss: 3.4307\nEpoch 639/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1741 - val_loss: 3.4180\nEpoch 640/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1735 - val_loss: 3.4056\nEpoch 641/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1729 - val_loss: 3.3936\nEpoch 642/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1724 - val_loss: 3.3813\nEpoch 643/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1719 - val_loss: 3.3682\nEpoch 644/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1713 - val_loss: 3.3556\nEpoch 645/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1708 - val_loss: 3.3429\nEpoch 646/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1702 - val_loss: 3.3306\nEpoch 647/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1697 - val_loss: 3.3180\nEpoch 648/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1691 - val_loss: 3.3060\nEpoch 649/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1686 - val_loss: 3.2933\nEpoch 650/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1681 - val_loss: 3.2807\nEpoch 651/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1675 - val_loss: 3.2675\nEpoch 652/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1670 - val_loss: 3.2545\nEpoch 653/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1664 - val_loss: 3.2423\nEpoch 654/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1658 - val_loss: 3.2296\nEpoch 655/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1653 - val_loss: 3.2170\nEpoch 656/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1648 - val_loss: 3.2043\nEpoch 657/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1642 - val_loss: 3.1921\nEpoch 658/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1637 - val_loss: 3.1786\nEpoch 659/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1631 - val_loss: 3.1659\nEpoch 660/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1626 - val_loss: 3.1534\nEpoch 661/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1621 - val_loss: 3.1400\nEpoch 662/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1615 - val_loss: 3.1277\nEpoch 663/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1610 - val_loss: 3.1148\nEpoch 664/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1604 - val_loss: 3.1015\nEpoch 665/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1599 - val_loss: 3.0887\nEpoch 666/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1593 - val_loss: 3.0764\nEpoch 667/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1588 - val_loss: 3.0640\nEpoch 668/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1583 - val_loss: 3.0516\nEpoch 669/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1578 - val_loss: 3.0398\nEpoch 670/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1572 - val_loss: 3.0274\nEpoch 671/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1567 - val_loss: 3.0159\nEpoch 672/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1562 - val_loss: 3.0039\nEpoch 673/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1557 - val_loss: 2.9921\nEpoch 674/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1551 - val_loss: 2.9799\nEpoch 675/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1546 - val_loss: 2.9669\nEpoch 676/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1540 - val_loss: 2.9550\nEpoch 677/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1535 - val_loss: 2.9428\nEpoch 678/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1530 - val_loss: 2.9305\nEpoch 679/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1525 - val_loss: 2.9178\nEpoch 680/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1519 - val_loss: 2.9059\nEpoch 681/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1514 - val_loss: 2.8935\nEpoch 682/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1509 - val_loss: 2.8816\nEpoch 683/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1504 - val_loss: 2.8687\nEpoch 684/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1499 - val_loss: 2.8565\nEpoch 685/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1494 - val_loss: 2.8445\nEpoch 686/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1489 - val_loss: 2.8318\nEpoch 687/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1484 - val_loss: 2.8200\nEpoch 688/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1479 - val_loss: 2.8081\nEpoch 689/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1474 - val_loss: 2.7957\nEpoch 690/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1468 - val_loss: 2.7837\nEpoch 691/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1463 - val_loss: 2.7715\nEpoch 692/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1458 - val_loss: 2.7596\nEpoch 693/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1453 - val_loss: 2.7476\nEpoch 694/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1448 - val_loss: 2.7354\nEpoch 695/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1443 - val_loss: 2.7233\nEpoch 696/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1438 - val_loss: 2.7113\nEpoch 697/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1433 - val_loss: 2.6996\nEpoch 698/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1428 - val_loss: 2.6874\nEpoch 699/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1423 - val_loss: 2.6760\nEpoch 700/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1418 - val_loss: 2.6651\nEpoch 701/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1413 - val_loss: 2.6537\nEpoch 702/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1408 - val_loss: 2.6435\nEpoch 703/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1403 - val_loss: 2.6322\nEpoch 704/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1398 - val_loss: 2.6204\nEpoch 705/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1394 - val_loss: 2.6084\nEpoch 706/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1389 - val_loss: 2.5964\nEpoch 707/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1384 - val_loss: 2.5844\nEpoch 708/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1378 - val_loss: 2.5730\nEpoch 709/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1374 - val_loss: 2.5604\nEpoch 710/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1369 - val_loss: 2.5488\nEpoch 711/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1364 - val_loss: 2.5374\nEpoch 712/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1359 - val_loss: 2.5266\nEpoch 713/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1354 - val_loss: 2.5150\nEpoch 714/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1350 - val_loss: 2.5034\nEpoch 715/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1345 - val_loss: 2.4924\nEpoch 716/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1340 - val_loss: 2.4817\nEpoch 717/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1335 - val_loss: 2.4699\nEpoch 718/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1331 - val_loss: 2.4581\nEpoch 719/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1326 - val_loss: 2.4463\nEpoch 720/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1321 - val_loss: 2.4342\nEpoch 721/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1316 - val_loss: 2.4222\nEpoch 722/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1311 - val_loss: 2.4105\nEpoch 723/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1306 - val_loss: 2.3996\nEpoch 724/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1302 - val_loss: 2.3888\nEpoch 725/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1297 - val_loss: 2.3774\nEpoch 726/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1292 - val_loss: 2.3659\nEpoch 727/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1287 - val_loss: 2.3542\nEpoch 728/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1283 - val_loss: 2.3425\nEpoch 729/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1278 - val_loss: 2.3306\nEpoch 730/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1273 - val_loss: 2.3193\nEpoch 731/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1269 - val_loss: 2.3079\nEpoch 732/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1264 - val_loss: 2.2972\nEpoch 733/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1260 - val_loss: 2.2863\nEpoch 734/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1255 - val_loss: 2.2755\nEpoch 735/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1250 - val_loss: 2.2650\nEpoch 736/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1246 - val_loss: 2.2541\nEpoch 737/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1241 - val_loss: 2.2434\nEpoch 738/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1236 - val_loss: 2.2326\nEpoch 739/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1232 - val_loss: 2.2211\nEpoch 740/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1227 - val_loss: 2.2096\nEpoch 741/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1223 - val_loss: 2.1982\nEpoch 742/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1218 - val_loss: 2.1873\nEpoch 743/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1213 - val_loss: 2.1772\nEpoch 744/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1209 - val_loss: 2.1657\nEpoch 745/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1204 - val_loss: 2.1546\nEpoch 746/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1200 - val_loss: 2.1438\nEpoch 747/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1195 - val_loss: 2.1334\nEpoch 748/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1191 - val_loss: 2.1226\nEpoch 749/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1186 - val_loss: 2.1122\nEpoch 750/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1181 - val_loss: 2.1021\nEpoch 751/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1177 - val_loss: 2.0914\nEpoch 752/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1173 - val_loss: 2.0800\nEpoch 753/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1168 - val_loss: 2.0693\nEpoch 754/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1164 - val_loss: 2.0587\nEpoch 755/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1159 - val_loss: 2.0479\nEpoch 756/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1154 - val_loss: 2.0373\nEpoch 757/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1150 - val_loss: 2.0264\nEpoch 758/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1146 - val_loss: 2.0153\nEpoch 759/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1141 - val_loss: 2.0050\nEpoch 760/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1137 - val_loss: 1.9949\nEpoch 761/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1133 - val_loss: 1.9837\nEpoch 762/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1128 - val_loss: 1.9731\nEpoch 763/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1124 - val_loss: 1.9618\nEpoch 764/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1119 - val_loss: 1.9511\nEpoch 765/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1115 - val_loss: 1.9405\nEpoch 766/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1111 - val_loss: 1.9295\nEpoch 767/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1106 - val_loss: 1.9198\nEpoch 768/2000\n6/6 [==============================] - 0s 2ms/step - loss: 0.1102 - val_loss: 1.9094\nEpoch 769/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1098 - val_loss: 1.8989\nEpoch 770/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1094 - val_loss: 1.8886\nEpoch 771/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1089 - val_loss: 1.8785\nEpoch 772/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1085 - val_loss: 1.8684\nEpoch 773/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1081 - val_loss: 1.8580\nEpoch 774/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1077 - val_loss: 1.8486\nEpoch 775/2000\n6/6 [==============================] - 0s 2ms/step - loss: 0.1073 - val_loss: 1.8389\nEpoch 776/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1069 - val_loss: 1.8292\nEpoch 777/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1065 - val_loss: 1.8192\nEpoch 778/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1061 - val_loss: 1.8095\nEpoch 779/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1057 - val_loss: 1.7997\nEpoch 780/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1052 - val_loss: 1.7901\nEpoch 781/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1049 - val_loss: 1.7800\nEpoch 782/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1044 - val_loss: 1.7697\nEpoch 783/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1040 - val_loss: 1.7598\nEpoch 784/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1036 - val_loss: 1.7504\nEpoch 785/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1032 - val_loss: 1.7409\nEpoch 786/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1028 - val_loss: 1.7314\nEpoch 787/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1024 - val_loss: 1.7218\nEpoch 788/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1020 - val_loss: 1.7124\nEpoch 789/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1016 - val_loss: 1.7032\nEpoch 790/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1013 - val_loss: 1.6925\nEpoch 791/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1009 - val_loss: 1.6821\nEpoch 792/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1005 - val_loss: 1.6722\nEpoch 793/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1001 - val_loss: 1.6626\nEpoch 794/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0997 - val_loss: 1.6531\nEpoch 795/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0993 - val_loss: 1.6433\nEpoch 796/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0989 - val_loss: 1.6334\nEpoch 797/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0985 - val_loss: 1.6241\nEpoch 798/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0981 - val_loss: 1.6152\nEpoch 799/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0978 - val_loss: 1.6056\nEpoch 800/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0974 - val_loss: 1.5965\nEpoch 801/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0970 - val_loss: 1.5872\nEpoch 802/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0967 - val_loss: 1.5778\nEpoch 803/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0962 - val_loss: 1.5689\nEpoch 804/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0959 - val_loss: 1.5599\nEpoch 805/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0955 - val_loss: 1.5510\nEpoch 806/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0951 - val_loss: 1.5423\nEpoch 807/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0947 - val_loss: 1.5337\nEpoch 808/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0944 - val_loss: 1.5249\nEpoch 809/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0940 - val_loss: 1.5160\nEpoch 810/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0936 - val_loss: 1.5071\nEpoch 811/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0933 - val_loss: 1.4979\nEpoch 812/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0929 - val_loss: 1.4890\nEpoch 813/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0925 - val_loss: 1.4799\nEpoch 814/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0921 - val_loss: 1.4709\nEpoch 815/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0918 - val_loss: 1.4622\nEpoch 816/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0914 - val_loss: 1.4533\nEpoch 817/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0910 - val_loss: 1.4441\nEpoch 818/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0907 - val_loss: 1.4352\nEpoch 819/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0903 - val_loss: 1.4268\nEpoch 820/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0899 - val_loss: 1.4182\nEpoch 821/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0896 - val_loss: 1.4094\nEpoch 822/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0893 - val_loss: 1.4007\nEpoch 823/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0889 - val_loss: 1.3927\nEpoch 824/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0885 - val_loss: 1.3842\nEpoch 825/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0882 - val_loss: 1.3756\nEpoch 826/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0879 - val_loss: 1.3669\nEpoch 827/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0875 - val_loss: 1.3588\nEpoch 828/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0871 - val_loss: 1.3501\nEpoch 829/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0868 - val_loss: 1.3413\nEpoch 830/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0864 - val_loss: 1.3330\nEpoch 831/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0861 - val_loss: 1.3249\nEpoch 832/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0858 - val_loss: 1.3168\nEpoch 833/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0854 - val_loss: 1.3080\nEpoch 834/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0851 - val_loss: 1.2990\nEpoch 835/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0847 - val_loss: 1.2905\nEpoch 836/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0844 - val_loss: 1.2821\nEpoch 837/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0840 - val_loss: 1.2737\nEpoch 838/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0837 - val_loss: 1.2656\nEpoch 839/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0834 - val_loss: 1.2577\nEpoch 840/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0831 - val_loss: 1.2491\nEpoch 841/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0827 - val_loss: 1.2417\nEpoch 842/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0824 - val_loss: 1.2343\nEpoch 843/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0820 - val_loss: 1.2269\nEpoch 844/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0817 - val_loss: 1.2187\nEpoch 845/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0814 - val_loss: 1.2108\nEpoch 846/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0811 - val_loss: 1.2027\nEpoch 847/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0807 - val_loss: 1.1946\nEpoch 848/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0804 - val_loss: 1.1866\nEpoch 849/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0801 - val_loss: 1.1785\nEpoch 850/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0797 - val_loss: 1.1710\nEpoch 851/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0794 - val_loss: 1.1633\nEpoch 852/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0791 - val_loss: 1.1551\nEpoch 853/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0788 - val_loss: 1.1476\nEpoch 854/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0785 - val_loss: 1.1401\nEpoch 855/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0782 - val_loss: 1.1325\nEpoch 856/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0779 - val_loss: 1.1250\nEpoch 857/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0776 - val_loss: 1.1173\nEpoch 858/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0773 - val_loss: 1.1094\nEpoch 859/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0769 - val_loss: 1.1020\nEpoch 860/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0767 - val_loss: 1.0947\nEpoch 861/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0763 - val_loss: 1.0881\nEpoch 862/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0760 - val_loss: 1.0809\nEpoch 863/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0757 - val_loss: 1.0733\nEpoch 864/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0754 - val_loss: 1.0658\nEpoch 865/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0751 - val_loss: 1.0587\nEpoch 866/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0748 - val_loss: 1.0514\nEpoch 867/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0745 - val_loss: 1.0443\nEpoch 868/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0742 - val_loss: 1.0366\nEpoch 869/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0740 - val_loss: 1.0293\nEpoch 870/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0736 - val_loss: 1.0222\nEpoch 871/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0733 - val_loss: 1.0156\nEpoch 872/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0731 - val_loss: 1.0084\nEpoch 873/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0728 - val_loss: 1.0014\nEpoch 874/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0725 - val_loss: 0.9942\nEpoch 875/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0722 - val_loss: 0.9875\nEpoch 876/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0719 - val_loss: 0.9811\nEpoch 877/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0716 - val_loss: 0.9742\nEpoch 878/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0714 - val_loss: 0.9676\nEpoch 879/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0711 - val_loss: 0.9613\nEpoch 880/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0708 - val_loss: 0.9546\nEpoch 881/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0705 - val_loss: 0.9475\nEpoch 882/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0702 - val_loss: 0.9403\nEpoch 883/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0700 - val_loss: 0.9331\nEpoch 884/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0697 - val_loss: 0.9261\nEpoch 885/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0694 - val_loss: 0.9197\nEpoch 886/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0691 - val_loss: 0.9135\nEpoch 887/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0689 - val_loss: 0.9066\nEpoch 888/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0686 - val_loss: 0.8993\nEpoch 889/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0683 - val_loss: 0.8924\nEpoch 890/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0681 - val_loss: 0.8855\nEpoch 891/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0678 - val_loss: 0.8796\nEpoch 892/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0675 - val_loss: 0.8733\nEpoch 893/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0673 - val_loss: 0.8668\nEpoch 894/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0670 - val_loss: 0.8608\nEpoch 895/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0668 - val_loss: 0.8547\nEpoch 896/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0665 - val_loss: 0.8485\nEpoch 897/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0663 - val_loss: 0.8425\nEpoch 898/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0660 - val_loss: 0.8362\nEpoch 899/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0658 - val_loss: 0.8303\nEpoch 900/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0655 - val_loss: 0.8244\nEpoch 901/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0653 - val_loss: 0.8186\nEpoch 902/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0651 - val_loss: 0.8127\nEpoch 903/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0648 - val_loss: 0.8066\nEpoch 904/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0646 - val_loss: 0.8011\nEpoch 905/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0643 - val_loss: 0.7951\nEpoch 906/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0641 - val_loss: 0.7896\nEpoch 907/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0639 - val_loss: 0.7837\nEpoch 908/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0636 - val_loss: 0.7782\nEpoch 909/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0634 - val_loss: 0.7722\nEpoch 910/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0632 - val_loss: 0.7668\nEpoch 911/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0629 - val_loss: 0.7612\nEpoch 912/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0627 - val_loss: 0.7556\nEpoch 913/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0625 - val_loss: 0.7500\nEpoch 914/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0623 - val_loss: 0.7444\nEpoch 915/2000\n6/6 [==============================] - 0s 2ms/step - loss: 0.0620 - val_loss: 0.7387\nEpoch 916/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0618 - val_loss: 0.7329\nEpoch 917/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0616 - val_loss: 0.7272\nEpoch 918/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0614 - val_loss: 0.7216\nEpoch 919/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0611 - val_loss: 0.7163\nEpoch 920/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0609 - val_loss: 0.7108\nEpoch 921/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0607 - val_loss: 0.7053\nEpoch 922/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0605 - val_loss: 0.6999\nEpoch 923/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0603 - val_loss: 0.6943\nEpoch 924/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0600 - val_loss: 0.6890\nEpoch 925/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0598 - val_loss: 0.6838\nEpoch 926/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0596 - val_loss: 0.6785\nEpoch 927/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0594 - val_loss: 0.6733\nEpoch 928/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0592 - val_loss: 0.6678\nEpoch 929/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0590 - val_loss: 0.6624\nEpoch 930/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0587 - val_loss: 0.6573\nEpoch 931/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0585 - val_loss: 0.6522\nEpoch 932/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0583 - val_loss: 0.6472\nEpoch 933/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0581 - val_loss: 0.6422\nEpoch 934/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0579 - val_loss: 0.6372\nEpoch 935/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0577 - val_loss: 0.6322\nEpoch 936/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0575 - val_loss: 0.6270\nEpoch 937/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0573 - val_loss: 0.6220\nEpoch 938/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0571 - val_loss: 0.6170\nEpoch 939/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0569 - val_loss: 0.6121\nEpoch 940/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0567 - val_loss: 0.6070\nEpoch 941/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0565 - val_loss: 0.6022\nEpoch 942/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0563 - val_loss: 0.5973\nEpoch 943/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0561 - val_loss: 0.5923\nEpoch 944/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0559 - val_loss: 0.5877\nEpoch 945/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0557 - val_loss: 0.5830\nEpoch 946/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0555 - val_loss: 0.5779\nEpoch 947/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0553 - val_loss: 0.5730\nEpoch 948/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0551 - val_loss: 0.5684\nEpoch 949/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0549 - val_loss: 0.5634\nEpoch 950/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0547 - val_loss: 0.5588\nEpoch 951/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0545 - val_loss: 0.5542\nEpoch 952/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0544 - val_loss: 0.5495\nEpoch 953/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0542 - val_loss: 0.5449\nEpoch 954/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0540 - val_loss: 0.5401\nEpoch 955/2000\n6/6 [==============================] - 0s 2ms/step - loss: 0.0538 - val_loss: 0.5357\nEpoch 956/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0536 - val_loss: 0.5312\nEpoch 957/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0534 - val_loss: 0.5266\nEpoch 958/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0533 - val_loss: 0.5221\nEpoch 959/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0531 - val_loss: 0.5178\nEpoch 960/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0529 - val_loss: 0.5137\nEpoch 961/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0527 - val_loss: 0.5098\nEpoch 962/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0526 - val_loss: 0.5058\nEpoch 963/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0524 - val_loss: 0.5017\nEpoch 964/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0522 - val_loss: 0.4976\nEpoch 965/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0521 - val_loss: 0.4935\nEpoch 966/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0519 - val_loss: 0.4896\nEpoch 967/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0517 - val_loss: 0.4859\nEpoch 968/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0516 - val_loss: 0.4814\nEpoch 969/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0514 - val_loss: 0.4773\nEpoch 970/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0512 - val_loss: 0.4733\nEpoch 971/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0511 - val_loss: 0.4691\nEpoch 972/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0509 - val_loss: 0.4649\nEpoch 973/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0508 - val_loss: 0.4611\nEpoch 974/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0506 - val_loss: 0.4570\nEpoch 975/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0505 - val_loss: 0.4527\nEpoch 976/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0503 - val_loss: 0.4487\nEpoch 977/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0501 - val_loss: 0.4446\nEpoch 978/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0500 - val_loss: 0.4406\nEpoch 979/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0498 - val_loss: 0.4368\nEpoch 980/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0497 - val_loss: 0.4333\nEpoch 981/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0495 - val_loss: 0.4294\nEpoch 982/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0494 - val_loss: 0.4259\nEpoch 983/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0492 - val_loss: 0.4224\nEpoch 984/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0491 - val_loss: 0.4187\nEpoch 985/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0489 - val_loss: 0.4149\nEpoch 986/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0488 - val_loss: 0.4111\nEpoch 987/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0486 - val_loss: 0.4071\nEpoch 988/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0485 - val_loss: 0.4037\nEpoch 989/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0483 - val_loss: 0.4002\nEpoch 990/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0482 - val_loss: 0.3968\nEpoch 991/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0480 - val_loss: 0.3931\nEpoch 992/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0479 - val_loss: 0.3897\nEpoch 993/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0478 - val_loss: 0.3861\nEpoch 994/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0476 - val_loss: 0.3829\nEpoch 995/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0475 - val_loss: 0.3797\nEpoch 996/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0474 - val_loss: 0.3764\nEpoch 997/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0472 - val_loss: 0.3733\nEpoch 998/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0471 - val_loss: 0.3703\nEpoch 999/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0470 - val_loss: 0.3669\nEpoch 1000/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0468 - val_loss: 0.3635\nEpoch 1001/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0467 - val_loss: 0.3602\nEpoch 1002/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0466 - val_loss: 0.3569\nEpoch 1003/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0464 - val_loss: 0.3540\nEpoch 1004/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0463 - val_loss: 0.3506\nEpoch 1005/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0462 - val_loss: 0.3476\nEpoch 1006/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0460 - val_loss: 0.3444\nEpoch 1007/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0459 - val_loss: 0.3409\nEpoch 1008/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0458 - val_loss: 0.3378\nEpoch 1009/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0457 - val_loss: 0.3347\nEpoch 1010/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0456 - val_loss: 0.3315\nEpoch 1011/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0454 - val_loss: 0.3286\nEpoch 1012/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0453 - val_loss: 0.3256\nEpoch 1013/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0452 - val_loss: 0.3225\nEpoch 1014/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0451 - val_loss: 0.3197\nEpoch 1015/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0449 - val_loss: 0.3168\nEpoch 1016/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0448 - val_loss: 0.3138\nEpoch 1017/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0447 - val_loss: 0.3111\nEpoch 1018/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0446 - val_loss: 0.3082\nEpoch 1019/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0445 - val_loss: 0.3054\nEpoch 1020/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0444 - val_loss: 0.3027\nEpoch 1021/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0442 - val_loss: 0.2998\nEpoch 1022/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0441 - val_loss: 0.2970\nEpoch 1023/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0440 - val_loss: 0.2940\nEpoch 1024/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0439 - val_loss: 0.2913\nEpoch 1025/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0438 - val_loss: 0.2886\nEpoch 1026/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0437 - val_loss: 0.2858\nEpoch 1027/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0436 - val_loss: 0.2831\nEpoch 1028/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0435 - val_loss: 0.2806\nEpoch 1029/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0434 - val_loss: 0.2782\nEpoch 1030/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0433 - val_loss: 0.2755\nEpoch 1031/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0432 - val_loss: 0.2731\nEpoch 1032/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0431 - val_loss: 0.2702\nEpoch 1033/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0430 - val_loss: 0.2675\nEpoch 1034/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0429 - val_loss: 0.2650\nEpoch 1035/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0428 - val_loss: 0.2623\nEpoch 1036/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0427 - val_loss: 0.2599\nEpoch 1037/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0426 - val_loss: 0.2575\nEpoch 1038/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0425 - val_loss: 0.2553\nEpoch 1039/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0424 - val_loss: 0.2532\nEpoch 1040/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0423 - val_loss: 0.2508\nEpoch 1041/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0422 - val_loss: 0.2486\nEpoch 1042/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0421 - val_loss: 0.2461\nEpoch 1043/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0420 - val_loss: 0.2439\nEpoch 1044/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0419 - val_loss: 0.2415\nEpoch 1045/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0418 - val_loss: 0.2393\nEpoch 1046/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0417 - val_loss: 0.2369\nEpoch 1047/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0416 - val_loss: 0.2347\nEpoch 1048/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0416 - val_loss: 0.2325\nEpoch 1049/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0415 - val_loss: 0.2303\nEpoch 1050/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0414 - val_loss: 0.2282\nEpoch 1051/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0413 - val_loss: 0.2259\nEpoch 1052/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0412 - val_loss: 0.2238\nEpoch 1053/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0411 - val_loss: 0.2215\nEpoch 1054/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0411 - val_loss: 0.2191\nEpoch 1055/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0410 - val_loss: 0.2170\nEpoch 1056/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0409 - val_loss: 0.2149\nEpoch 1057/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0408 - val_loss: 0.2129\nEpoch 1058/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0407 - val_loss: 0.2111\nEpoch 1059/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0406 - val_loss: 0.2093\nEpoch 1060/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0406 - val_loss: 0.2075\nEpoch 1061/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0405 - val_loss: 0.2056\nEpoch 1062/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0404 - val_loss: 0.2037\nEpoch 1063/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0403 - val_loss: 0.2018\nEpoch 1064/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0402 - val_loss: 0.1999\nEpoch 1065/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0402 - val_loss: 0.1978\nEpoch 1066/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0401 - val_loss: 0.1958\nEpoch 1067/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0400 - val_loss: 0.1939\nEpoch 1068/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0400 - val_loss: 0.1920\nEpoch 1069/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0399 - val_loss: 0.1904\nEpoch 1070/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0398 - val_loss: 0.1886\nEpoch 1071/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0397 - val_loss: 0.1868\nEpoch 1072/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0397 - val_loss: 0.1851\nEpoch 1073/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0396 - val_loss: 0.1834\nEpoch 1074/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0395 - val_loss: 0.1816\nEpoch 1075/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0395 - val_loss: 0.1799\nEpoch 1076/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0394 - val_loss: 0.1784\nEpoch 1077/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0393 - val_loss: 0.1765\nEpoch 1078/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0393 - val_loss: 0.1747\nEpoch 1079/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0392 - val_loss: 0.1731\nEpoch 1080/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0392 - val_loss: 0.1715\nEpoch 1081/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0391 - val_loss: 0.1699\nEpoch 1082/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0390 - val_loss: 0.1682\nEpoch 1083/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0390 - val_loss: 0.1667\nEpoch 1084/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0389 - val_loss: 0.1650\nEpoch 1085/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0388 - val_loss: 0.1636\nEpoch 1086/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0388 - val_loss: 0.1622\nEpoch 1087/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0387 - val_loss: 0.1606\nEpoch 1088/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0387 - val_loss: 0.1592\nEpoch 1089/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0386 - val_loss: 0.1578\nEpoch 1090/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0385 - val_loss: 0.1563\nEpoch 1091/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0385 - val_loss: 0.1547\nEpoch 1092/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0384 - val_loss: 0.1532\nEpoch 1093/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0384 - val_loss: 0.1517\nEpoch 1094/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0383 - val_loss: 0.1503\nEpoch 1095/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0382 - val_loss: 0.1489\nEpoch 1096/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0382 - val_loss: 0.1475\nEpoch 1097/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0381 - val_loss: 0.1460\nEpoch 1098/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0381 - val_loss: 0.1446\nEpoch 1099/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0380 - val_loss: 0.1434\nEpoch 1100/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0380 - val_loss: 0.1419\nEpoch 1101/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0379 - val_loss: 0.1407\nEpoch 1102/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0379 - val_loss: 0.1393\nEpoch 1103/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0378 - val_loss: 0.1381\nEpoch 1104/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0378 - val_loss: 0.1367\nEpoch 1105/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0377 - val_loss: 0.1355\nEpoch 1106/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0377 - val_loss: 0.1343\nEpoch 1107/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0376 - val_loss: 0.1333\nEpoch 1108/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0376 - val_loss: 0.1321\nEpoch 1109/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0375 - val_loss: 0.1308\nEpoch 1110/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0375 - val_loss: 0.1295\nEpoch 1111/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0374 - val_loss: 0.1283\nEpoch 1112/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0374 - val_loss: 0.1270\nEpoch 1113/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0374 - val_loss: 0.1257\nEpoch 1114/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0373 - val_loss: 0.1246\nEpoch 1115/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0373 - val_loss: 0.1234\nEpoch 1116/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0372 - val_loss: 0.1223\nEpoch 1117/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0372 - val_loss: 0.1211\nEpoch 1118/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0371 - val_loss: 0.1201\nEpoch 1119/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0371 - val_loss: 0.1190\nEpoch 1120/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0371 - val_loss: 0.1178\nEpoch 1121/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0370 - val_loss: 0.1166\nEpoch 1122/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0370 - val_loss: 0.1154\nEpoch 1123/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0369 - val_loss: 0.1143\nEpoch 1124/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0369 - val_loss: 0.1133\nEpoch 1125/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0368 - val_loss: 0.1123\nEpoch 1126/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0368 - val_loss: 0.1112\nEpoch 1127/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0368 - val_loss: 0.1102\nEpoch 1128/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0367 - val_loss: 0.1094\nEpoch 1129/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0367 - val_loss: 0.1086\nEpoch 1130/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0367 - val_loss: 0.1076\nEpoch 1131/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0366 - val_loss: 0.1066\nEpoch 1132/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0366 - val_loss: 0.1057\nEpoch 1133/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0366 - val_loss: 0.1049\nEpoch 1134/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0365 - val_loss: 0.1040\nEpoch 1135/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0365 - val_loss: 0.1030\nEpoch 1136/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0364 - val_loss: 0.1021\nEpoch 1137/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0364 - val_loss: 0.1012\nEpoch 1138/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0364 - val_loss: 0.1003\nEpoch 1139/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0364 - val_loss: 0.0995\nEpoch 1140/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0363 - val_loss: 0.0988\nEpoch 1141/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0363 - val_loss: 0.0980\nEpoch 1142/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0363 - val_loss: 0.0972\nEpoch 1143/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0362 - val_loss: 0.0965\nEpoch 1144/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0362 - val_loss: 0.0956\nEpoch 1145/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0362 - val_loss: 0.0947\nEpoch 1146/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0361 - val_loss: 0.0939\nEpoch 1147/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0361 - val_loss: 0.0930\nEpoch 1148/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0361 - val_loss: 0.0923\nEpoch 1149/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0360 - val_loss: 0.0916\nEpoch 1150/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0360 - val_loss: 0.0910\nEpoch 1151/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0360 - val_loss: 0.0903\nEpoch 1152/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0360 - val_loss: 0.0895\nEpoch 1153/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0359 - val_loss: 0.0888\nEpoch 1154/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0359 - val_loss: 0.0880\nEpoch 1155/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0359 - val_loss: 0.0874\nEpoch 1156/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0359 - val_loss: 0.0867\nEpoch 1157/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0358 - val_loss: 0.0858\nEpoch 1158/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0358 - val_loss: 0.0851\nEpoch 1159/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0358 - val_loss: 0.0844\nEpoch 1160/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0357 - val_loss: 0.0837\nEpoch 1161/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0357 - val_loss: 0.0830\nEpoch 1162/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0357 - val_loss: 0.0824\nEpoch 1163/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0357 - val_loss: 0.0818\nEpoch 1164/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0356 - val_loss: 0.0812\nEpoch 1165/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0356 - val_loss: 0.0808\nEpoch 1166/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0356 - val_loss: 0.0803\nEpoch 1167/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0356 - val_loss: 0.0797\nEpoch 1168/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0356 - val_loss: 0.0792\nEpoch 1169/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0355 - val_loss: 0.0786\nEpoch 1170/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0355 - val_loss: 0.0780\nEpoch 1171/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0355 - val_loss: 0.0775\nEpoch 1172/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0355 - val_loss: 0.0768\nEpoch 1173/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0354 - val_loss: 0.0762\nEpoch 1174/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0354 - val_loss: 0.0756\nEpoch 1175/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0354 - val_loss: 0.0750\nEpoch 1176/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0354 - val_loss: 0.0745\nEpoch 1177/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0354 - val_loss: 0.0740\nEpoch 1178/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0354 - val_loss: 0.0734\nEpoch 1179/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0353 - val_loss: 0.0729\nEpoch 1180/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0353 - val_loss: 0.0724\nEpoch 1181/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0353 - val_loss: 0.0719\nEpoch 1182/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0353 - val_loss: 0.0714\nEpoch 1183/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0709\nEpoch 1184/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0704\nEpoch 1185/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0698\nEpoch 1186/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0694\nEpoch 1187/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0689\nEpoch 1188/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0684\nEpoch 1189/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0679\nEpoch 1190/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0674\nEpoch 1191/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0669\nEpoch 1192/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0666\nEpoch 1193/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0661\nEpoch 1194/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0657\nEpoch 1195/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0652\nEpoch 1196/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0350 - val_loss: 0.0648\nEpoch 1197/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0644\nEpoch 1198/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0350 - val_loss: 0.0640\nEpoch 1199/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0636\nEpoch 1200/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0632\nEpoch 1201/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0627\nEpoch 1202/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0623\nEpoch 1203/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0621\nEpoch 1204/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0617\nEpoch 1205/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0614\nEpoch 1206/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0611\nEpoch 1207/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0607\nEpoch 1208/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0604\nEpoch 1209/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0601\nEpoch 1210/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0597\nEpoch 1211/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0594\nEpoch 1212/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0590\nEpoch 1213/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0587\nEpoch 1214/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0583\nEpoch 1215/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0581\nEpoch 1216/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0348 - val_loss: 0.0578\nEpoch 1217/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0575\nEpoch 1218/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0571\nEpoch 1219/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0568\nEpoch 1220/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0565\nEpoch 1221/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0562\nEpoch 1222/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0559\nEpoch 1223/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0556\nEpoch 1224/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0553\nEpoch 1225/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0551\nEpoch 1226/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0548\nEpoch 1227/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0545\nEpoch 1228/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0543\nEpoch 1229/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0540\nEpoch 1230/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0537\nEpoch 1231/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0535\nEpoch 1232/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0532\nEpoch 1233/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0530\nEpoch 1234/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0527\nEpoch 1235/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0525\nEpoch 1236/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0523\nEpoch 1237/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0521\nEpoch 1238/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0518\nEpoch 1239/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0516\nEpoch 1240/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0514\nEpoch 1241/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0513\nEpoch 1242/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0511\nEpoch 1243/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0509\nEpoch 1244/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0507\nEpoch 1245/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0505\nEpoch 1246/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0503\nEpoch 1247/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0500\nEpoch 1248/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0499\nEpoch 1249/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0345 - val_loss: 0.0497\nEpoch 1250/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0495\nEpoch 1251/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0493\nEpoch 1252/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0491\nEpoch 1253/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0489\nEpoch 1254/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0488\nEpoch 1255/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0486\nEpoch 1256/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0484\nEpoch 1257/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0482\nEpoch 1258/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0480\nEpoch 1259/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0479\nEpoch 1260/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0344 - val_loss: 0.0477\nEpoch 1261/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0344 - val_loss: 0.0475\nEpoch 1262/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0473\nEpoch 1263/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0473\nEpoch 1264/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0344 - val_loss: 0.0472\nEpoch 1265/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0470\nEpoch 1266/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0468\nEpoch 1267/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0344 - val_loss: 0.0467\nEpoch 1268/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0465\nEpoch 1269/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0463\nEpoch 1270/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0462\nEpoch 1271/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0344 - val_loss: 0.0460\nEpoch 1272/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0458\nEpoch 1273/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0344 - val_loss: 0.0457\nEpoch 1274/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0456\nEpoch 1275/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0454\nEpoch 1276/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0344 - val_loss: 0.0453\nEpoch 1277/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0452\nEpoch 1278/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0451\nEpoch 1279/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0343 - val_loss: 0.0450\nEpoch 1280/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0343 - val_loss: 0.0449\nEpoch 1281/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0448\nEpoch 1282/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0447\nEpoch 1283/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0446\nEpoch 1284/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0445\nEpoch 1285/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0444\nEpoch 1286/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0443\nEpoch 1287/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0442\nEpoch 1288/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0441\nEpoch 1289/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0440\nEpoch 1290/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0439\nEpoch 1291/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0438\nEpoch 1292/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0437\nEpoch 1293/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0437\nEpoch 1294/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0343 - val_loss: 0.0436\nEpoch 1295/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0435\nEpoch 1296/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0434\nEpoch 1297/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0434\nEpoch 1298/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0433\nEpoch 1299/2000\n6/6 [==============================] - 0s 2ms/step - loss: 0.0343 - val_loss: 0.0432\nEpoch 1300/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0432\nEpoch 1301/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0430\nEpoch 1302/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0430\nEpoch 1303/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0429\nEpoch 1304/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0343 - val_loss: 0.0428\nEpoch 1305/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0343 - val_loss: 0.0427\nEpoch 1306/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0426\nEpoch 1307/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0426\nEpoch 1308/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0425\nEpoch 1309/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0424\nEpoch 1310/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0423\nEpoch 1311/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0423\nEpoch 1312/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0422\nEpoch 1313/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0421\nEpoch 1314/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0421\nEpoch 1315/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0420\nEpoch 1316/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0420\nEpoch 1317/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0419\nEpoch 1318/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0419\nEpoch 1319/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0418\nEpoch 1320/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0417\nEpoch 1321/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0417\nEpoch 1322/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0416\nEpoch 1323/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0415\nEpoch 1324/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0415\nEpoch 1325/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0414\nEpoch 1326/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0414\nEpoch 1327/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0413\nEpoch 1328/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0413\nEpoch 1329/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0412\nEpoch 1330/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0412\nEpoch 1331/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0412\nEpoch 1332/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0411\nEpoch 1333/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0411\nEpoch 1334/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0411\nEpoch 1335/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0410\nEpoch 1336/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0410\nEpoch 1337/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0410\nEpoch 1338/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0409\nEpoch 1339/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0408\nEpoch 1340/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0408\nEpoch 1341/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0407\nEpoch 1342/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0407\nEpoch 1343/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0406\nEpoch 1344/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0406\nEpoch 1345/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0405\nEpoch 1346/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0405\nEpoch 1347/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0405\nEpoch 1348/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0405\nEpoch 1349/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0404\nEpoch 1350/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0404\nEpoch 1351/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0404\nEpoch 1352/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0403\nEpoch 1353/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0403\nEpoch 1354/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0402\nEpoch 1355/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0402\nEpoch 1356/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0402\nEpoch 1357/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0401\nEpoch 1358/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0401\nEpoch 1359/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0401\nEpoch 1360/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0400\nEpoch 1361/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0400\nEpoch 1362/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0400\nEpoch 1363/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0400\nEpoch 1364/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0399\nEpoch 1365/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0399\nEpoch 1366/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0399\nEpoch 1367/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0399\nEpoch 1368/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0399\nEpoch 1369/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0399\nEpoch 1370/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0399\nEpoch 1371/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0398\nEpoch 1372/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0398\nEpoch 1373/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0398\nEpoch 1374/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0398\nEpoch 1375/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0397\nEpoch 1376/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0397\nEpoch 1377/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0397\nEpoch 1378/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0397\nEpoch 1379/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0397\nEpoch 1380/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0397\nEpoch 1381/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0396\nEpoch 1382/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0396\nEpoch 1383/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0396\nEpoch 1384/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0396\nEpoch 1385/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0396\nEpoch 1386/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0396\nEpoch 1387/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0396\nEpoch 1388/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0395\nEpoch 1389/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0395\nEpoch 1390/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0395\nEpoch 1391/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0395\nEpoch 1392/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1393/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1394/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1395/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1396/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1397/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1398/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1399/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1400/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1401/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1402/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1403/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1404/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1405/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1406/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1407/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1408/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1409/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1410/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1411/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1412/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1413/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1414/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0392\nEpoch 1415/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0392\nEpoch 1416/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0392\nEpoch 1417/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0392\nEpoch 1418/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0392\nEpoch 1419/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1420/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1421/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1422/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1423/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1424/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1425/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1426/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1427/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1428/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1429/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1430/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1431/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1432/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1433/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1434/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1435/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1436/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1437/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1438/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1439/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1440/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1441/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1442/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1443/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1444/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1445/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1446/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1447/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1448/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1449/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1450/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1451/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1452/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1453/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1454/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1455/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1456/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1457/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1458/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1459/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1460/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1461/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1462/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1463/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1464/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1465/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1466/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1467/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1468/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1469/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1470/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1471/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1472/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1473/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1474/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1475/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1476/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1477/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1478/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1479/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1480/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1481/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1482/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1483/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1484/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1485/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1486/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1487/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1488/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1489/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1490/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1491/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1492/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1493/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1494/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1495/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1496/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1497/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1498/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1499/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1500/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1501/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1502/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1503/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1504/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1505/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1506/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1507/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1508/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1509/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1510/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1511/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1512/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1513/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1514/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1515/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1516/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1517/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1518/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1519/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1520/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1521/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1522/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1523/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1524/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1525/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1526/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1527/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1528/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1529/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1530/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1531/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1532/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1533/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1534/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1535/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1536/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1537/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1538/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1539/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1540/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1541/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1542/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1543/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1544/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1545/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1546/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1547/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1548/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1549/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1550/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1551/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1552/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1553/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1554/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1555/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1556/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1557/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1558/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1559/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1560/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1561/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1562/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1563/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1564/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1565/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1566/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1567/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1568/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1569/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1570/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1571/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1572/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1573/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1574/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1575/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1576/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1577/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1578/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1579/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1580/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1581/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1582/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1583/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1584/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1585/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1586/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1587/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1588/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1589/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1590/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1591/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1592/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1593/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1594/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1595/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1596/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1597/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1598/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1599/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1600/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1601/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1602/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1603/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1604/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1605/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1606/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1607/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1608/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1609/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1610/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1611/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1612/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1613/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1614/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1615/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1616/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1617/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1618/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1619/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1620/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1621/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1622/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1623/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1624/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1625/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1626/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1627/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1628/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1629/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1630/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1631/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1632/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1633/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1634/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1635/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1636/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1637/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1638/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1639/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1640/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1641/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1642/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1643/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1644/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1645/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1646/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1647/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1648/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1649/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1650/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1651/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1652/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1653/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1654/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1655/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1656/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1657/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1658/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1659/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1660/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1661/2000\n6/6 [==============================] - ETA: 0s - loss: 0.034 - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1662/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1663/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1664/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1665/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1666/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1667/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1668/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1669/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1670/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1671/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1672/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1673/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1674/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1675/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1676/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1677/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1678/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1679/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1680/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1681/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1682/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1683/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1684/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1685/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1686/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1687/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1688/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1689/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1690/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1691/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1692/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1693/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1694/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1695/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1696/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1697/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1698/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1699/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1700/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1701/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1702/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1703/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1704/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1705/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1706/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1707/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1708/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1709/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1710/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1711/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1712/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1713/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1714/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1715/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1716/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1717/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1718/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1719/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1720/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1721/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1722/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1723/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1724/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1725/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1726/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1727/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1728/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1729/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1730/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1731/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1732/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1733/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1734/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1735/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1736/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1737/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1738/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1739/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1740/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1741/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1742/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1743/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1744/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1745/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1746/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1747/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1748/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1749/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1750/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1751/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1752/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1753/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1754/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1755/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1756/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1757/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1758/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1759/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1760/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1761/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1762/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1763/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1764/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1765/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1766/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1767/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1768/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1769/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1770/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1771/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1772/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1773/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1774/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1775/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1776/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1777/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1778/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1779/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1780/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1781/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1782/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1783/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1784/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1785/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1786/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1787/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1788/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1789/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1790/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1791/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1792/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1793/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1794/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1795/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1796/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1797/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1798/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1799/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1800/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1801/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1802/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1803/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1804/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1805/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1806/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1807/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1808/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1809/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1810/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1811/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1812/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1813/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1814/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1815/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1816/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1817/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1818/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1819/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1820/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1821/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1822/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1823/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1824/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1825/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1826/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1827/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1828/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1829/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1830/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1831/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1832/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1833/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1834/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1835/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1836/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1837/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1838/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1839/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1840/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1841/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1842/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1843/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1844/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1845/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1846/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1847/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1848/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1849/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1850/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1851/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1852/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1853/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1854/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1855/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1856/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1857/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1858/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1859/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1860/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1861/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1862/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1863/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1864/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1865/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1866/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1867/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1868/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1869/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1870/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1871/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1872/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1873/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1874/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1875/2000\n6/6 [==============================] - 0s 5ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1876/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1877/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1878/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1879/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1880/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1881/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1882/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1883/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1884/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1885/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1886/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1887/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1888/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1889/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1890/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1891/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1892/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1893/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1894/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1895/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1896/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1897/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1898/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1899/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1900/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1901/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1902/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1903/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1904/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1905/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1906/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1907/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1908/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1909/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1910/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1911/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1912/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1913/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1914/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1915/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1916/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1917/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1918/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1919/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1920/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1921/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1922/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1923/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1924/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1925/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1926/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1927/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1928/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1929/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1930/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1931/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1932/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1933/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1934/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1935/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1936/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1937/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1938/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1939/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1940/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1941/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1942/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1943/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1944/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1945/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1946/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1947/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1948/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1949/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1950/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1951/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1952/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1953/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1954/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1955/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1956/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1957/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1958/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1959/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1960/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1961/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1962/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1963/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1964/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1965/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1966/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1967/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1968/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1969/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1970/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1971/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1972/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1973/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1974/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1975/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1976/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1977/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1978/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1979/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1980/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1981/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1982/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1983/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1984/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1985/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1986/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1987/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1988/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1989/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1990/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1991/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1992/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1993/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1994/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1995/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1996/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1997/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1998/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1999/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 2000/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\n\n\n&lt;keras.callbacks.History at 0x7f708bd97e50&gt;\n\n\n\nplt.plot(y,'.',alpha=0.1)\nplt.plot(net(X),'--')\n\n\n\n\n\n#\n#%tensorboard --logdir logs --host 0.0.0.0\n\n\n이런것은 오버핏이 아님!\n\n- 결론적으로 말해서 위와 같은 net는 설계하였을 경우 val을 빼는 것은 어리석음. (데이터만 버리는 꼴임)\n- 더 많은 데이터를 남겨주면 더 빨리 학습한다.\n\n#collapse_output\n!rm -rf logs\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(1))\nnet.compile(loss='mse',optimizer='adam')\nnet.fit(X,y,epochs=500,batch_size=100, validation_split=0.1, callbacks=tf.keras.callbacks.TensorBoard())\n\nEpoch 1/500\n9/9 [==============================] - 0s 5ms/step - loss: 11.1529 - val_loss: 17.6322\nEpoch 2/500\n9/9 [==============================] - 0s 2ms/step - loss: 11.0510 - val_loss: 17.4478\nEpoch 3/500\n9/9 [==============================] - 0s 2ms/step - loss: 10.9482 - val_loss: 17.2670\nEpoch 4/500\n9/9 [==============================] - 0s 2ms/step - loss: 10.8465 - val_loss: 17.0850\nEpoch 5/500\n9/9 [==============================] - 0s 2ms/step - loss: 10.7443 - val_loss: 16.9074\nEpoch 6/500\n9/9 [==============================] - 0s 2ms/step - loss: 10.6457 - val_loss: 16.7250\nEpoch 7/500\n9/9 [==============================] - 0s 1ms/step - loss: 10.5456 - val_loss: 16.5480\nEpoch 8/500\n9/9 [==============================] - 0s 2ms/step - loss: 10.4474 - val_loss: 16.3721\nEpoch 9/500\n9/9 [==============================] - 0s 2ms/step - loss: 10.3499 - val_loss: 16.1945\nEpoch 10/500\n9/9 [==============================] - 0s 2ms/step - loss: 10.2516 - val_loss: 16.0212\nEpoch 11/500\n9/9 [==============================] - 0s 2ms/step - loss: 10.1587 - val_loss: 15.8501\nEpoch 12/500\n9/9 [==============================] - 0s 2ms/step - loss: 10.0600 - val_loss: 15.6844\nEpoch 13/500\n9/9 [==============================] - 0s 2ms/step - loss: 9.9677 - val_loss: 15.5179\nEpoch 14/500\n9/9 [==============================] - 0s 2ms/step - loss: 9.8747 - val_loss: 15.3479\nEpoch 15/500\n9/9 [==============================] - 0s 2ms/step - loss: 9.7806 - val_loss: 15.1842\nEpoch 16/500\n9/9 [==============================] - 0s 2ms/step - loss: 9.6911 - val_loss: 15.0164\nEpoch 17/500\n9/9 [==============================] - 0s 2ms/step - loss: 9.5969 - val_loss: 14.8620\nEpoch 18/500\n9/9 [==============================] - 0s 2ms/step - loss: 9.5088 - val_loss: 14.6981\nEpoch 19/500\n9/9 [==============================] - 0s 2ms/step - loss: 9.4182 - val_loss: 14.5400\nEpoch 20/500\n9/9 [==============================] - 0s 1ms/step - loss: 9.3294 - val_loss: 14.3857\nEpoch 21/500\n9/9 [==============================] - 0s 2ms/step - loss: 9.2421 - val_loss: 14.2279\nEpoch 22/500\n9/9 [==============================] - 0s 1ms/step - loss: 9.1551 - val_loss: 14.0740\nEpoch 23/500\n9/9 [==============================] - 0s 2ms/step - loss: 9.0690 - val_loss: 13.9182\nEpoch 24/500\n9/9 [==============================] - 0s 1ms/step - loss: 8.9823 - val_loss: 13.7646\nEpoch 25/500\n9/9 [==============================] - 0s 2ms/step - loss: 8.8961 - val_loss: 13.6238\nEpoch 26/500\n9/9 [==============================] - 0s 1ms/step - loss: 8.8128 - val_loss: 13.4795\nEpoch 27/500\n9/9 [==============================] - 0s 2ms/step - loss: 8.7307 - val_loss: 13.3281\nEpoch 28/500\n9/9 [==============================] - 0s 2ms/step - loss: 8.6474 - val_loss: 13.1848\nEpoch 29/500\n9/9 [==============================] - 0s 2ms/step - loss: 8.5655 - val_loss: 13.0439\nEpoch 30/500\n9/9 [==============================] - 0s 2ms/step - loss: 8.4836 - val_loss: 12.9032\nEpoch 31/500\n9/9 [==============================] - 0s 1ms/step - loss: 8.4041 - val_loss: 12.7584\nEpoch 32/500\n9/9 [==============================] - 0s 2ms/step - loss: 8.3237 - val_loss: 12.6194\nEpoch 33/500\n9/9 [==============================] - 0s 2ms/step - loss: 8.2445 - val_loss: 12.4818\nEpoch 34/500\n9/9 [==============================] - 0s 2ms/step - loss: 8.1652 - val_loss: 12.3472\nEpoch 35/500\n9/9 [==============================] - 0s 2ms/step - loss: 8.0891 - val_loss: 12.2060\nEpoch 36/500\n9/9 [==============================] - 0s 2ms/step - loss: 8.0112 - val_loss: 12.0756\nEpoch 37/500\n9/9 [==============================] - 0s 2ms/step - loss: 7.9331 - val_loss: 11.9493\nEpoch 38/500\n9/9 [==============================] - 0s 1ms/step - loss: 7.8597 - val_loss: 11.8165\nEpoch 39/500\n9/9 [==============================] - 0s 2ms/step - loss: 7.7832 - val_loss: 11.6866\nEpoch 40/500\n9/9 [==============================] - 0s 2ms/step - loss: 7.7085 - val_loss: 11.5609\nEpoch 41/500\n9/9 [==============================] - 0s 1ms/step - loss: 7.6347 - val_loss: 11.4300\nEpoch 42/500\n9/9 [==============================] - 0s 2ms/step - loss: 7.5625 - val_loss: 11.3039\nEpoch 43/500\n9/9 [==============================] - 0s 2ms/step - loss: 7.4886 - val_loss: 11.1786\nEpoch 44/500\n9/9 [==============================] - 0s 2ms/step - loss: 7.4170 - val_loss: 11.0613\nEpoch 45/500\n9/9 [==============================] - 0s 2ms/step - loss: 7.3459 - val_loss: 10.9341\nEpoch 46/500\n9/9 [==============================] - 0s 2ms/step - loss: 7.2742 - val_loss: 10.8175\nEpoch 47/500\n9/9 [==============================] - 0s 2ms/step - loss: 7.2045 - val_loss: 10.6986\nEpoch 48/500\n9/9 [==============================] - 0s 2ms/step - loss: 7.1351 - val_loss: 10.5782\nEpoch 49/500\n9/9 [==============================] - 0s 2ms/step - loss: 7.0656 - val_loss: 10.4625\nEpoch 50/500\n9/9 [==============================] - 0s 2ms/step - loss: 6.9977 - val_loss: 10.3436\nEpoch 51/500\n9/9 [==============================] - 0s 2ms/step - loss: 6.9311 - val_loss: 10.2223\nEpoch 52/500\n9/9 [==============================] - 0s 2ms/step - loss: 6.8624 - val_loss: 10.1109\nEpoch 53/500\n9/9 [==============================] - 0s 2ms/step - loss: 6.7964 - val_loss: 9.9984\nEpoch 54/500\n9/9 [==============================] - 0s 2ms/step - loss: 6.7296 - val_loss: 9.8888\nEpoch 55/500\n9/9 [==============================] - 0s 2ms/step - loss: 6.6645 - val_loss: 9.7811\nEpoch 56/500\n9/9 [==============================] - 0s 2ms/step - loss: 6.5996 - val_loss: 9.6738\nEpoch 57/500\n9/9 [==============================] - 0s 1ms/step - loss: 6.5362 - val_loss: 9.5644\nEpoch 58/500\n9/9 [==============================] - 0s 2ms/step - loss: 6.4717 - val_loss: 9.4555\nEpoch 59/500\n9/9 [==============================] - 0s 2ms/step - loss: 6.4094 - val_loss: 9.3465\nEpoch 60/500\n9/9 [==============================] - 0s 2ms/step - loss: 6.3461 - val_loss: 9.2445\nEpoch 61/500\n9/9 [==============================] - 0s 2ms/step - loss: 6.2842 - val_loss: 9.1374\nEpoch 62/500\n9/9 [==============================] - 0s 2ms/step - loss: 6.2229 - val_loss: 9.0369\nEpoch 63/500\n9/9 [==============================] - 0s 2ms/step - loss: 6.1613 - val_loss: 8.9344\nEpoch 64/500\n9/9 [==============================] - 0s 2ms/step - loss: 6.1003 - val_loss: 8.8330\nEpoch 65/500\n9/9 [==============================] - 0s 2ms/step - loss: 6.0411 - val_loss: 8.7345\nEpoch 66/500\n9/9 [==============================] - 0s 2ms/step - loss: 5.9818 - val_loss: 8.6328\nEpoch 67/500\n9/9 [==============================] - 0s 2ms/step - loss: 5.9227 - val_loss: 8.5349\nEpoch 68/500\n9/9 [==============================] - 0s 2ms/step - loss: 5.8640 - val_loss: 8.4380\nEpoch 69/500\n9/9 [==============================] - 0s 2ms/step - loss: 5.8069 - val_loss: 8.3396\nEpoch 70/500\n9/9 [==============================] - 0s 2ms/step - loss: 5.7490 - val_loss: 8.2460\nEpoch 71/500\n9/9 [==============================] - 0s 1ms/step - loss: 5.6922 - val_loss: 8.1507\nEpoch 72/500\n9/9 [==============================] - 0s 1ms/step - loss: 5.6366 - val_loss: 8.0578\nEpoch 73/500\n9/9 [==============================] - 0s 2ms/step - loss: 5.5798 - val_loss: 7.9645\nEpoch 74/500\n9/9 [==============================] - 0s 1ms/step - loss: 5.5245 - val_loss: 7.8750\nEpoch 75/500\n9/9 [==============================] - 0s 2ms/step - loss: 5.4699 - val_loss: 7.7816\nEpoch 76/500\n9/9 [==============================] - 0s 1ms/step - loss: 5.4155 - val_loss: 7.6949\nEpoch 77/500\n9/9 [==============================] - 0s 2ms/step - loss: 5.3615 - val_loss: 7.6049\nEpoch 78/500\n9/9 [==============================] - 0s 2ms/step - loss: 5.3088 - val_loss: 7.5146\nEpoch 79/500\n9/9 [==============================] - 0s 2ms/step - loss: 5.2546 - val_loss: 7.4298\nEpoch 80/500\n9/9 [==============================] - 0s 2ms/step - loss: 5.2024 - val_loss: 7.3431\nEpoch 81/500\n9/9 [==============================] - 0s 2ms/step - loss: 5.1507 - val_loss: 7.2562\nEpoch 82/500\n9/9 [==============================] - 0s 2ms/step - loss: 5.0991 - val_loss: 7.1726\nEpoch 83/500\n9/9 [==============================] - 0s 2ms/step - loss: 5.0482 - val_loss: 7.0886\nEpoch 84/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.9975 - val_loss: 7.0058\nEpoch 85/500\n9/9 [==============================] - 0s 1ms/step - loss: 4.9472 - val_loss: 6.9263\nEpoch 86/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.8969 - val_loss: 6.8459\nEpoch 87/500\n9/9 [==============================] - 0s 1ms/step - loss: 4.8479 - val_loss: 6.7668\nEpoch 88/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.7994 - val_loss: 6.6891\nEpoch 89/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.7512 - val_loss: 6.6063\nEpoch 90/500\n9/9 [==============================] - 0s 1ms/step - loss: 4.7022 - val_loss: 6.5310\nEpoch 91/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.6547 - val_loss: 6.4561\nEpoch 92/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.6079 - val_loss: 6.3786\nEpoch 93/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.5607 - val_loss: 6.3046\nEpoch 94/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.5144 - val_loss: 6.2286\nEpoch 95/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.4690 - val_loss: 6.1530\nEpoch 96/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.4237 - val_loss: 6.0801\nEpoch 97/500\n9/9 [==============================] - 0s 1ms/step - loss: 4.3780 - val_loss: 6.0086\nEpoch 98/500\n9/9 [==============================] - 0s 1ms/step - loss: 4.3333 - val_loss: 5.9393\nEpoch 99/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.2886 - val_loss: 5.8690\nEpoch 100/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.2452 - val_loss: 5.7963\nEpoch 101/500\n9/9 [==============================] - 0s 1ms/step - loss: 4.2013 - val_loss: 5.7301\nEpoch 102/500\n9/9 [==============================] - 0s 1ms/step - loss: 4.1583 - val_loss: 5.6612\nEpoch 103/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.1153 - val_loss: 5.5936\nEpoch 104/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.0736 - val_loss: 5.5238\nEpoch 105/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.0305 - val_loss: 5.4606\nEpoch 106/500\n9/9 [==============================] - 0s 1ms/step - loss: 3.9894 - val_loss: 5.3935\nEpoch 107/500\n9/9 [==============================] - 0s 1ms/step - loss: 3.9481 - val_loss: 5.3268\nEpoch 108/500\n9/9 [==============================] - 0s 1ms/step - loss: 3.9076 - val_loss: 5.2610\nEpoch 109/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.8664 - val_loss: 5.1996\nEpoch 110/500\n9/9 [==============================] - 0s 1ms/step - loss: 3.8257 - val_loss: 5.1385\nEpoch 111/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.7870 - val_loss: 5.0747\nEpoch 112/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.7474 - val_loss: 5.0111\nEpoch 113/500\n9/9 [==============================] - 0s 1ms/step - loss: 3.7078 - val_loss: 4.9471\nEpoch 114/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.6688 - val_loss: 4.8894\nEpoch 115/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.6306 - val_loss: 4.8294\nEpoch 116/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.5922 - val_loss: 4.7704\nEpoch 117/500\n9/9 [==============================] - 0s 1ms/step - loss: 3.5549 - val_loss: 4.7096\nEpoch 118/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.5174 - val_loss: 4.6514\nEpoch 119/500\n9/9 [==============================] - 0s 1ms/step - loss: 3.4798 - val_loss: 4.5948\nEpoch 120/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.4434 - val_loss: 4.5386\nEpoch 121/500\n9/9 [==============================] - 0s 1ms/step - loss: 3.4072 - val_loss: 4.4816\nEpoch 122/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.3708 - val_loss: 4.4268\nEpoch 123/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.3349 - val_loss: 4.3720\nEpoch 124/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.2994 - val_loss: 4.3174\nEpoch 125/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.2643 - val_loss: 4.2653\nEpoch 126/500\n9/9 [==============================] - 0s 1ms/step - loss: 3.2300 - val_loss: 4.2094\nEpoch 127/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.1955 - val_loss: 4.1579\nEpoch 128/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.1605 - val_loss: 4.1050\nEpoch 129/500\n9/9 [==============================] - 0s 1ms/step - loss: 3.1269 - val_loss: 4.0541\nEpoch 130/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.0935 - val_loss: 4.0013\nEpoch 131/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.0599 - val_loss: 3.9509\nEpoch 132/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.0270 - val_loss: 3.9000\nEpoch 133/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.9942 - val_loss: 3.8525\nEpoch 134/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.9619 - val_loss: 3.8035\nEpoch 135/500\n9/9 [==============================] - 0s 1ms/step - loss: 2.9301 - val_loss: 3.7534\nEpoch 136/500\n9/9 [==============================] - 0s 1ms/step - loss: 2.8981 - val_loss: 3.7043\nEpoch 137/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.8662 - val_loss: 3.6581\nEpoch 138/500\n9/9 [==============================] - 0s 1ms/step - loss: 2.8350 - val_loss: 3.6088\nEpoch 139/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.8040 - val_loss: 3.5631\nEpoch 140/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.7735 - val_loss: 3.5183\nEpoch 141/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.7429 - val_loss: 3.4733\nEpoch 142/500\n9/9 [==============================] - 0s 1ms/step - loss: 2.7129 - val_loss: 3.4262\nEpoch 143/500\n9/9 [==============================] - 0s 1ms/step - loss: 2.6828 - val_loss: 3.3835\nEpoch 144/500\n9/9 [==============================] - 0s 1ms/step - loss: 2.6533 - val_loss: 3.3392\nEpoch 145/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.6238 - val_loss: 3.2961\nEpoch 146/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.5950 - val_loss: 3.2534\nEpoch 147/500\n9/9 [==============================] - 0s 1ms/step - loss: 2.5661 - val_loss: 3.2100\nEpoch 148/500\n9/9 [==============================] - 0s 1ms/step - loss: 2.5375 - val_loss: 3.1676\nEpoch 149/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.5092 - val_loss: 3.1254\nEpoch 150/500\n9/9 [==============================] - 0s 1ms/step - loss: 2.4813 - val_loss: 3.0836\nEpoch 151/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.4537 - val_loss: 3.0419\nEpoch 152/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.4258 - val_loss: 3.0024\nEpoch 153/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.3987 - val_loss: 2.9623\nEpoch 154/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.3716 - val_loss: 2.9227\nEpoch 155/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.3446 - val_loss: 2.8838\nEpoch 156/500\n9/9 [==============================] - 0s 1ms/step - loss: 2.3184 - val_loss: 2.8463\nEpoch 157/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.2919 - val_loss: 2.8087\nEpoch 158/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.2661 - val_loss: 2.7687\nEpoch 159/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.2403 - val_loss: 2.7325\nEpoch 160/500\n9/9 [==============================] - 0s 1ms/step - loss: 2.2147 - val_loss: 2.6946\nEpoch 161/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.1892 - val_loss: 2.6583\nEpoch 162/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.1645 - val_loss: 2.6212\nEpoch 163/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.1394 - val_loss: 2.5858\nEpoch 164/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.1146 - val_loss: 2.5519\nEpoch 165/500\n9/9 [==============================] - 0s 1ms/step - loss: 2.0904 - val_loss: 2.5177\nEpoch 166/500\n9/9 [==============================] - 0s 1ms/step - loss: 2.0662 - val_loss: 2.4819\nEpoch 167/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.0422 - val_loss: 2.4472\nEpoch 168/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.0186 - val_loss: 2.4146\nEpoch 169/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.9950 - val_loss: 2.3812\nEpoch 170/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.9717 - val_loss: 2.3481\nEpoch 171/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.9486 - val_loss: 2.3145\nEpoch 172/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.9259 - val_loss: 2.2815\nEpoch 173/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.9030 - val_loss: 2.2488\nEpoch 174/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.8807 - val_loss: 2.2179\nEpoch 175/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.8587 - val_loss: 2.1859\nEpoch 176/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.8365 - val_loss: 2.1555\nEpoch 177/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.8148 - val_loss: 2.1247\nEpoch 178/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.7933 - val_loss: 2.0933\nEpoch 179/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.7719 - val_loss: 2.0639\nEpoch 180/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.7508 - val_loss: 2.0344\nEpoch 181/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.7297 - val_loss: 2.0057\nEpoch 182/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.7092 - val_loss: 1.9759\nEpoch 183/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.6885 - val_loss: 1.9478\nEpoch 184/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.6680 - val_loss: 1.9216\nEpoch 185/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.6480 - val_loss: 1.8931\nEpoch 186/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.6280 - val_loss: 1.8653\nEpoch 187/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.6082 - val_loss: 1.8372\nEpoch 188/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.5887 - val_loss: 1.8108\nEpoch 189/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.5693 - val_loss: 1.7840\nEpoch 190/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.5501 - val_loss: 1.7577\nEpoch 191/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.5310 - val_loss: 1.7306\nEpoch 192/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.5123 - val_loss: 1.7052\nEpoch 193/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.4938 - val_loss: 1.6806\nEpoch 194/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.4752 - val_loss: 1.6546\nEpoch 195/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.4568 - val_loss: 1.6293\nEpoch 196/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.4390 - val_loss: 1.6049\nEpoch 197/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.4210 - val_loss: 1.5808\nEpoch 198/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.4031 - val_loss: 1.5575\nEpoch 199/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.3857 - val_loss: 1.5334\nEpoch 200/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.3684 - val_loss: 1.5114\nEpoch 201/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.3513 - val_loss: 1.4878\nEpoch 202/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.3341 - val_loss: 1.4668\nEpoch 203/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.3173 - val_loss: 1.4431\nEpoch 204/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.3006 - val_loss: 1.4205\nEpoch 205/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.2841 - val_loss: 1.3977\nEpoch 206/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.2679 - val_loss: 1.3771\nEpoch 207/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.2517 - val_loss: 1.3541\nEpoch 208/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.2358 - val_loss: 1.3334\nEpoch 209/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.2198 - val_loss: 1.3130\nEpoch 210/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.2042 - val_loss: 1.2929\nEpoch 211/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.1888 - val_loss: 1.2718\nEpoch 212/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.1734 - val_loss: 1.2505\nEpoch 213/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.1582 - val_loss: 1.2305\nEpoch 214/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.1431 - val_loss: 1.2123\nEpoch 215/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.1284 - val_loss: 1.1923\nEpoch 216/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.1135 - val_loss: 1.1739\nEpoch 217/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.0991 - val_loss: 1.1556\nEpoch 218/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.0848 - val_loss: 1.1372\nEpoch 219/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.0705 - val_loss: 1.1175\nEpoch 220/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.0565 - val_loss: 1.0985\nEpoch 221/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.0425 - val_loss: 1.0817\nEpoch 222/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.0286 - val_loss: 1.0638\nEpoch 223/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.0150 - val_loss: 1.0465\nEpoch 224/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.0016 - val_loss: 1.0295\nEpoch 225/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.9882 - val_loss: 1.0121\nEpoch 226/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.9751 - val_loss: 0.9955\nEpoch 227/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.9620 - val_loss: 0.9794\nEpoch 228/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.9490 - val_loss: 0.9625\nEpoch 229/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.9363 - val_loss: 0.9461\nEpoch 230/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.9237 - val_loss: 0.9302\nEpoch 231/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.9113 - val_loss: 0.9141\nEpoch 232/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.8988 - val_loss: 0.8987\nEpoch 233/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.8868 - val_loss: 0.8831\nEpoch 234/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.8747 - val_loss: 0.8684\nEpoch 235/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.8628 - val_loss: 0.8537\nEpoch 236/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.8509 - val_loss: 0.8394\nEpoch 237/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.8393 - val_loss: 0.8247\nEpoch 238/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.8278 - val_loss: 0.8101\nEpoch 239/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.8164 - val_loss: 0.7968\nEpoch 240/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.8051 - val_loss: 0.7825\nEpoch 241/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.7940 - val_loss: 0.7689\nEpoch 242/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.7831 - val_loss: 0.7554\nEpoch 243/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.7721 - val_loss: 0.7424\nEpoch 244/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.7614 - val_loss: 0.7292\nEpoch 245/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.7507 - val_loss: 0.7159\nEpoch 246/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.7403 - val_loss: 0.7026\nEpoch 247/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.7300 - val_loss: 0.6907\nEpoch 248/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.7197 - val_loss: 0.6785\nEpoch 249/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.7096 - val_loss: 0.6669\nEpoch 250/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.6996 - val_loss: 0.6546\nEpoch 251/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.6896 - val_loss: 0.6430\nEpoch 252/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.6799 - val_loss: 0.6311\nEpoch 253/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.6702 - val_loss: 0.6192\nEpoch 254/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.6608 - val_loss: 0.6076\nEpoch 255/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.6513 - val_loss: 0.5967\nEpoch 256/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.6420 - val_loss: 0.5856\nEpoch 257/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.6329 - val_loss: 0.5744\nEpoch 258/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.6238 - val_loss: 0.5640\nEpoch 259/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.6148 - val_loss: 0.5537\nEpoch 260/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.6060 - val_loss: 0.5435\nEpoch 261/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.5972 - val_loss: 0.5333\nEpoch 262/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.5886 - val_loss: 0.5234\nEpoch 263/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.5801 - val_loss: 0.5131\nEpoch 264/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.5717 - val_loss: 0.5038\nEpoch 265/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.5634 - val_loss: 0.4936\nEpoch 266/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.5552 - val_loss: 0.4847\nEpoch 267/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.5470 - val_loss: 0.4754\nEpoch 268/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.5391 - val_loss: 0.4663\nEpoch 269/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.5312 - val_loss: 0.4569\nEpoch 270/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.5234 - val_loss: 0.4483\nEpoch 271/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.5157 - val_loss: 0.4397\nEpoch 272/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.5081 - val_loss: 0.4307\nEpoch 273/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.5007 - val_loss: 0.4224\nEpoch 274/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.4933 - val_loss: 0.4148\nEpoch 275/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.4860 - val_loss: 0.4066\nEpoch 276/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.4788 - val_loss: 0.3987\nEpoch 277/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.4717 - val_loss: 0.3905\nEpoch 278/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.4647 - val_loss: 0.3831\nEpoch 279/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.4577 - val_loss: 0.3754\nEpoch 280/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.4509 - val_loss: 0.3683\nEpoch 281/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.4442 - val_loss: 0.3608\nEpoch 282/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.4376 - val_loss: 0.3536\nEpoch 283/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.4310 - val_loss: 0.3468\nEpoch 284/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.4245 - val_loss: 0.3395\nEpoch 285/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.4182 - val_loss: 0.3326\nEpoch 286/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.4120 - val_loss: 0.3255\nEpoch 287/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.4057 - val_loss: 0.3190\nEpoch 288/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.3996 - val_loss: 0.3124\nEpoch 289/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.3936 - val_loss: 0.3065\nEpoch 290/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.3876 - val_loss: 0.3002\nEpoch 291/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.3818 - val_loss: 0.2941\nEpoch 292/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.3760 - val_loss: 0.2880\nEpoch 293/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.3703 - val_loss: 0.2820\nEpoch 294/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.3646 - val_loss: 0.2763\nEpoch 295/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.3591 - val_loss: 0.2704\nEpoch 296/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.3537 - val_loss: 0.2649\nEpoch 297/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.3483 - val_loss: 0.2594\nEpoch 298/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.3430 - val_loss: 0.2540\nEpoch 299/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.3378 - val_loss: 0.2489\nEpoch 300/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.3326 - val_loss: 0.2438\nEpoch 301/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.3275 - val_loss: 0.2386\nEpoch 302/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.3225 - val_loss: 0.2335\nEpoch 303/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.3176 - val_loss: 0.2288\nEpoch 304/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.3128 - val_loss: 0.2238\nEpoch 305/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.3080 - val_loss: 0.2190\nEpoch 306/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.3032 - val_loss: 0.2144\nEpoch 307/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.2986 - val_loss: 0.2099\nEpoch 308/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.2941 - val_loss: 0.2054\nEpoch 309/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.2895 - val_loss: 0.2010\nEpoch 310/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.2851 - val_loss: 0.1969\nEpoch 311/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.2807 - val_loss: 0.1927\nEpoch 312/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.2764 - val_loss: 0.1885\nEpoch 313/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.2722 - val_loss: 0.1847\nEpoch 314/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.2680 - val_loss: 0.1807\nEpoch 315/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.2639 - val_loss: 0.1769\nEpoch 316/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.2598 - val_loss: 0.1731\nEpoch 317/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.2559 - val_loss: 0.1694\nEpoch 318/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.2519 - val_loss: 0.1658\nEpoch 319/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.2480 - val_loss: 0.1622\nEpoch 320/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.2443 - val_loss: 0.1590\nEpoch 321/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.2405 - val_loss: 0.1555\nEpoch 322/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.2368 - val_loss: 0.1522\nEpoch 323/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.2332 - val_loss: 0.1488\nEpoch 324/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.2296 - val_loss: 0.1457\nEpoch 325/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.2261 - val_loss: 0.1428\nEpoch 326/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.2227 - val_loss: 0.1396\nEpoch 327/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.2192 - val_loss: 0.1366\nEpoch 328/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.2159 - val_loss: 0.1337\nEpoch 329/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.2126 - val_loss: 0.1312\nEpoch 330/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.2094 - val_loss: 0.1284\nEpoch 331/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.2062 - val_loss: 0.1255\nEpoch 332/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.2030 - val_loss: 0.1228\nEpoch 333/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.2000 - val_loss: 0.1203\nEpoch 334/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1969 - val_loss: 0.1179\nEpoch 335/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1940 - val_loss: 0.1153\nEpoch 336/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1910 - val_loss: 0.1129\nEpoch 337/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1881 - val_loss: 0.1107\nEpoch 338/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1853 - val_loss: 0.1082\nEpoch 339/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1825 - val_loss: 0.1060\nEpoch 340/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1797 - val_loss: 0.1039\nEpoch 341/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1770 - val_loss: 0.1018\nEpoch 342/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1744 - val_loss: 0.0996\nEpoch 343/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1718 - val_loss: 0.0976\nEpoch 344/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1692 - val_loss: 0.0956\nEpoch 345/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1667 - val_loss: 0.0939\nEpoch 346/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1642 - val_loss: 0.0918\nEpoch 347/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1618 - val_loss: 0.0900\nEpoch 348/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1594 - val_loss: 0.0882\nEpoch 349/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1570 - val_loss: 0.0865\nEpoch 350/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1547 - val_loss: 0.0849\nEpoch 351/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1524 - val_loss: 0.0833\nEpoch 352/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1502 - val_loss: 0.0815\nEpoch 353/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1480 - val_loss: 0.0800\nEpoch 354/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1458 - val_loss: 0.0785\nEpoch 355/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1437 - val_loss: 0.0769\nEpoch 356/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1416 - val_loss: 0.0755\nEpoch 357/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1396 - val_loss: 0.0741\nEpoch 358/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1376 - val_loss: 0.0728\nEpoch 359/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1356 - val_loss: 0.0715\nEpoch 360/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1337 - val_loss: 0.0702\nEpoch 361/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1317 - val_loss: 0.0691\nEpoch 362/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1298 - val_loss: 0.0678\nEpoch 363/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1280 - val_loss: 0.0667\nEpoch 364/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1262 - val_loss: 0.0654\nEpoch 365/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1244 - val_loss: 0.0643\nEpoch 366/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1227 - val_loss: 0.0632\nEpoch 367/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1210 - val_loss: 0.0622\nEpoch 368/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1193 - val_loss: 0.0612\nEpoch 369/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1177 - val_loss: 0.0601\nEpoch 370/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1160 - val_loss: 0.0592\nEpoch 371/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1144 - val_loss: 0.0582\nEpoch 372/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1129 - val_loss: 0.0574\nEpoch 373/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1114 - val_loss: 0.0565\nEpoch 374/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1099 - val_loss: 0.0556\nEpoch 375/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1084 - val_loss: 0.0548\nEpoch 376/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1069 - val_loss: 0.0541\nEpoch 377/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1055 - val_loss: 0.0533\nEpoch 378/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1041 - val_loss: 0.0525\nEpoch 379/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1028 - val_loss: 0.0518\nEpoch 380/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1014 - val_loss: 0.0511\nEpoch 381/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1001 - val_loss: 0.0504\nEpoch 382/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0988 - val_loss: 0.0498\nEpoch 383/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0975 - val_loss: 0.0492\nEpoch 384/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0963 - val_loss: 0.0486\nEpoch 385/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0951 - val_loss: 0.0480\nEpoch 386/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0939 - val_loss: 0.0474\nEpoch 387/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0927 - val_loss: 0.0469\nEpoch 388/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0915 - val_loss: 0.0463\nEpoch 389/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0904 - val_loss: 0.0458\nEpoch 390/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0893 - val_loss: 0.0453\nEpoch 391/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0882 - val_loss: 0.0448\nEpoch 392/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0871 - val_loss: 0.0444\nEpoch 393/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0861 - val_loss: 0.0439\nEpoch 394/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0851 - val_loss: 0.0435\nEpoch 395/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0841 - val_loss: 0.0431\nEpoch 396/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0831 - val_loss: 0.0427\nEpoch 397/500\n9/9 [==============================] - ETA: 0s - loss: 0.088 - 0s 2ms/step - loss: 0.0821 - val_loss: 0.0423\nEpoch 398/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0812 - val_loss: 0.0420\nEpoch 399/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0802 - val_loss: 0.0416\nEpoch 400/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0793 - val_loss: 0.0413\nEpoch 401/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0784 - val_loss: 0.0410\nEpoch 402/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0775 - val_loss: 0.0407\nEpoch 403/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0767 - val_loss: 0.0403\nEpoch 404/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0758 - val_loss: 0.0401\nEpoch 405/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0750 - val_loss: 0.0398\nEpoch 406/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0742 - val_loss: 0.0395\nEpoch 407/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0734 - val_loss: 0.0392\nEpoch 408/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0726 - val_loss: 0.0390\nEpoch 409/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0718 - val_loss: 0.0388\nEpoch 410/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0711 - val_loss: 0.0386\nEpoch 411/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0704 - val_loss: 0.0384\nEpoch 412/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0696 - val_loss: 0.0382\nEpoch 413/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0689 - val_loss: 0.0380\nEpoch 414/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0682 - val_loss: 0.0378\nEpoch 415/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0676 - val_loss: 0.0376\nEpoch 416/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0669 - val_loss: 0.0374\nEpoch 417/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0663 - val_loss: 0.0373\nEpoch 418/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0656 - val_loss: 0.0371\nEpoch 419/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0650 - val_loss: 0.0370\nEpoch 420/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0644 - val_loss: 0.0369\nEpoch 421/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0638 - val_loss: 0.0367\nEpoch 422/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0632 - val_loss: 0.0366\nEpoch 423/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0626 - val_loss: 0.0365\nEpoch 424/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0620 - val_loss: 0.0364\nEpoch 425/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0615 - val_loss: 0.0363\nEpoch 426/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0609 - val_loss: 0.0362\nEpoch 427/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0604 - val_loss: 0.0361\nEpoch 428/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0599 - val_loss: 0.0360\nEpoch 429/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0594 - val_loss: 0.0359\nEpoch 430/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0589 - val_loss: 0.0358\nEpoch 431/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0584 - val_loss: 0.0358\nEpoch 432/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0579 - val_loss: 0.0357\nEpoch 433/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0574 - val_loss: 0.0356\nEpoch 434/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0570 - val_loss: 0.0356\nEpoch 435/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0565 - val_loss: 0.0355\nEpoch 436/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0561 - val_loss: 0.0355\nEpoch 437/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0556 - val_loss: 0.0354\nEpoch 438/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0552 - val_loss: 0.0354\nEpoch 439/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0548 - val_loss: 0.0353\nEpoch 440/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0544 - val_loss: 0.0353\nEpoch 441/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0540 - val_loss: 0.0353\nEpoch 442/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0536 - val_loss: 0.0352\nEpoch 443/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0532 - val_loss: 0.0352\nEpoch 444/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0528 - val_loss: 0.0352\nEpoch 445/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0525 - val_loss: 0.0352\nEpoch 446/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0521 - val_loss: 0.0351\nEpoch 447/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0518 - val_loss: 0.0351\nEpoch 448/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0514 - val_loss: 0.0351\nEpoch 449/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0511 - val_loss: 0.0351\nEpoch 450/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0507 - val_loss: 0.0351\nEpoch 451/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0504 - val_loss: 0.0350\nEpoch 452/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0501 - val_loss: 0.0350\nEpoch 453/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0498 - val_loss: 0.0350\nEpoch 454/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0495 - val_loss: 0.0350\nEpoch 455/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0492 - val_loss: 0.0350\nEpoch 456/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0489 - val_loss: 0.0350\nEpoch 457/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0486 - val_loss: 0.0350\nEpoch 458/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0483 - val_loss: 0.0350\nEpoch 459/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0480 - val_loss: 0.0350\nEpoch 460/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0478 - val_loss: 0.0350\nEpoch 461/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0475 - val_loss: 0.0350\nEpoch 462/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0473 - val_loss: 0.0350\nEpoch 463/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0470 - val_loss: 0.0350\nEpoch 464/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0468 - val_loss: 0.0350\nEpoch 465/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0465 - val_loss: 0.0350\nEpoch 466/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0463 - val_loss: 0.0350\nEpoch 467/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0460 - val_loss: 0.0350\nEpoch 468/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0458 - val_loss: 0.0350\nEpoch 469/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0456 - val_loss: 0.0350\nEpoch 470/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0454 - val_loss: 0.0350\nEpoch 471/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0452 - val_loss: 0.0351\nEpoch 472/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0449 - val_loss: 0.0351\nEpoch 473/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0447 - val_loss: 0.0351\nEpoch 474/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0445 - val_loss: 0.0351\nEpoch 475/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0443 - val_loss: 0.0351\nEpoch 476/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0442 - val_loss: 0.0351\nEpoch 477/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0440 - val_loss: 0.0351\nEpoch 478/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0438 - val_loss: 0.0351\nEpoch 479/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0436 - val_loss: 0.0351\nEpoch 480/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0434 - val_loss: 0.0351\nEpoch 481/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0432 - val_loss: 0.0351\nEpoch 482/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0431 - val_loss: 0.0351\nEpoch 483/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0429 - val_loss: 0.0351\nEpoch 484/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0428 - val_loss: 0.0352\nEpoch 485/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0426 - val_loss: 0.0352\nEpoch 486/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0424 - val_loss: 0.0352\nEpoch 487/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0423 - val_loss: 0.0352\nEpoch 488/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0421 - val_loss: 0.0352\nEpoch 489/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0420 - val_loss: 0.0352\nEpoch 490/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0418 - val_loss: 0.0352\nEpoch 491/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0417 - val_loss: 0.0352\nEpoch 492/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0416 - val_loss: 0.0352\nEpoch 493/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0414 - val_loss: 0.0352\nEpoch 494/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0413 - val_loss: 0.0352\nEpoch 495/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0412 - val_loss: 0.0352\nEpoch 496/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0411 - val_loss: 0.0353\nEpoch 497/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0409 - val_loss: 0.0353\nEpoch 498/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0408 - val_loss: 0.0353\nEpoch 499/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0407 - val_loss: 0.0353\nEpoch 500/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0406 - val_loss: 0.0353\n\n\n&lt;keras.callbacks.History at 0x7f708be9a470&gt;\n\n\n\nplt.plot(y,'.',alpha=0.1)\nplt.plot(net(X),'--')\n\n\n\n\n\n#\n#%tensorboard --logdir logs --host 0.0.0.0\n\n\n\n텐서보드: 적합결과 시각화\n- 시각화결과는 모두 텐서보드에서 보고 싶다! 적합결과를 보여주는 fig 오브젝트를 텐서보드에 끼워넣어서 출력하는 방법을 알아보자.\n\n#collapse_output\n!rm -rf logs\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(1))\nnet.compile(loss='mse',optimizer='adam')\nnet.fit(X,y,epochs=500,batch_size=100, validation_split=0.1, callbacks=tf.keras.callbacks.TensorBoard())\n\nEpoch 1/500\n9/9 [==============================] - 0s 5ms/step - loss: 11.9832 - val_loss: 10.8663\nEpoch 2/500\n9/9 [==============================] - 0s 2ms/step - loss: 11.8783 - val_loss: 10.7961\nEpoch 3/500\n9/9 [==============================] - 0s 2ms/step - loss: 11.7750 - val_loss: 10.7292\nEpoch 4/500\n9/9 [==============================] - 0s 2ms/step - loss: 11.6712 - val_loss: 10.6628\nEpoch 5/500\n9/9 [==============================] - 0s 2ms/step - loss: 11.5688 - val_loss: 10.5916\nEpoch 6/500\n9/9 [==============================] - 0s 1ms/step - loss: 11.4671 - val_loss: 10.5235\nEpoch 7/500\n9/9 [==============================] - 0s 2ms/step - loss: 11.3650 - val_loss: 10.4527\nEpoch 8/500\n9/9 [==============================] - 0s 2ms/step - loss: 11.2672 - val_loss: 10.3818\nEpoch 9/500\n9/9 [==============================] - 0s 2ms/step - loss: 11.1664 - val_loss: 10.3141\nEpoch 10/500\n9/9 [==============================] - 0s 2ms/step - loss: 11.0684 - val_loss: 10.2464\nEpoch 11/500\n9/9 [==============================] - 0s 2ms/step - loss: 10.9698 - val_loss: 10.1806\nEpoch 12/500\n9/9 [==============================] - 0s 1ms/step - loss: 10.8727 - val_loss: 10.1120\nEpoch 13/500\n9/9 [==============================] - 0s 2ms/step - loss: 10.7777 - val_loss: 10.0403\nEpoch 14/500\n9/9 [==============================] - 0s 1ms/step - loss: 10.6817 - val_loss: 9.9739\nEpoch 15/500\n9/9 [==============================] - 0s 2ms/step - loss: 10.5871 - val_loss: 9.9000\nEpoch 16/500\n9/9 [==============================] - 0s 2ms/step - loss: 10.4931 - val_loss: 9.8320\nEpoch 17/500\n9/9 [==============================] - 0s 1ms/step - loss: 10.3994 - val_loss: 9.7659\nEpoch 18/500\n9/9 [==============================] - 0s 1ms/step - loss: 10.3062 - val_loss: 9.7024\nEpoch 19/500\n9/9 [==============================] - 0s 2ms/step - loss: 10.2162 - val_loss: 9.6333\nEpoch 20/500\n9/9 [==============================] - 0s 2ms/step - loss: 10.1243 - val_loss: 9.5670\nEpoch 21/500\n9/9 [==============================] - 0s 2ms/step - loss: 10.0331 - val_loss: 9.5003\nEpoch 22/500\n9/9 [==============================] - 0s 1ms/step - loss: 9.9449 - val_loss: 9.4318\nEpoch 23/500\n9/9 [==============================] - 0s 1ms/step - loss: 9.8568 - val_loss: 9.3642\nEpoch 24/500\n9/9 [==============================] - 0s 2ms/step - loss: 9.7675 - val_loss: 9.2992\nEpoch 25/500\n9/9 [==============================] - 0s 2ms/step - loss: 9.6794 - val_loss: 9.2379\nEpoch 26/500\n9/9 [==============================] - 0s 2ms/step - loss: 9.5940 - val_loss: 9.1682\nEpoch 27/500\n9/9 [==============================] - 0s 2ms/step - loss: 9.5071 - val_loss: 9.1010\nEpoch 28/500\n9/9 [==============================] - 0s 2ms/step - loss: 9.4226 - val_loss: 9.0350\nEpoch 29/500\n9/9 [==============================] - 0s 2ms/step - loss: 9.3376 - val_loss: 8.9707\nEpoch 30/500\n9/9 [==============================] - 0s 2ms/step - loss: 9.2533 - val_loss: 8.9077\nEpoch 31/500\n9/9 [==============================] - 0s 2ms/step - loss: 9.1692 - val_loss: 8.8398\nEpoch 32/500\n9/9 [==============================] - 0s 2ms/step - loss: 9.0872 - val_loss: 8.7771\nEpoch 33/500\n9/9 [==============================] - 0s 1ms/step - loss: 9.0056 - val_loss: 8.7131\nEpoch 34/500\n9/9 [==============================] - 0s 2ms/step - loss: 8.9235 - val_loss: 8.6456\nEpoch 35/500\n9/9 [==============================] - 0s 2ms/step - loss: 8.8424 - val_loss: 8.5801\nEpoch 36/500\n9/9 [==============================] - 0s 1ms/step - loss: 8.7621 - val_loss: 8.5169\nEpoch 37/500\n9/9 [==============================] - 0s 2ms/step - loss: 8.6824 - val_loss: 8.4507\nEpoch 38/500\n9/9 [==============================] - 0s 2ms/step - loss: 8.6039 - val_loss: 8.3903\nEpoch 39/500\n9/9 [==============================] - 0s 1ms/step - loss: 8.5253 - val_loss: 8.3273\nEpoch 40/500\n9/9 [==============================] - 0s 2ms/step - loss: 8.4475 - val_loss: 8.2648\nEpoch 41/500\n9/9 [==============================] - 0s 2ms/step - loss: 8.3703 - val_loss: 8.2014\nEpoch 42/500\n9/9 [==============================] - 0s 2ms/step - loss: 8.2945 - val_loss: 8.1372\nEpoch 43/500\n9/9 [==============================] - 0s 2ms/step - loss: 8.2177 - val_loss: 8.0751\nEpoch 44/500\n9/9 [==============================] - 0s 1ms/step - loss: 8.1433 - val_loss: 8.0141\nEpoch 45/500\n9/9 [==============================] - 0s 2ms/step - loss: 8.0678 - val_loss: 7.9512\nEpoch 46/500\n9/9 [==============================] - 0s 1ms/step - loss: 7.9936 - val_loss: 7.8923\nEpoch 47/500\n9/9 [==============================] - 0s 2ms/step - loss: 7.9200 - val_loss: 7.8267\nEpoch 48/500\n9/9 [==============================] - 0s 1ms/step - loss: 7.8475 - val_loss: 7.7637\nEpoch 49/500\n9/9 [==============================] - 0s 2ms/step - loss: 7.7742 - val_loss: 7.7034\nEpoch 50/500\n9/9 [==============================] - 0s 2ms/step - loss: 7.7027 - val_loss: 7.6417\nEpoch 51/500\n9/9 [==============================] - 0s 2ms/step - loss: 7.6314 - val_loss: 7.5816\nEpoch 52/500\n9/9 [==============================] - 0s 1ms/step - loss: 7.5596 - val_loss: 7.5203\nEpoch 53/500\n9/9 [==============================] - 0s 2ms/step - loss: 7.4893 - val_loss: 7.4622\nEpoch 54/500\n9/9 [==============================] - 0s 1ms/step - loss: 7.4199 - val_loss: 7.4046\nEpoch 55/500\n9/9 [==============================] - 0s 2ms/step - loss: 7.3517 - val_loss: 7.3413\nEpoch 56/500\n9/9 [==============================] - 0s 1ms/step - loss: 7.2821 - val_loss: 7.2826\nEpoch 57/500\n9/9 [==============================] - 0s 2ms/step - loss: 7.2138 - val_loss: 7.2205\nEpoch 58/500\n9/9 [==============================] - 0s 1ms/step - loss: 7.1461 - val_loss: 7.1610\nEpoch 59/500\n9/9 [==============================] - 0s 2ms/step - loss: 7.0787 - val_loss: 7.1049\nEpoch 60/500\n9/9 [==============================] - 0s 2ms/step - loss: 7.0124 - val_loss: 7.0466\nEpoch 61/500\n9/9 [==============================] - 0s 2ms/step - loss: 6.9463 - val_loss: 6.9873\nEpoch 62/500\n9/9 [==============================] - 0s 1ms/step - loss: 6.8811 - val_loss: 6.9300\nEpoch 63/500\n9/9 [==============================] - 0s 1ms/step - loss: 6.8151 - val_loss: 6.8720\nEpoch 64/500\n9/9 [==============================] - 0s 1ms/step - loss: 6.7508 - val_loss: 6.8148\nEpoch 65/500\n9/9 [==============================] - 0s 2ms/step - loss: 6.6870 - val_loss: 6.7572\nEpoch 66/500\n9/9 [==============================] - 0s 2ms/step - loss: 6.6227 - val_loss: 6.7017\nEpoch 67/500\n9/9 [==============================] - 0s 1ms/step - loss: 6.5596 - val_loss: 6.6439\nEpoch 68/500\n9/9 [==============================] - 0s 2ms/step - loss: 6.4973 - val_loss: 6.5899\nEpoch 69/500\n9/9 [==============================] - 0s 1ms/step - loss: 6.4353 - val_loss: 6.5328\nEpoch 70/500\n9/9 [==============================] - 0s 2ms/step - loss: 6.3738 - val_loss: 6.4786\nEpoch 71/500\n9/9 [==============================] - 0s 2ms/step - loss: 6.3121 - val_loss: 6.4191\nEpoch 72/500\n9/9 [==============================] - 0s 2ms/step - loss: 6.2514 - val_loss: 6.3663\nEpoch 73/500\n9/9 [==============================] - 0s 2ms/step - loss: 6.1910 - val_loss: 6.3117\nEpoch 74/500\n9/9 [==============================] - 0s 2ms/step - loss: 6.1314 - val_loss: 6.2588\nEpoch 75/500\n9/9 [==============================] - 0s 2ms/step - loss: 6.0724 - val_loss: 6.2038\nEpoch 76/500\n9/9 [==============================] - 0s 2ms/step - loss: 6.0138 - val_loss: 6.1501\nEpoch 77/500\n9/9 [==============================] - 0s 2ms/step - loss: 5.9543 - val_loss: 6.0937\nEpoch 78/500\n9/9 [==============================] - 0s 2ms/step - loss: 5.8974 - val_loss: 6.0370\nEpoch 79/500\n9/9 [==============================] - 0s 2ms/step - loss: 5.8396 - val_loss: 5.9847\nEpoch 80/500\n9/9 [==============================] - 0s 2ms/step - loss: 5.7821 - val_loss: 5.9322\nEpoch 81/500\n9/9 [==============================] - 0s 2ms/step - loss: 5.7258 - val_loss: 5.8811\nEpoch 82/500\n9/9 [==============================] - 0s 2ms/step - loss: 5.6695 - val_loss: 5.8268\nEpoch 83/500\n9/9 [==============================] - 0s 2ms/step - loss: 5.6137 - val_loss: 5.7750\nEpoch 84/500\n9/9 [==============================] - 0s 2ms/step - loss: 5.5585 - val_loss: 5.7240\nEpoch 85/500\n9/9 [==============================] - 0s 2ms/step - loss: 5.5036 - val_loss: 5.6720\nEpoch 86/500\n9/9 [==============================] - 0s 2ms/step - loss: 5.4493 - val_loss: 5.6208\nEpoch 87/500\n9/9 [==============================] - 0s 2ms/step - loss: 5.3949 - val_loss: 5.5691\nEpoch 88/500\n9/9 [==============================] - 0s 2ms/step - loss: 5.3418 - val_loss: 5.5173\nEpoch 89/500\n9/9 [==============================] - 0s 2ms/step - loss: 5.2885 - val_loss: 5.4681\nEpoch 90/500\n9/9 [==============================] - 0s 2ms/step - loss: 5.2360 - val_loss: 5.4166\nEpoch 91/500\n9/9 [==============================] - 0s 2ms/step - loss: 5.1837 - val_loss: 5.3662\nEpoch 92/500\n9/9 [==============================] - 0s 2ms/step - loss: 5.1319 - val_loss: 5.3159\nEpoch 93/500\n9/9 [==============================] - 0s 2ms/step - loss: 5.0797 - val_loss: 5.2649\nEpoch 94/500\n9/9 [==============================] - 0s 1ms/step - loss: 5.0286 - val_loss: 5.2175\nEpoch 95/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.9783 - val_loss: 5.1691\nEpoch 96/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.9281 - val_loss: 5.1196\nEpoch 97/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.8785 - val_loss: 5.0692\nEpoch 98/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.8286 - val_loss: 5.0219\nEpoch 99/500\n9/9 [==============================] - 0s 1ms/step - loss: 4.7799 - val_loss: 4.9733\nEpoch 100/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.7310 - val_loss: 4.9264\nEpoch 101/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.6824 - val_loss: 4.8787\nEpoch 102/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.6351 - val_loss: 4.8324\nEpoch 103/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.5871 - val_loss: 4.7854\nEpoch 104/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.5403 - val_loss: 4.7400\nEpoch 105/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.4937 - val_loss: 4.6915\nEpoch 106/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.4473 - val_loss: 4.6465\nEpoch 107/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.4013 - val_loss: 4.5990\nEpoch 108/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.3553 - val_loss: 4.5538\nEpoch 109/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.3105 - val_loss: 4.5088\nEpoch 110/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.2656 - val_loss: 4.4612\nEpoch 111/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.2210 - val_loss: 4.4165\nEpoch 112/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.1765 - val_loss: 4.3754\nEpoch 113/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.1330 - val_loss: 4.3317\nEpoch 114/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.0899 - val_loss: 4.2881\nEpoch 115/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.0470 - val_loss: 4.2423\nEpoch 116/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.0040 - val_loss: 4.1998\nEpoch 117/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.9619 - val_loss: 4.1579\nEpoch 118/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.9198 - val_loss: 4.1154\nEpoch 119/500\n9/9 [==============================] - 0s 1ms/step - loss: 3.8786 - val_loss: 4.0717\nEpoch 120/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.8369 - val_loss: 4.0288\nEpoch 121/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.7964 - val_loss: 3.9861\nEpoch 122/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.7558 - val_loss: 3.9436\nEpoch 123/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.7153 - val_loss: 3.9022\nEpoch 124/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.6752 - val_loss: 3.8638\nEpoch 125/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.6362 - val_loss: 3.8228\nEpoch 126/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.5969 - val_loss: 3.7823\nEpoch 127/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.5579 - val_loss: 3.7407\nEpoch 128/500\n9/9 [==============================] - 0s 1ms/step - loss: 3.5196 - val_loss: 3.7022\nEpoch 129/500\n9/9 [==============================] - 0s 1ms/step - loss: 3.4812 - val_loss: 3.6638\nEpoch 130/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.4437 - val_loss: 3.6228\nEpoch 131/500\n9/9 [==============================] - 0s 1ms/step - loss: 3.4061 - val_loss: 3.5835\nEpoch 132/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.3685 - val_loss: 3.5470\nEpoch 133/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.3320 - val_loss: 3.5076\nEpoch 134/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.2952 - val_loss: 3.4692\nEpoch 135/500\n9/9 [==============================] - 0s 1ms/step - loss: 3.2594 - val_loss: 3.4310\nEpoch 136/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.2230 - val_loss: 3.3932\nEpoch 137/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.1874 - val_loss: 3.3545\nEpoch 138/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.1525 - val_loss: 3.3177\nEpoch 139/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.1174 - val_loss: 3.2801\nEpoch 140/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.0823 - val_loss: 3.2447\nEpoch 141/500\n9/9 [==============================] - 0s 1ms/step - loss: 3.0488 - val_loss: 3.2077\nEpoch 142/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.0140 - val_loss: 3.1742\nEpoch 143/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.9804 - val_loss: 3.1380\nEpoch 144/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.9471 - val_loss: 3.1027\nEpoch 145/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.9140 - val_loss: 3.0676\nEpoch 146/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.8808 - val_loss: 3.0307\nEpoch 147/500\n9/9 [==============================] - 0s 1ms/step - loss: 2.8484 - val_loss: 2.9959\nEpoch 148/500\n9/9 [==============================] - 0s 1ms/step - loss: 2.8164 - val_loss: 2.9628\nEpoch 149/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.7844 - val_loss: 2.9261\nEpoch 150/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.7524 - val_loss: 2.8912\nEpoch 151/500\n9/9 [==============================] - 0s 1ms/step - loss: 2.7214 - val_loss: 2.8600\nEpoch 152/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.6900 - val_loss: 2.8276\nEpoch 153/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.6593 - val_loss: 2.7939\nEpoch 154/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.6286 - val_loss: 2.7605\nEpoch 155/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.5985 - val_loss: 2.7283\nEpoch 156/500\n9/9 [==============================] - 0s 1ms/step - loss: 2.5685 - val_loss: 2.6961\nEpoch 157/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.5390 - val_loss: 2.6633\nEpoch 158/500\n9/9 [==============================] - 0s 1ms/step - loss: 2.5093 - val_loss: 2.6319\nEpoch 159/500\n9/9 [==============================] - 0s 1ms/step - loss: 2.4803 - val_loss: 2.6009\nEpoch 160/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.4516 - val_loss: 2.5689\nEpoch 161/500\n9/9 [==============================] - 0s 1ms/step - loss: 2.4225 - val_loss: 2.5390\nEpoch 162/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.3946 - val_loss: 2.5096\nEpoch 163/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.3666 - val_loss: 2.4766\nEpoch 164/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.3385 - val_loss: 2.4479\nEpoch 165/500\n9/9 [==============================] - 0s 1ms/step - loss: 2.3107 - val_loss: 2.4180\nEpoch 166/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.2840 - val_loss: 2.3889\nEpoch 167/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.2567 - val_loss: 2.3590\nEpoch 168/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.2301 - val_loss: 2.3314\nEpoch 169/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.2035 - val_loss: 2.3024\nEpoch 170/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.1773 - val_loss: 2.2721\nEpoch 171/500\n9/9 [==============================] - 0s 1ms/step - loss: 2.1512 - val_loss: 2.2444\nEpoch 172/500\n9/9 [==============================] - 0s 1ms/step - loss: 2.1257 - val_loss: 2.2158\nEpoch 173/500\n9/9 [==============================] - 0s 1ms/step - loss: 2.0998 - val_loss: 2.1893\nEpoch 174/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.0749 - val_loss: 2.1608\nEpoch 175/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.0499 - val_loss: 2.1329\nEpoch 176/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.0252 - val_loss: 2.1053\nEpoch 177/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.0007 - val_loss: 2.0789\nEpoch 178/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.9762 - val_loss: 2.0535\nEpoch 179/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.9523 - val_loss: 2.0267\nEpoch 180/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.9286 - val_loss: 1.9995\nEpoch 181/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.9051 - val_loss: 1.9733\nEpoch 182/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.8815 - val_loss: 1.9489\nEpoch 183/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.8588 - val_loss: 1.9230\nEpoch 184/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.8357 - val_loss: 1.8970\nEpoch 185/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.8133 - val_loss: 1.8730\nEpoch 186/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.7908 - val_loss: 1.8493\nEpoch 187/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.7688 - val_loss: 1.8231\nEpoch 188/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.7467 - val_loss: 1.7991\nEpoch 189/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.7249 - val_loss: 1.7762\nEpoch 190/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.7032 - val_loss: 1.7522\nEpoch 191/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.6821 - val_loss: 1.7295\nEpoch 192/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.6611 - val_loss: 1.7063\nEpoch 193/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.6404 - val_loss: 1.6835\nEpoch 194/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.6197 - val_loss: 1.6604\nEpoch 195/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.5993 - val_loss: 1.6368\nEpoch 196/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.5791 - val_loss: 1.6148\nEpoch 197/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.5591 - val_loss: 1.5931\nEpoch 198/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.5395 - val_loss: 1.5701\nEpoch 199/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.5196 - val_loss: 1.5490\nEpoch 200/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.5003 - val_loss: 1.5291\nEpoch 201/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.4815 - val_loss: 1.5062\nEpoch 202/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.4623 - val_loss: 1.4843\nEpoch 203/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.4435 - val_loss: 1.4639\nEpoch 204/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.4250 - val_loss: 1.4437\nEpoch 205/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.4067 - val_loss: 1.4226\nEpoch 206/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.3884 - val_loss: 1.4033\nEpoch 207/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.3705 - val_loss: 1.3828\nEpoch 208/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.3529 - val_loss: 1.3636\nEpoch 209/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.3351 - val_loss: 1.3454\nEpoch 210/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.3178 - val_loss: 1.3249\nEpoch 211/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.3007 - val_loss: 1.3058\nEpoch 212/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.2838 - val_loss: 1.2861\nEpoch 213/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.2668 - val_loss: 1.2681\nEpoch 214/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.2502 - val_loss: 1.2499\nEpoch 215/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.2336 - val_loss: 1.2314\nEpoch 216/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.2174 - val_loss: 1.2133\nEpoch 217/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.2013 - val_loss: 1.1955\nEpoch 218/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.1853 - val_loss: 1.1776\nEpoch 219/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.1697 - val_loss: 1.1601\nEpoch 220/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.1541 - val_loss: 1.1430\nEpoch 221/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.1388 - val_loss: 1.1252\nEpoch 222/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.1236 - val_loss: 1.1084\nEpoch 223/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.1084 - val_loss: 1.0921\nEpoch 224/500\n9/9 [==============================] - ETA: 0s - loss: 1.099 - 0s 1ms/step - loss: 1.0936 - val_loss: 1.0763\nEpoch 225/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.0789 - val_loss: 1.0605\nEpoch 226/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.0643 - val_loss: 1.0438\nEpoch 227/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.0500 - val_loss: 1.0274\nEpoch 228/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.0359 - val_loss: 1.0106\nEpoch 229/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.0217 - val_loss: 0.9951\nEpoch 230/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.0079 - val_loss: 0.9800\nEpoch 231/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.9942 - val_loss: 0.9651\nEpoch 232/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.9807 - val_loss: 0.9507\nEpoch 233/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.9674 - val_loss: 0.9347\nEpoch 234/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.9539 - val_loss: 0.9199\nEpoch 235/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.9408 - val_loss: 0.9063\nEpoch 236/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.9279 - val_loss: 0.8911\nEpoch 237/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.9152 - val_loss: 0.8767\nEpoch 238/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.9026 - val_loss: 0.8635\nEpoch 239/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.8902 - val_loss: 0.8494\nEpoch 240/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.8778 - val_loss: 0.8349\nEpoch 241/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.8655 - val_loss: 0.8223\nEpoch 242/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.8536 - val_loss: 0.8099\nEpoch 243/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.8417 - val_loss: 0.7958\nEpoch 244/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.8298 - val_loss: 0.7832\nEpoch 245/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.8182 - val_loss: 0.7705\nEpoch 246/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.8068 - val_loss: 0.7583\nEpoch 247/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.7955 - val_loss: 0.7460\nEpoch 248/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.7844 - val_loss: 0.7334\nEpoch 249/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.7732 - val_loss: 0.7206\nEpoch 250/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.7623 - val_loss: 0.7089\nEpoch 251/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.7516 - val_loss: 0.6966\nEpoch 252/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.7409 - val_loss: 0.6849\nEpoch 253/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.7304 - val_loss: 0.6738\nEpoch 254/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.7199 - val_loss: 0.6619\nEpoch 255/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.7097 - val_loss: 0.6512\nEpoch 256/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.6997 - val_loss: 0.6401\nEpoch 257/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.6896 - val_loss: 0.6294\nEpoch 258/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.6798 - val_loss: 0.6186\nEpoch 259/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.6700 - val_loss: 0.6080\nEpoch 260/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.6604 - val_loss: 0.5974\nEpoch 261/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.6508 - val_loss: 0.5873\nEpoch 262/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.6415 - val_loss: 0.5765\nEpoch 263/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.6322 - val_loss: 0.5665\nEpoch 264/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.6231 - val_loss: 0.5565\nEpoch 265/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.6140 - val_loss: 0.5466\nEpoch 266/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.6052 - val_loss: 0.5370\nEpoch 267/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.5964 - val_loss: 0.5284\nEpoch 268/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.5876 - val_loss: 0.5181\nEpoch 269/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.5792 - val_loss: 0.5088\nEpoch 270/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.5706 - val_loss: 0.4992\nEpoch 271/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.5622 - val_loss: 0.4903\nEpoch 272/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.5541 - val_loss: 0.4816\nEpoch 273/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.5459 - val_loss: 0.4731\nEpoch 274/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.5379 - val_loss: 0.4642\nEpoch 275/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.5300 - val_loss: 0.4553\nEpoch 276/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.5222 - val_loss: 0.4471\nEpoch 277/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.5145 - val_loss: 0.4391\nEpoch 278/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.5069 - val_loss: 0.4313\nEpoch 279/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.4994 - val_loss: 0.4231\nEpoch 280/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.4921 - val_loss: 0.4154\nEpoch 281/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.4847 - val_loss: 0.4079\nEpoch 282/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.4776 - val_loss: 0.4006\nEpoch 283/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.4704 - val_loss: 0.3929\nEpoch 284/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.4634 - val_loss: 0.3851\nEpoch 285/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.4565 - val_loss: 0.3780\nEpoch 286/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.4497 - val_loss: 0.3709\nEpoch 287/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.4430 - val_loss: 0.3632\nEpoch 288/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.4364 - val_loss: 0.3567\nEpoch 289/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.4298 - val_loss: 0.3499\nEpoch 290/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.4234 - val_loss: 0.3432\nEpoch 291/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.4171 - val_loss: 0.3366\nEpoch 292/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.4108 - val_loss: 0.3304\nEpoch 293/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.4047 - val_loss: 0.3237\nEpoch 294/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.3985 - val_loss: 0.3174\nEpoch 295/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.3926 - val_loss: 0.3115\nEpoch 296/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.3867 - val_loss: 0.3055\nEpoch 297/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.3808 - val_loss: 0.2996\nEpoch 298/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.3751 - val_loss: 0.2935\nEpoch 299/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.3694 - val_loss: 0.2875\nEpoch 300/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.3639 - val_loss: 0.2818\nEpoch 301/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.3583 - val_loss: 0.2761\nEpoch 302/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.3530 - val_loss: 0.2708\nEpoch 303/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.3476 - val_loss: 0.2655\nEpoch 304/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.3423 - val_loss: 0.2605\nEpoch 305/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.3372 - val_loss: 0.2553\nEpoch 306/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.3320 - val_loss: 0.2504\nEpoch 307/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.3270 - val_loss: 0.2452\nEpoch 308/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.3221 - val_loss: 0.2401\nEpoch 309/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.3171 - val_loss: 0.2351\nEpoch 310/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.3124 - val_loss: 0.2308\nEpoch 311/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.3076 - val_loss: 0.2260\nEpoch 312/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.3030 - val_loss: 0.2216\nEpoch 313/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.2983 - val_loss: 0.2171\nEpoch 314/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.2938 - val_loss: 0.2126\nEpoch 315/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.2894 - val_loss: 0.2082\nEpoch 316/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.2850 - val_loss: 0.2039\nEpoch 317/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.2806 - val_loss: 0.1997\nEpoch 318/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.2764 - val_loss: 0.1957\nEpoch 319/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.2722 - val_loss: 0.1917\nEpoch 320/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.2681 - val_loss: 0.1879\nEpoch 321/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.2640 - val_loss: 0.1839\nEpoch 322/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.2600 - val_loss: 0.1802\nEpoch 323/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.2560 - val_loss: 0.1765\nEpoch 324/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.2522 - val_loss: 0.1728\nEpoch 325/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.2483 - val_loss: 0.1690\nEpoch 326/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.2446 - val_loss: 0.1656\nEpoch 327/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.2409 - val_loss: 0.1623\nEpoch 328/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.2372 - val_loss: 0.1589\nEpoch 329/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.2337 - val_loss: 0.1556\nEpoch 330/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.2301 - val_loss: 0.1525\nEpoch 331/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.2266 - val_loss: 0.1493\nEpoch 332/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.2232 - val_loss: 0.1462\nEpoch 333/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.2199 - val_loss: 0.1431\nEpoch 334/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.2165 - val_loss: 0.1402\nEpoch 335/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.2133 - val_loss: 0.1375\nEpoch 336/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.2101 - val_loss: 0.1348\nEpoch 337/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.2069 - val_loss: 0.1317\nEpoch 338/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.2038 - val_loss: 0.1289\nEpoch 339/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.2008 - val_loss: 0.1262\nEpoch 340/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1978 - val_loss: 0.1238\nEpoch 341/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1948 - val_loss: 0.1213\nEpoch 342/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1919 - val_loss: 0.1188\nEpoch 343/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1891 - val_loss: 0.1166\nEpoch 344/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1863 - val_loss: 0.1142\nEpoch 345/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1835 - val_loss: 0.1118\nEpoch 346/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1808 - val_loss: 0.1095\nEpoch 347/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1781 - val_loss: 0.1073\nEpoch 348/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1755 - val_loss: 0.1052\nEpoch 349/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1729 - val_loss: 0.1031\nEpoch 350/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1704 - val_loss: 0.1009\nEpoch 351/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1679 - val_loss: 0.0989\nEpoch 352/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1654 - val_loss: 0.0971\nEpoch 353/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1630 - val_loss: 0.0952\nEpoch 354/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1606 - val_loss: 0.0934\nEpoch 355/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1583 - val_loss: 0.0914\nEpoch 356/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1560 - val_loss: 0.0896\nEpoch 357/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1537 - val_loss: 0.0880\nEpoch 358/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1515 - val_loss: 0.0863\nEpoch 359/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1494 - val_loss: 0.0845\nEpoch 360/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1472 - val_loss: 0.0830\nEpoch 361/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1451 - val_loss: 0.0815\nEpoch 362/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1430 - val_loss: 0.0799\nEpoch 363/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1410 - val_loss: 0.0784\nEpoch 364/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1390 - val_loss: 0.0770\nEpoch 365/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1370 - val_loss: 0.0755\nEpoch 366/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1351 - val_loss: 0.0741\nEpoch 367/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1333 - val_loss: 0.0728\nEpoch 368/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1314 - val_loss: 0.0715\nEpoch 369/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1295 - val_loss: 0.0704\nEpoch 370/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1278 - val_loss: 0.0691\nEpoch 371/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1260 - val_loss: 0.0679\nEpoch 372/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1242 - val_loss: 0.0667\nEpoch 373/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1225 - val_loss: 0.0655\nEpoch 374/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1209 - val_loss: 0.0645\nEpoch 375/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1193 - val_loss: 0.0635\nEpoch 376/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1176 - val_loss: 0.0624\nEpoch 377/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1161 - val_loss: 0.0614\nEpoch 378/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1145 - val_loss: 0.0603\nEpoch 379/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1129 - val_loss: 0.0594\nEpoch 380/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1115 - val_loss: 0.0584\nEpoch 381/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1100 - val_loss: 0.0575\nEpoch 382/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1086 - val_loss: 0.0567\nEpoch 383/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1071 - val_loss: 0.0559\nEpoch 384/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1058 - val_loss: 0.0551\nEpoch 385/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1044 - val_loss: 0.0543\nEpoch 386/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1030 - val_loss: 0.0536\nEpoch 387/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1017 - val_loss: 0.0528\nEpoch 388/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1004 - val_loss: 0.0520\nEpoch 389/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0992 - val_loss: 0.0513\nEpoch 390/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0979 - val_loss: 0.0507\nEpoch 391/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0967 - val_loss: 0.0500\nEpoch 392/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0955 - val_loss: 0.0495\nEpoch 393/500\n9/9 [==============================] - 0s 4ms/step - loss: 0.0943 - val_loss: 0.0489\nEpoch 394/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0931 - val_loss: 0.0482\nEpoch 395/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0920 - val_loss: 0.0477\nEpoch 396/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0909 - val_loss: 0.0471\nEpoch 397/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0898 - val_loss: 0.0465\nEpoch 398/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0887 - val_loss: 0.0461\nEpoch 399/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0877 - val_loss: 0.0456\nEpoch 400/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0866 - val_loss: 0.0451\nEpoch 401/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0856 - val_loss: 0.0446\nEpoch 402/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0846 - val_loss: 0.0442\nEpoch 403/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0836 - val_loss: 0.0437\nEpoch 404/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0827 - val_loss: 0.0433\nEpoch 405/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0817 - val_loss: 0.0430\nEpoch 406/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0808 - val_loss: 0.0425\nEpoch 407/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0799 - val_loss: 0.0422\nEpoch 408/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0790 - val_loss: 0.0418\nEpoch 409/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0782 - val_loss: 0.0415\nEpoch 410/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0773 - val_loss: 0.0411\nEpoch 411/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0765 - val_loss: 0.0408\nEpoch 412/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0756 - val_loss: 0.0405\nEpoch 413/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0749 - val_loss: 0.0403\nEpoch 414/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0740 - val_loss: 0.0400\nEpoch 415/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0733 - val_loss: 0.0397\nEpoch 416/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0725 - val_loss: 0.0394\nEpoch 417/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0718 - val_loss: 0.0392\nEpoch 418/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0710 - val_loss: 0.0389\nEpoch 419/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0703 - val_loss: 0.0387\nEpoch 420/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0696 - val_loss: 0.0385\nEpoch 421/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0689 - val_loss: 0.0383\nEpoch 422/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0682 - val_loss: 0.0381\nEpoch 423/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0676 - val_loss: 0.0379\nEpoch 424/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0669 - val_loss: 0.0377\nEpoch 425/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0663 - val_loss: 0.0376\nEpoch 426/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0657 - val_loss: 0.0374\nEpoch 427/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0650 - val_loss: 0.0372\nEpoch 428/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0644 - val_loss: 0.0371\nEpoch 429/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0638 - val_loss: 0.0370\nEpoch 430/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0633 - val_loss: 0.0368\nEpoch 431/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0627 - val_loss: 0.0367\nEpoch 432/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0621 - val_loss: 0.0366\nEpoch 433/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0616 - val_loss: 0.0365\nEpoch 434/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0611 - val_loss: 0.0364\nEpoch 435/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0605 - val_loss: 0.0363\nEpoch 436/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0600 - val_loss: 0.0362\nEpoch 437/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0595 - val_loss: 0.0361\nEpoch 438/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0590 - val_loss: 0.0360\nEpoch 439/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0585 - val_loss: 0.0359\nEpoch 440/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0580 - val_loss: 0.0358\nEpoch 441/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0576 - val_loss: 0.0357\nEpoch 442/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0571 - val_loss: 0.0357\nEpoch 443/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0567 - val_loss: 0.0356\nEpoch 444/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0562 - val_loss: 0.0355\nEpoch 445/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0558 - val_loss: 0.0355\nEpoch 446/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0554 - val_loss: 0.0354\nEpoch 447/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0550 - val_loss: 0.0354\nEpoch 448/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0546 - val_loss: 0.0353\nEpoch 449/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0542 - val_loss: 0.0353\nEpoch 450/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0538 - val_loss: 0.0353\nEpoch 451/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0534 - val_loss: 0.0352\nEpoch 452/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0530 - val_loss: 0.0352\nEpoch 453/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0527 - val_loss: 0.0352\nEpoch 454/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0523 - val_loss: 0.0351\nEpoch 455/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0519 - val_loss: 0.0351\nEpoch 456/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0516 - val_loss: 0.0351\nEpoch 457/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0513 - val_loss: 0.0351\nEpoch 458/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0509 - val_loss: 0.0350\nEpoch 459/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0506 - val_loss: 0.0350\nEpoch 460/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0503 - val_loss: 0.0350\nEpoch 461/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0500 - val_loss: 0.0350\nEpoch 462/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0497 - val_loss: 0.0350\nEpoch 463/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0494 - val_loss: 0.0350\nEpoch 464/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0491 - val_loss: 0.0350\nEpoch 465/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0488 - val_loss: 0.0350\nEpoch 466/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0485 - val_loss: 0.0350\nEpoch 467/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0482 - val_loss: 0.0350\nEpoch 468/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0479 - val_loss: 0.0350\nEpoch 469/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0477 - val_loss: 0.0350\nEpoch 470/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0474 - val_loss: 0.0350\nEpoch 471/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0472 - val_loss: 0.0350\nEpoch 472/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0469 - val_loss: 0.0350\nEpoch 473/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0467 - val_loss: 0.0350\nEpoch 474/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0464 - val_loss: 0.0350\nEpoch 475/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0462 - val_loss: 0.0350\nEpoch 476/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0460 - val_loss: 0.0350\nEpoch 477/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0457 - val_loss: 0.0350\nEpoch 478/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0455 - val_loss: 0.0350\nEpoch 479/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0453 - val_loss: 0.0350\nEpoch 480/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0451 - val_loss: 0.0350\nEpoch 481/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0449 - val_loss: 0.0350\nEpoch 482/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0447 - val_loss: 0.0350\nEpoch 483/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0445 - val_loss: 0.0350\nEpoch 484/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0443 - val_loss: 0.0350\nEpoch 485/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0441 - val_loss: 0.0350\nEpoch 486/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0439 - val_loss: 0.0350\nEpoch 487/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0437 - val_loss: 0.0350\nEpoch 488/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0436 - val_loss: 0.0350\nEpoch 489/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0434 - val_loss: 0.0351\nEpoch 490/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0432 - val_loss: 0.0351\nEpoch 491/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0430 - val_loss: 0.0351\nEpoch 492/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0429 - val_loss: 0.0351\nEpoch 493/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0427 - val_loss: 0.0351\nEpoch 494/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0425 - val_loss: 0.0351\nEpoch 495/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0424 - val_loss: 0.0351\nEpoch 496/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0422 - val_loss: 0.0351\nEpoch 497/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0421 - val_loss: 0.0351\nEpoch 498/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0419 - val_loss: 0.0351\nEpoch 499/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0418 - val_loss: 0.0351\nEpoch 500/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0417 - val_loss: 0.0351\n\n\n&lt;keras.callbacks.History at 0x7f70988babc0&gt;\n\n\n\n#\n#%tensorboard --logdir logs --host 0.0.0.0\n\n- 끼워넣을 오브젝트 만들기\n\nfig, ax = plt.subplots()\nax.plot(y,'.',alpha=0.2)\nax.plot(net(X),'--')\n\n\n\n\n\nfig\n\n\n\n\n- 이제 fig 오브젝트를 끼워넣을 코드를 구성하자. (공식홈페이지 참고)\n\nhttps://www.tensorflow.org/tensorboard/image_summaries\n\n\n# 이코드는 한번만 실행\n#from datetime import datetime\nimport io\nlogdir = \"logs\"\n#logdir = \"logs\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n\ndef plot_to_image(fig): # 사용자가 지정한 그림오브젝트 fig를 넣으면 텐서보드에 끼워넣을수 있는 형태로 출력해주는 함수\n    \"\"\"Converts the matplotlib plot specified by 'figure' to a PNG image and\n    returns it. The supplied figure is closed and inaccessible after this call.\"\"\"\n    # Save the plot to a PNG in memory.\n    buf = io.BytesIO()\n    fig.savefig(buf, format='png')\n    # Closing the figure prevents it from being displayed directly inside\n    # the notebook.\n    plt.close(fig)\n    buf.seek(0)\n    # Convert PNG buffer to TF image\n    image = tf.image.decode_png(buf.getvalue(), channels=4)\n    # Add the batch dimension\n    image = tf.expand_dims(image, 0)\n    return image\n\n\nwith tf.summary.create_file_writer(logdir).as_default():\n    tf.summary.image(\"적합결과시각화\", plot_to_image(fig), step=0)\n\n\n#\n#%tensorboard --logdir logs --host 0.0.0.0\n\n\n\n\n학습과정분석\n\n텐서보드: 가중치 시각화\n- 에폭별로 가중치가 수렴하는 모양을 보고 싶다.\n3-(1) 아래와 같은 모형을 고려하자.\n\\[y_i= \\beta_0 + \\sum_{k=1}^{5} \\beta_k \\cos(k t_i)+\\epsilon_i\\]\n여기에서 \\(t=(t_1,\\dots,t_{1000})=\\) np.linspace(0,5,1000) 이다. 그리고 \\(\\epsilon_i \\sim i.i.d~ N(0,\\sigma^2)\\), 즉 서로 독립인 표준정규분포에서 추출된 샘플이다. 위의 모형에서 아래와 같은 데이터를 관측했다고 가정하자.\n\nnp.random.seed(43052)\nt= np.linspace(0,5,1000)\ny = -2+ 3*np.cos(t) + 1*np.cos(2*t) + 0.5*np.cos(5*t) + np.random.randn(1000)*0.2\nplt.plot(t,y,'.',alpha=0.1)\n\n\n\n\n- 학습을 진행하면서 가중치가 어떻게 업데이트 되는지 시각화하자.\n\n# y = -2+ 3*np.cos(t) + 1*np.cos(2*t) + 0.5*np.cos(5*t) + np.random.randn(1000)*0.2\n\n\n#collapse_output\n!rm -rf logs\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(1))\nnet.compile(loss='mse',optimizer='adam')\ncb1= tf.keras.callbacks.TensorBoard(update_freq='epoch',histogram_freq=100)\nnet.fit(X,y,epochs=2000, batch_size=100, validation_split=0.45,callbacks=cb1)\n\nEpoch 1/2000\n6/6 [==============================] - 0s 10ms/step - loss: 10.5761 - val_loss: 13.6319\nEpoch 2/2000\n6/6 [==============================] - 0s 3ms/step - loss: 10.5050 - val_loss: 13.5579\nEpoch 3/2000\n6/6 [==============================] - 0s 3ms/step - loss: 10.4352 - val_loss: 13.4839\nEpoch 4/2000\n6/6 [==============================] - 0s 3ms/step - loss: 10.3643 - val_loss: 13.4108\nEpoch 5/2000\n6/6 [==============================] - 0s 3ms/step - loss: 10.2954 - val_loss: 13.3387\nEpoch 6/2000\n6/6 [==============================] - 0s 4ms/step - loss: 10.2266 - val_loss: 13.2667\nEpoch 7/2000\n6/6 [==============================] - 0s 3ms/step - loss: 10.1574 - val_loss: 13.1948\nEpoch 8/2000\n6/6 [==============================] - 0s 3ms/step - loss: 10.0893 - val_loss: 13.1239\nEpoch 9/2000\n6/6 [==============================] - 0s 3ms/step - loss: 10.0220 - val_loss: 13.0539\nEpoch 10/2000\n6/6 [==============================] - 0s 3ms/step - loss: 9.9545 - val_loss: 12.9839\nEpoch 11/2000\n6/6 [==============================] - 0s 3ms/step - loss: 9.8885 - val_loss: 12.9143\nEpoch 12/2000\n6/6 [==============================] - 0s 3ms/step - loss: 9.8218 - val_loss: 12.8446\nEpoch 13/2000\n6/6 [==============================] - 0s 3ms/step - loss: 9.7546 - val_loss: 12.7763\nEpoch 14/2000\n6/6 [==============================] - 0s 3ms/step - loss: 9.6900 - val_loss: 12.7072\nEpoch 15/2000\n6/6 [==============================] - 0s 4ms/step - loss: 9.6251 - val_loss: 12.6390\nEpoch 16/2000\n6/6 [==============================] - 0s 4ms/step - loss: 9.5592 - val_loss: 12.5710\nEpoch 17/2000\n6/6 [==============================] - 0s 3ms/step - loss: 9.4956 - val_loss: 12.5030\nEpoch 18/2000\n6/6 [==============================] - 0s 3ms/step - loss: 9.4315 - val_loss: 12.4345\nEpoch 19/2000\n6/6 [==============================] - 0s 3ms/step - loss: 9.3670 - val_loss: 12.3678\nEpoch 20/2000\n6/6 [==============================] - 0s 3ms/step - loss: 9.3029 - val_loss: 12.3018\nEpoch 21/2000\n6/6 [==============================] - 0s 3ms/step - loss: 9.2407 - val_loss: 12.2358\nEpoch 22/2000\n6/6 [==============================] - 0s 3ms/step - loss: 9.1777 - val_loss: 12.1697\nEpoch 23/2000\n6/6 [==============================] - 0s 3ms/step - loss: 9.1158 - val_loss: 12.1035\nEpoch 24/2000\n6/6 [==============================] - 0s 3ms/step - loss: 9.0525 - val_loss: 12.0391\nEpoch 25/2000\n6/6 [==============================] - 0s 3ms/step - loss: 8.9924 - val_loss: 11.9733\nEpoch 26/2000\n6/6 [==============================] - 0s 3ms/step - loss: 8.9292 - val_loss: 11.9101\nEpoch 27/2000\n6/6 [==============================] - 0s 3ms/step - loss: 8.8694 - val_loss: 11.8461\nEpoch 28/2000\n6/6 [==============================] - 0s 3ms/step - loss: 8.8079 - val_loss: 11.7831\nEpoch 29/2000\n6/6 [==============================] - 0s 3ms/step - loss: 8.7476 - val_loss: 11.7193\nEpoch 30/2000\n6/6 [==============================] - 0s 3ms/step - loss: 8.6889 - val_loss: 11.6557\nEpoch 31/2000\n6/6 [==============================] - 0s 3ms/step - loss: 8.6272 - val_loss: 11.5937\nEpoch 32/2000\n6/6 [==============================] - 0s 3ms/step - loss: 8.5683 - val_loss: 11.5312\nEpoch 33/2000\n6/6 [==============================] - 0s 3ms/step - loss: 8.5101 - val_loss: 11.4687\nEpoch 34/2000\n6/6 [==============================] - 0s 3ms/step - loss: 8.4503 - val_loss: 11.4082\nEpoch 35/2000\n6/6 [==============================] - 0s 3ms/step - loss: 8.3924 - val_loss: 11.3462\nEpoch 36/2000\n6/6 [==============================] - 0s 3ms/step - loss: 8.3351 - val_loss: 11.2848\nEpoch 37/2000\n6/6 [==============================] - 0s 3ms/step - loss: 8.2768 - val_loss: 11.2241\nEpoch 38/2000\n6/6 [==============================] - 0s 3ms/step - loss: 8.2203 - val_loss: 11.1644\nEpoch 39/2000\n6/6 [==============================] - 0s 3ms/step - loss: 8.1620 - val_loss: 11.1063\nEpoch 40/2000\n6/6 [==============================] - 0s 3ms/step - loss: 8.1069 - val_loss: 11.0478\nEpoch 41/2000\n6/6 [==============================] - 0s 3ms/step - loss: 8.0510 - val_loss: 10.9894\nEpoch 42/2000\n6/6 [==============================] - 0s 3ms/step - loss: 7.9946 - val_loss: 10.9322\nEpoch 43/2000\n6/6 [==============================] - 0s 3ms/step - loss: 7.9396 - val_loss: 10.8745\nEpoch 44/2000\n6/6 [==============================] - 0s 3ms/step - loss: 7.8851 - val_loss: 10.8172\nEpoch 45/2000\n6/6 [==============================] - 0s 3ms/step - loss: 7.8309 - val_loss: 10.7601\nEpoch 46/2000\n6/6 [==============================] - 0s 4ms/step - loss: 7.7758 - val_loss: 10.7029\nEpoch 47/2000\n6/6 [==============================] - 0s 3ms/step - loss: 7.7227 - val_loss: 10.6449\nEpoch 48/2000\n6/6 [==============================] - 0s 3ms/step - loss: 7.6674 - val_loss: 10.5891\nEpoch 49/2000\n6/6 [==============================] - 0s 3ms/step - loss: 7.6149 - val_loss: 10.5327\nEpoch 50/2000\n6/6 [==============================] - 0s 3ms/step - loss: 7.5614 - val_loss: 10.4775\nEpoch 51/2000\n6/6 [==============================] - 0s 3ms/step - loss: 7.5091 - val_loss: 10.4221\nEpoch 52/2000\n6/6 [==============================] - 0s 4ms/step - loss: 7.4576 - val_loss: 10.3669\nEpoch 53/2000\n6/6 [==============================] - 0s 4ms/step - loss: 7.4050 - val_loss: 10.3126\nEpoch 54/2000\n6/6 [==============================] - 0s 3ms/step - loss: 7.3522 - val_loss: 10.2590\nEpoch 55/2000\n6/6 [==============================] - 0s 3ms/step - loss: 7.3023 - val_loss: 10.2050\nEpoch 56/2000\n6/6 [==============================] - 0s 4ms/step - loss: 7.2503 - val_loss: 10.1516\nEpoch 57/2000\n6/6 [==============================] - 0s 4ms/step - loss: 7.2003 - val_loss: 10.0971\nEpoch 58/2000\n6/6 [==============================] - 0s 3ms/step - loss: 7.1490 - val_loss: 10.0436\nEpoch 59/2000\n6/6 [==============================] - 0s 3ms/step - loss: 7.0988 - val_loss: 9.9912\nEpoch 60/2000\n6/6 [==============================] - 0s 3ms/step - loss: 7.0486 - val_loss: 9.9394\nEpoch 61/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.9988 - val_loss: 9.8870\nEpoch 62/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.9508 - val_loss: 9.8347\nEpoch 63/2000\n6/6 [==============================] - 0s 4ms/step - loss: 6.9000 - val_loss: 9.7832\nEpoch 64/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.8507 - val_loss: 9.7309\nEpoch 65/2000\n6/6 [==============================] - 0s 4ms/step - loss: 6.8023 - val_loss: 9.6794\nEpoch 66/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.7536 - val_loss: 9.6278\nEpoch 67/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.7058 - val_loss: 9.5765\nEpoch 68/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.6573 - val_loss: 9.5260\nEpoch 69/2000\n6/6 [==============================] - 0s 4ms/step - loss: 6.6098 - val_loss: 9.4752\nEpoch 70/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.5625 - val_loss: 9.4245\nEpoch 71/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.5147 - val_loss: 9.3755\nEpoch 72/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.4692 - val_loss: 9.3261\nEpoch 73/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.4218 - val_loss: 9.2776\nEpoch 74/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.3757 - val_loss: 9.2286\nEpoch 75/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.3297 - val_loss: 9.1803\nEpoch 76/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.2845 - val_loss: 9.1314\nEpoch 77/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.2383 - val_loss: 9.0835\nEpoch 78/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.1934 - val_loss: 9.0348\nEpoch 79/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.1484 - val_loss: 8.9875\nEpoch 80/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.1032 - val_loss: 8.9405\nEpoch 81/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.0604 - val_loss: 8.8922\nEpoch 82/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.0153 - val_loss: 8.8456\nEpoch 83/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.9708 - val_loss: 8.7999\nEpoch 84/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.9286 - val_loss: 8.7539\nEpoch 85/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.8848 - val_loss: 8.7093\nEpoch 86/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.8427 - val_loss: 8.6642\nEpoch 87/2000\n6/6 [==============================] - 0s 4ms/step - loss: 5.8006 - val_loss: 8.6189\nEpoch 88/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.7591 - val_loss: 8.5736\nEpoch 89/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.7162 - val_loss: 8.5291\nEpoch 90/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.6751 - val_loss: 8.4846\nEpoch 91/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.6328 - val_loss: 8.4410\nEpoch 92/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.5924 - val_loss: 8.3969\nEpoch 93/2000\n6/6 [==============================] - 0s 4ms/step - loss: 5.5510 - val_loss: 8.3528\nEpoch 94/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.5104 - val_loss: 8.3092\nEpoch 95/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.4697 - val_loss: 8.2662\nEpoch 96/2000\n6/6 [==============================] - 0s 4ms/step - loss: 5.4291 - val_loss: 8.2228\nEpoch 97/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.3892 - val_loss: 8.1801\nEpoch 98/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.3491 - val_loss: 8.1378\nEpoch 99/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.3104 - val_loss: 8.0953\nEpoch 100/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.2704 - val_loss: 8.0537\nEpoch 101/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.2316 - val_loss: 8.0121\nEpoch 102/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.1927 - val_loss: 7.9702\nEpoch 103/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.1547 - val_loss: 7.9291\nEpoch 104/2000\n6/6 [==============================] - 0s 4ms/step - loss: 5.1153 - val_loss: 7.8887\nEpoch 105/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.0776 - val_loss: 7.8474\nEpoch 106/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.0398 - val_loss: 7.8072\nEpoch 107/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.0018 - val_loss: 7.7670\nEpoch 108/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.9650 - val_loss: 7.7268\nEpoch 109/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.9272 - val_loss: 7.6866\nEpoch 110/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.8898 - val_loss: 7.6479\nEpoch 111/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.8535 - val_loss: 7.6090\nEpoch 112/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.8171 - val_loss: 7.5696\nEpoch 113/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.7808 - val_loss: 7.5310\nEpoch 114/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.7459 - val_loss: 7.4919\nEpoch 115/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.7091 - val_loss: 7.4538\nEpoch 116/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.6743 - val_loss: 7.4156\nEpoch 117/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.6391 - val_loss: 7.3770\nEpoch 118/2000\n6/6 [==============================] - 0s 4ms/step - loss: 4.6037 - val_loss: 7.3395\nEpoch 119/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.5691 - val_loss: 7.3022\nEpoch 120/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.5346 - val_loss: 7.2650\nEpoch 121/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.5012 - val_loss: 7.2272\nEpoch 122/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.4662 - val_loss: 7.1898\nEpoch 123/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.4323 - val_loss: 7.1527\nEpoch 124/2000\n6/6 [==============================] - 0s 4ms/step - loss: 4.3984 - val_loss: 7.1157\nEpoch 125/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.3651 - val_loss: 7.0795\nEpoch 126/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.3322 - val_loss: 7.0431\nEpoch 127/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.2990 - val_loss: 7.0075\nEpoch 128/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.2665 - val_loss: 6.9722\nEpoch 129/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.2339 - val_loss: 6.9372\nEpoch 130/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.2018 - val_loss: 6.9018\nEpoch 131/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.1699 - val_loss: 6.8666\nEpoch 132/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.1382 - val_loss: 6.8318\nEpoch 133/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.1064 - val_loss: 6.7976\nEpoch 134/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.0750 - val_loss: 6.7637\nEpoch 135/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.0437 - val_loss: 6.7300\nEpoch 136/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.0137 - val_loss: 6.6962\nEpoch 137/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.9831 - val_loss: 6.6622\nEpoch 138/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.9520 - val_loss: 6.6287\nEpoch 139/2000\n6/6 [==============================] - 0s 4ms/step - loss: 3.9218 - val_loss: 6.5953\nEpoch 140/2000\n6/6 [==============================] - 0s 4ms/step - loss: 3.8921 - val_loss: 6.5620\nEpoch 141/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.8617 - val_loss: 6.5292\nEpoch 142/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.8319 - val_loss: 6.4965\nEpoch 143/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.8020 - val_loss: 6.4639\nEpoch 144/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.7732 - val_loss: 6.4315\nEpoch 145/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.7435 - val_loss: 6.3991\nEpoch 146/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.7143 - val_loss: 6.3669\nEpoch 147/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.6857 - val_loss: 6.3345\nEpoch 148/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.6562 - val_loss: 6.3026\nEpoch 149/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.6286 - val_loss: 6.2697\nEpoch 150/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.5990 - val_loss: 6.2380\nEpoch 151/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.5708 - val_loss: 6.2063\nEpoch 152/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.5426 - val_loss: 6.1751\nEpoch 153/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.5147 - val_loss: 6.1438\nEpoch 154/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.4869 - val_loss: 6.1125\nEpoch 155/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.4598 - val_loss: 6.0812\nEpoch 156/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.4324 - val_loss: 6.0509\nEpoch 157/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.4057 - val_loss: 6.0204\nEpoch 158/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.3787 - val_loss: 5.9906\nEpoch 159/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.3523 - val_loss: 5.9610\nEpoch 160/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.3257 - val_loss: 5.9312\nEpoch 161/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.2997 - val_loss: 5.9018\nEpoch 162/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.2740 - val_loss: 5.8723\nEpoch 163/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.2478 - val_loss: 5.8435\nEpoch 164/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.2222 - val_loss: 5.8143\nEpoch 165/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.1972 - val_loss: 5.7853\nEpoch 166/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.1719 - val_loss: 5.7565\nEpoch 167/2000\n6/6 [==============================] - 0s 4ms/step - loss: 3.1462 - val_loss: 5.7283\nEpoch 168/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.1213 - val_loss: 5.7000\nEpoch 169/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.0966 - val_loss: 5.6719\nEpoch 170/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.0715 - val_loss: 5.6443\nEpoch 171/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.0475 - val_loss: 5.6166\nEpoch 172/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.0236 - val_loss: 5.5888\nEpoch 173/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.9992 - val_loss: 5.5612\nEpoch 174/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.9749 - val_loss: 5.5341\nEpoch 175/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.9514 - val_loss: 5.5070\nEpoch 176/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.9280 - val_loss: 5.4798\nEpoch 177/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.9046 - val_loss: 5.4530\nEpoch 178/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.8809 - val_loss: 5.4262\nEpoch 179/2000\n6/6 [==============================] - 0s 4ms/step - loss: 2.8579 - val_loss: 5.3988\nEpoch 180/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.8347 - val_loss: 5.3721\nEpoch 181/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.8117 - val_loss: 5.3455\nEpoch 182/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.7892 - val_loss: 5.3190\nEpoch 183/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.7668 - val_loss: 5.2926\nEpoch 184/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.7439 - val_loss: 5.2664\nEpoch 185/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.7219 - val_loss: 5.2400\nEpoch 186/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.6998 - val_loss: 5.2143\nEpoch 187/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.6775 - val_loss: 5.1886\nEpoch 188/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.6560 - val_loss: 5.1632\nEpoch 189/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.6344 - val_loss: 5.1375\nEpoch 190/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.6128 - val_loss: 5.1125\nEpoch 191/2000\n6/6 [==============================] - 0s 2ms/step - loss: 2.5919 - val_loss: 5.0868\nEpoch 192/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.5703 - val_loss: 5.0617\nEpoch 193/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.5491 - val_loss: 5.0373\nEpoch 194/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.5285 - val_loss: 5.0125\nEpoch 195/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.5071 - val_loss: 4.9879\nEpoch 196/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.4868 - val_loss: 4.9628\nEpoch 197/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.4659 - val_loss: 4.9382\nEpoch 198/2000\n6/6 [==============================] - 0s 4ms/step - loss: 2.4455 - val_loss: 4.9135\nEpoch 199/2000\n6/6 [==============================] - 0s 4ms/step - loss: 2.4248 - val_loss: 4.8893\nEpoch 200/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.4049 - val_loss: 4.8653\nEpoch 201/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.3847 - val_loss: 4.8420\nEpoch 202/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.3649 - val_loss: 4.8186\nEpoch 203/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.3451 - val_loss: 4.7950\nEpoch 204/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.3261 - val_loss: 4.7714\nEpoch 205/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.3060 - val_loss: 4.7487\nEpoch 206/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.2871 - val_loss: 4.7257\nEpoch 207/2000\n6/6 [==============================] - 0s 4ms/step - loss: 2.2681 - val_loss: 4.7027\nEpoch 208/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.2489 - val_loss: 4.6803\nEpoch 209/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.2303 - val_loss: 4.6579\nEpoch 210/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.2115 - val_loss: 4.6353\nEpoch 211/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.1931 - val_loss: 4.6133\nEpoch 212/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.1750 - val_loss: 4.5909\nEpoch 213/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.1568 - val_loss: 4.5686\nEpoch 214/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.1387 - val_loss: 4.5462\nEpoch 215/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.1208 - val_loss: 4.5243\nEpoch 216/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.1031 - val_loss: 4.5023\nEpoch 217/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.0855 - val_loss: 4.4811\nEpoch 218/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.0677 - val_loss: 4.4596\nEpoch 219/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.0503 - val_loss: 4.4384\nEpoch 220/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.0331 - val_loss: 4.4170\nEpoch 221/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.0158 - val_loss: 4.3960\nEpoch 222/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.9988 - val_loss: 4.3747\nEpoch 223/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.9816 - val_loss: 4.3535\nEpoch 224/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.9646 - val_loss: 4.3322\nEpoch 225/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.9478 - val_loss: 4.3112\nEpoch 226/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.9309 - val_loss: 4.2906\nEpoch 227/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.9147 - val_loss: 4.2702\nEpoch 228/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.8978 - val_loss: 4.2495\nEpoch 229/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.8817 - val_loss: 4.2287\nEpoch 230/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.8654 - val_loss: 4.2084\nEpoch 231/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.8493 - val_loss: 4.1881\nEpoch 232/2000\n6/6 [==============================] - 0s 4ms/step - loss: 1.8336 - val_loss: 4.1677\nEpoch 233/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.8177 - val_loss: 4.1479\nEpoch 234/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.8023 - val_loss: 4.1278\nEpoch 235/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.7865 - val_loss: 4.1084\nEpoch 236/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.7710 - val_loss: 4.0889\nEpoch 237/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.7556 - val_loss: 4.0694\nEpoch 238/2000\n6/6 [==============================] - 0s 4ms/step - loss: 1.7405 - val_loss: 4.0502\nEpoch 239/2000\n6/6 [==============================] - 0s 4ms/step - loss: 1.7254 - val_loss: 4.0309\nEpoch 240/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.7103 - val_loss: 4.0120\nEpoch 241/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.6959 - val_loss: 3.9928\nEpoch 242/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.6808 - val_loss: 3.9739\nEpoch 243/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.6661 - val_loss: 3.9554\nEpoch 244/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.6518 - val_loss: 3.9367\nEpoch 245/2000\n6/6 [==============================] - 0s 4ms/step - loss: 1.6372 - val_loss: 3.9182\nEpoch 246/2000\n6/6 [==============================] - 0s 4ms/step - loss: 1.6232 - val_loss: 3.8994\nEpoch 247/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.6087 - val_loss: 3.8812\nEpoch 248/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.5946 - val_loss: 3.8630\nEpoch 249/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.5808 - val_loss: 3.8450\nEpoch 250/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.5669 - val_loss: 3.8269\nEpoch 251/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.5532 - val_loss: 3.8090\nEpoch 252/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.5396 - val_loss: 3.7908\nEpoch 253/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.5259 - val_loss: 3.7723\nEpoch 254/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.5123 - val_loss: 3.7543\nEpoch 255/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.4990 - val_loss: 3.7366\nEpoch 256/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.4855 - val_loss: 3.7190\nEpoch 257/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.4724 - val_loss: 3.7013\nEpoch 258/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.4592 - val_loss: 3.6839\nEpoch 259/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.4462 - val_loss: 3.6666\nEpoch 260/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.4335 - val_loss: 3.6491\nEpoch 261/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.4207 - val_loss: 3.6321\nEpoch 262/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.4078 - val_loss: 3.6151\nEpoch 263/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.3955 - val_loss: 3.5979\nEpoch 264/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.3828 - val_loss: 3.5807\nEpoch 265/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.3705 - val_loss: 3.5637\nEpoch 266/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.3581 - val_loss: 3.5469\nEpoch 267/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.3462 - val_loss: 3.5302\nEpoch 268/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.3340 - val_loss: 3.5136\nEpoch 269/2000\n6/6 [==============================] - 0s 4ms/step - loss: 1.3218 - val_loss: 3.4973\nEpoch 270/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.3100 - val_loss: 3.4807\nEpoch 271/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.2982 - val_loss: 3.4645\nEpoch 272/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.2865 - val_loss: 3.4483\nEpoch 273/2000\n6/6 [==============================] - 0s 4ms/step - loss: 1.2750 - val_loss: 3.4321\nEpoch 274/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.2634 - val_loss: 3.4163\nEpoch 275/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.2521 - val_loss: 3.4002\nEpoch 276/2000\n6/6 [==============================] - 0s 2ms/step - loss: 1.2408 - val_loss: 3.3844\nEpoch 277/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.2293 - val_loss: 3.3688\nEpoch 278/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.2182 - val_loss: 3.3532\nEpoch 279/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.2075 - val_loss: 3.3375\nEpoch 280/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.1963 - val_loss: 3.3221\nEpoch 281/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.1854 - val_loss: 3.3067\nEpoch 282/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.1747 - val_loss: 3.2914\nEpoch 283/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.1640 - val_loss: 3.2761\nEpoch 284/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.1534 - val_loss: 3.2609\nEpoch 285/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.1429 - val_loss: 3.2457\nEpoch 286/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.1323 - val_loss: 3.2309\nEpoch 287/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.1218 - val_loss: 3.2161\nEpoch 288/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.1116 - val_loss: 3.2014\nEpoch 289/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.1014 - val_loss: 3.1869\nEpoch 290/2000\n6/6 [==============================] - 0s 4ms/step - loss: 1.0913 - val_loss: 3.1723\nEpoch 291/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0815 - val_loss: 3.1577\nEpoch 292/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0712 - val_loss: 3.1432\nEpoch 293/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0614 - val_loss: 3.1287\nEpoch 294/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0515 - val_loss: 3.1145\nEpoch 295/2000\n6/6 [==============================] - 0s 2ms/step - loss: 1.0418 - val_loss: 3.1001\nEpoch 296/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0321 - val_loss: 3.0862\nEpoch 297/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0224 - val_loss: 3.0722\nEpoch 298/2000\n6/6 [==============================] - 0s 4ms/step - loss: 1.0130 - val_loss: 3.0578\nEpoch 299/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0034 - val_loss: 3.0437\nEpoch 300/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9940 - val_loss: 3.0300\nEpoch 301/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.9848 - val_loss: 3.0160\nEpoch 302/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.9756 - val_loss: 3.0022\nEpoch 303/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9666 - val_loss: 2.9887\nEpoch 304/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9575 - val_loss: 2.9754\nEpoch 305/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.9486 - val_loss: 2.9620\nEpoch 306/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9398 - val_loss: 2.9483\nEpoch 307/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.9309 - val_loss: 2.9350\nEpoch 308/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9222 - val_loss: 2.9220\nEpoch 309/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9136 - val_loss: 2.9090\nEpoch 310/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9052 - val_loss: 2.8960\nEpoch 311/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8967 - val_loss: 2.8830\nEpoch 312/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8884 - val_loss: 2.8703\nEpoch 313/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8800 - val_loss: 2.8577\nEpoch 314/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8718 - val_loss: 2.8449\nEpoch 315/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8636 - val_loss: 2.8324\nEpoch 316/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8555 - val_loss: 2.8196\nEpoch 317/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8475 - val_loss: 2.8069\nEpoch 318/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8396 - val_loss: 2.7941\nEpoch 319/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.8316 - val_loss: 2.7815\nEpoch 320/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8237 - val_loss: 2.7691\nEpoch 321/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8159 - val_loss: 2.7563\nEpoch 322/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8081 - val_loss: 2.7440\nEpoch 323/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8005 - val_loss: 2.7317\nEpoch 324/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7930 - val_loss: 2.7191\nEpoch 325/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7853 - val_loss: 2.7069\nEpoch 326/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7778 - val_loss: 2.6946\nEpoch 327/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7704 - val_loss: 2.6826\nEpoch 328/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.7630 - val_loss: 2.6707\nEpoch 329/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7557 - val_loss: 2.6586\nEpoch 330/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7484 - val_loss: 2.6466\nEpoch 331/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7412 - val_loss: 2.6347\nEpoch 332/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7342 - val_loss: 2.6225\nEpoch 333/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7270 - val_loss: 2.6108\nEpoch 334/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7201 - val_loss: 2.5992\nEpoch 335/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7131 - val_loss: 2.5877\nEpoch 336/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7063 - val_loss: 2.5761\nEpoch 337/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6995 - val_loss: 2.5646\nEpoch 338/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 2.5533\nEpoch 339/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6861 - val_loss: 2.5420\nEpoch 340/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6795 - val_loss: 2.5309\nEpoch 341/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6730 - val_loss: 2.5197\nEpoch 342/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6665 - val_loss: 2.5087\nEpoch 343/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6600 - val_loss: 2.4978\nEpoch 344/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6537 - val_loss: 2.4868\nEpoch 345/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6473 - val_loss: 2.4760\nEpoch 346/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.6411 - val_loss: 2.4649\nEpoch 347/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6348 - val_loss: 2.4543\nEpoch 348/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6285 - val_loss: 2.4438\nEpoch 349/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6225 - val_loss: 2.4331\nEpoch 350/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6164 - val_loss: 2.4223\nEpoch 351/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6103 - val_loss: 2.4117\nEpoch 352/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6044 - val_loss: 2.4009\nEpoch 353/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5984 - val_loss: 2.3903\nEpoch 354/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5925 - val_loss: 2.3800\nEpoch 355/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5867 - val_loss: 2.3697\nEpoch 356/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5812 - val_loss: 2.3593\nEpoch 357/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5754 - val_loss: 2.3490\nEpoch 358/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5698 - val_loss: 2.3388\nEpoch 359/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5642 - val_loss: 2.3287\nEpoch 360/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5587 - val_loss: 2.3189\nEpoch 361/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5533 - val_loss: 2.3090\nEpoch 362/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.5478 - val_loss: 2.2990\nEpoch 363/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5424 - val_loss: 2.2892\nEpoch 364/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5372 - val_loss: 2.2795\nEpoch 365/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5320 - val_loss: 2.2696\nEpoch 366/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5268 - val_loss: 2.2600\nEpoch 367/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5215 - val_loss: 2.2505\nEpoch 368/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5165 - val_loss: 2.2408\nEpoch 369/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5113 - val_loss: 2.2315\nEpoch 370/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.5064 - val_loss: 2.2219\nEpoch 371/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5013 - val_loss: 2.2122\nEpoch 372/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4964 - val_loss: 2.2026\nEpoch 373/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4914 - val_loss: 2.1931\nEpoch 374/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4866 - val_loss: 2.1835\nEpoch 375/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4818 - val_loss: 2.1744\nEpoch 376/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4772 - val_loss: 2.1650\nEpoch 377/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4723 - val_loss: 2.1559\nEpoch 378/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4677 - val_loss: 2.1469\nEpoch 379/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4631 - val_loss: 2.1378\nEpoch 380/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4585 - val_loss: 2.1288\nEpoch 381/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4540 - val_loss: 2.1199\nEpoch 382/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4496 - val_loss: 2.1110\nEpoch 383/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4452 - val_loss: 2.1021\nEpoch 384/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4408 - val_loss: 2.0934\nEpoch 385/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4365 - val_loss: 2.0848\nEpoch 386/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4322 - val_loss: 2.0760\nEpoch 387/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4278 - val_loss: 2.0677\nEpoch 388/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4237 - val_loss: 2.0591\nEpoch 389/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4196 - val_loss: 2.0505\nEpoch 390/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4154 - val_loss: 2.0419\nEpoch 391/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4113 - val_loss: 2.0335\nEpoch 392/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4072 - val_loss: 2.0253\nEpoch 393/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4031 - val_loss: 2.0169\nEpoch 394/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3992 - val_loss: 2.0086\nEpoch 395/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3952 - val_loss: 2.0003\nEpoch 396/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3913 - val_loss: 1.9921\nEpoch 397/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3875 - val_loss: 1.9839\nEpoch 398/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3837 - val_loss: 1.9756\nEpoch 399/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3798 - val_loss: 1.9674\nEpoch 400/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3760 - val_loss: 1.9594\nEpoch 401/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.3723 - val_loss: 1.9515\nEpoch 402/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3687 - val_loss: 1.9434\nEpoch 403/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3650 - val_loss: 1.9356\nEpoch 404/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3613 - val_loss: 1.9277\nEpoch 405/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3578 - val_loss: 1.9199\nEpoch 406/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3543 - val_loss: 1.9120\nEpoch 407/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.3508 - val_loss: 1.9041\nEpoch 408/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3474 - val_loss: 1.8962\nEpoch 409/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3438 - val_loss: 1.8886\nEpoch 410/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3405 - val_loss: 1.8808\nEpoch 411/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3370 - val_loss: 1.8730\nEpoch 412/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3337 - val_loss: 1.8654\nEpoch 413/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3304 - val_loss: 1.8578\nEpoch 414/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3271 - val_loss: 1.8502\nEpoch 415/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3238 - val_loss: 1.8428\nEpoch 416/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3207 - val_loss: 1.8351\nEpoch 417/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3175 - val_loss: 1.8276\nEpoch 418/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3144 - val_loss: 1.8201\nEpoch 419/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3113 - val_loss: 1.8128\nEpoch 420/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3082 - val_loss: 1.8056\nEpoch 421/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3052 - val_loss: 1.7985\nEpoch 422/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.3022 - val_loss: 1.7914\nEpoch 423/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2992 - val_loss: 1.7842\nEpoch 424/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2963 - val_loss: 1.7773\nEpoch 425/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2935 - val_loss: 1.7702\nEpoch 426/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2906 - val_loss: 1.7632\nEpoch 427/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2877 - val_loss: 1.7563\nEpoch 428/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2850 - val_loss: 1.7494\nEpoch 429/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2822 - val_loss: 1.7424\nEpoch 430/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2794 - val_loss: 1.7355\nEpoch 431/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2767 - val_loss: 1.7285\nEpoch 432/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2740 - val_loss: 1.7217\nEpoch 433/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2713 - val_loss: 1.7148\nEpoch 434/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2687 - val_loss: 1.7078\nEpoch 435/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2660 - val_loss: 1.7011\nEpoch 436/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2634 - val_loss: 1.6943\nEpoch 437/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2609 - val_loss: 1.6876\nEpoch 438/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2583 - val_loss: 1.6811\nEpoch 439/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2559 - val_loss: 1.6744\nEpoch 440/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2534 - val_loss: 1.6676\nEpoch 441/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2509 - val_loss: 1.6609\nEpoch 442/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2485 - val_loss: 1.6541\nEpoch 443/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2461 - val_loss: 1.6475\nEpoch 444/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2437 - val_loss: 1.6411\nEpoch 445/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2414 - val_loss: 1.6346\nEpoch 446/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2391 - val_loss: 1.6283\nEpoch 447/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2367 - val_loss: 1.6220\nEpoch 448/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2345 - val_loss: 1.6157\nEpoch 449/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2322 - val_loss: 1.6094\nEpoch 450/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2300 - val_loss: 1.6031\nEpoch 451/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2278 - val_loss: 1.5968\nEpoch 452/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2256 - val_loss: 1.5905\nEpoch 453/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2234 - val_loss: 1.5844\nEpoch 454/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2213 - val_loss: 1.5781\nEpoch 455/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2192 - val_loss: 1.5718\nEpoch 456/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2171 - val_loss: 1.5656\nEpoch 457/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2151 - val_loss: 1.5595\nEpoch 458/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2130 - val_loss: 1.5536\nEpoch 459/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2111 - val_loss: 1.5476\nEpoch 460/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2091 - val_loss: 1.5415\nEpoch 461/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2071 - val_loss: 1.5357\nEpoch 462/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2052 - val_loss: 1.5299\nEpoch 463/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2033 - val_loss: 1.5241\nEpoch 464/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2014 - val_loss: 1.5181\nEpoch 465/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1995 - val_loss: 1.5122\nEpoch 466/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1976 - val_loss: 1.5063\nEpoch 467/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1958 - val_loss: 1.5006\nEpoch 468/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1940 - val_loss: 1.4947\nEpoch 469/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1922 - val_loss: 1.4890\nEpoch 470/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1904 - val_loss: 1.4832\nEpoch 471/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1887 - val_loss: 1.4777\nEpoch 472/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1869 - val_loss: 1.4722\nEpoch 473/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1852 - val_loss: 1.4668\nEpoch 474/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1835 - val_loss: 1.4614\nEpoch 475/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1819 - val_loss: 1.4560\nEpoch 476/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1803 - val_loss: 1.4504\nEpoch 477/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1787 - val_loss: 1.4450\nEpoch 478/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1771 - val_loss: 1.4397\nEpoch 479/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1755 - val_loss: 1.4342\nEpoch 480/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1739 - val_loss: 1.4290\nEpoch 481/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1724 - val_loss: 1.4235\nEpoch 482/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1709 - val_loss: 1.4181\nEpoch 483/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1693 - val_loss: 1.4129\nEpoch 484/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1679 - val_loss: 1.4075\nEpoch 485/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1664 - val_loss: 1.4023\nEpoch 486/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1649 - val_loss: 1.3974\nEpoch 487/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1635 - val_loss: 1.3922\nEpoch 488/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1621 - val_loss: 1.3870\nEpoch 489/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1606 - val_loss: 1.3818\nEpoch 490/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1593 - val_loss: 1.3766\nEpoch 491/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1579 - val_loss: 1.3714\nEpoch 492/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1565 - val_loss: 1.3665\nEpoch 493/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1552 - val_loss: 1.3615\nEpoch 494/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1539 - val_loss: 1.3565\nEpoch 495/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1526 - val_loss: 1.3517\nEpoch 496/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1513 - val_loss: 1.3466\nEpoch 497/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1500 - val_loss: 1.3417\nEpoch 498/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1488 - val_loss: 1.3366\nEpoch 499/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1475 - val_loss: 1.3318\nEpoch 500/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1463 - val_loss: 1.3271\nEpoch 501/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1451 - val_loss: 1.3222\nEpoch 502/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1439 - val_loss: 1.3173\nEpoch 503/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1427 - val_loss: 1.3124\nEpoch 504/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1416 - val_loss: 1.3075\nEpoch 505/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1404 - val_loss: 1.3027\nEpoch 506/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1392 - val_loss: 1.2979\nEpoch 507/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1381 - val_loss: 1.2933\nEpoch 508/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1370 - val_loss: 1.2886\nEpoch 509/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1360 - val_loss: 1.2839\nEpoch 510/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1349 - val_loss: 1.2792\nEpoch 511/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1338 - val_loss: 1.2747\nEpoch 512/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1327 - val_loss: 1.2702\nEpoch 513/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1317 - val_loss: 1.2654\nEpoch 514/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1307 - val_loss: 1.2608\nEpoch 515/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1297 - val_loss: 1.2564\nEpoch 516/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1287 - val_loss: 1.2521\nEpoch 517/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1277 - val_loss: 1.2476\nEpoch 518/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1267 - val_loss: 1.2432\nEpoch 519/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1258 - val_loss: 1.2386\nEpoch 520/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1248 - val_loss: 1.2342\nEpoch 521/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1238 - val_loss: 1.2300\nEpoch 522/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1229 - val_loss: 1.2255\nEpoch 523/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1220 - val_loss: 1.2212\nEpoch 524/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1211 - val_loss: 1.2168\nEpoch 525/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1202 - val_loss: 1.2126\nEpoch 526/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1193 - val_loss: 1.2082\nEpoch 527/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1185 - val_loss: 1.2039\nEpoch 528/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1176 - val_loss: 1.1997\nEpoch 529/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1167 - val_loss: 1.1954\nEpoch 530/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1159 - val_loss: 1.1911\nEpoch 531/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1151 - val_loss: 1.1870\nEpoch 532/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1142 - val_loss: 1.1831\nEpoch 533/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1134 - val_loss: 1.1791\nEpoch 534/2000\n6/6 [==============================] - 0s 2ms/step - loss: 0.1127 - val_loss: 1.1750\nEpoch 535/2000\n6/6 [==============================] - 0s 2ms/step - loss: 0.1119 - val_loss: 1.1710\nEpoch 536/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1111 - val_loss: 1.1669\nEpoch 537/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1103 - val_loss: 1.1629\nEpoch 538/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1096 - val_loss: 1.1590\nEpoch 539/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1089 - val_loss: 1.1551\nEpoch 540/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1081 - val_loss: 1.1512\nEpoch 541/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1074 - val_loss: 1.1471\nEpoch 542/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1067 - val_loss: 1.1431\nEpoch 543/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1060 - val_loss: 1.1388\nEpoch 544/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1053 - val_loss: 1.1347\nEpoch 545/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1046 - val_loss: 1.1307\nEpoch 546/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1039 - val_loss: 1.1267\nEpoch 547/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1032 - val_loss: 1.1226\nEpoch 548/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1026 - val_loss: 1.1187\nEpoch 549/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1020 - val_loss: 1.1147\nEpoch 550/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1013 - val_loss: 1.1107\nEpoch 551/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1006 - val_loss: 1.1068\nEpoch 552/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1000 - val_loss: 1.1028\nEpoch 553/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0994 - val_loss: 1.0990\nEpoch 554/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0988 - val_loss: 1.0952\nEpoch 555/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0982 - val_loss: 1.0914\nEpoch 556/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0976 - val_loss: 1.0878\nEpoch 557/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0970 - val_loss: 1.0838\nEpoch 558/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0964 - val_loss: 1.0799\nEpoch 559/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0958 - val_loss: 1.0761\nEpoch 560/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0953 - val_loss: 1.0723\nEpoch 561/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0947 - val_loss: 1.0685\nEpoch 562/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0942 - val_loss: 1.0649\nEpoch 563/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0936 - val_loss: 1.0613\nEpoch 564/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0931 - val_loss: 1.0577\nEpoch 565/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0926 - val_loss: 1.0539\nEpoch 566/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0921 - val_loss: 1.0501\nEpoch 567/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0915 - val_loss: 1.0466\nEpoch 568/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0910 - val_loss: 1.0428\nEpoch 569/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0905 - val_loss: 1.0394\nEpoch 570/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0901 - val_loss: 1.0356\nEpoch 571/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0895 - val_loss: 1.0320\nEpoch 572/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0891 - val_loss: 1.0284\nEpoch 573/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0886 - val_loss: 1.0249\nEpoch 574/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0881 - val_loss: 1.0212\nEpoch 575/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0877 - val_loss: 1.0176\nEpoch 576/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0872 - val_loss: 1.0141\nEpoch 577/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0867 - val_loss: 1.0107\nEpoch 578/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0863 - val_loss: 1.0072\nEpoch 579/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0859 - val_loss: 1.0037\nEpoch 580/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0854 - val_loss: 1.0002\nEpoch 581/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0850 - val_loss: 0.9969\nEpoch 582/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0846 - val_loss: 0.9933\nEpoch 583/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0842 - val_loss: 0.9898\nEpoch 584/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0838 - val_loss: 0.9865\nEpoch 585/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0834 - val_loss: 0.9831\nEpoch 586/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0830 - val_loss: 0.9796\nEpoch 587/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0826 - val_loss: 0.9761\nEpoch 588/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0822 - val_loss: 0.9726\nEpoch 589/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0818 - val_loss: 0.9689\nEpoch 590/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0814 - val_loss: 0.9655\nEpoch 591/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0811 - val_loss: 0.9621\nEpoch 592/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0807 - val_loss: 0.9589\nEpoch 593/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0803 - val_loss: 0.9555\nEpoch 594/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0800 - val_loss: 0.9522\nEpoch 595/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0796 - val_loss: 0.9488\nEpoch 596/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0792 - val_loss: 0.9454\nEpoch 597/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0789 - val_loss: 0.9419\nEpoch 598/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0786 - val_loss: 0.9388\nEpoch 599/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0782 - val_loss: 0.9355\nEpoch 600/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0779 - val_loss: 0.9324\nEpoch 601/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0776 - val_loss: 0.9292\nEpoch 602/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0773 - val_loss: 0.9261\nEpoch 603/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0770 - val_loss: 0.9230\nEpoch 604/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0767 - val_loss: 0.9196\nEpoch 605/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0763 - val_loss: 0.9165\nEpoch 606/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0760 - val_loss: 0.9135\nEpoch 607/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0758 - val_loss: 0.9102\nEpoch 608/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0755 - val_loss: 0.9070\nEpoch 609/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0752 - val_loss: 0.9039\nEpoch 610/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0749 - val_loss: 0.9008\nEpoch 611/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0746 - val_loss: 0.8976\nEpoch 612/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0743 - val_loss: 0.8945\nEpoch 613/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0740 - val_loss: 0.8914\nEpoch 614/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0737 - val_loss: 0.8883\nEpoch 615/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0735 - val_loss: 0.8852\nEpoch 616/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0732 - val_loss: 0.8820\nEpoch 617/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0729 - val_loss: 0.8789\nEpoch 618/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0727 - val_loss: 0.8759\nEpoch 619/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0724 - val_loss: 0.8729\nEpoch 620/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0722 - val_loss: 0.8700\nEpoch 621/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0719 - val_loss: 0.8669\nEpoch 622/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0717 - val_loss: 0.8641\nEpoch 623/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0714 - val_loss: 0.8610\nEpoch 624/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0712 - val_loss: 0.8578\nEpoch 625/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0709 - val_loss: 0.8547\nEpoch 626/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0707 - val_loss: 0.8518\nEpoch 627/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0705 - val_loss: 0.8489\nEpoch 628/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0702 - val_loss: 0.8459\nEpoch 629/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0700 - val_loss: 0.8430\nEpoch 630/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0698 - val_loss: 0.8400\nEpoch 631/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0696 - val_loss: 0.8370\nEpoch 632/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0693 - val_loss: 0.8339\nEpoch 633/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0691 - val_loss: 0.8309\nEpoch 634/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0689 - val_loss: 0.8279\nEpoch 635/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0687 - val_loss: 0.8249\nEpoch 636/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0685 - val_loss: 0.8221\nEpoch 637/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0683 - val_loss: 0.8193\nEpoch 638/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0681 - val_loss: 0.8163\nEpoch 639/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0679 - val_loss: 0.8137\nEpoch 640/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0677 - val_loss: 0.8107\nEpoch 641/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0675 - val_loss: 0.8078\nEpoch 642/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0673 - val_loss: 0.8048\nEpoch 643/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0671 - val_loss: 0.8015\nEpoch 644/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0669 - val_loss: 0.7986\nEpoch 645/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0667 - val_loss: 0.7956\nEpoch 646/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0665 - val_loss: 0.7927\nEpoch 647/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0663 - val_loss: 0.7898\nEpoch 648/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0662 - val_loss: 0.7870\nEpoch 649/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0660 - val_loss: 0.7843\nEpoch 650/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0658 - val_loss: 0.7816\nEpoch 651/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0656 - val_loss: 0.7787\nEpoch 652/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0654 - val_loss: 0.7759\nEpoch 653/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0653 - val_loss: 0.7731\nEpoch 654/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0651 - val_loss: 0.7703\nEpoch 655/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0649 - val_loss: 0.7676\nEpoch 656/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0647 - val_loss: 0.7649\nEpoch 657/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0646 - val_loss: 0.7621\nEpoch 658/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0644 - val_loss: 0.7593\nEpoch 659/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0642 - val_loss: 0.7565\nEpoch 660/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0641 - val_loss: 0.7537\nEpoch 661/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0639 - val_loss: 0.7511\nEpoch 662/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0638 - val_loss: 0.7482\nEpoch 663/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0636 - val_loss: 0.7454\nEpoch 664/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0635 - val_loss: 0.7426\nEpoch 665/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0633 - val_loss: 0.7400\nEpoch 666/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0631 - val_loss: 0.7373\nEpoch 667/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0630 - val_loss: 0.7345\nEpoch 668/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0628 - val_loss: 0.7320\nEpoch 669/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0627 - val_loss: 0.7292\nEpoch 670/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0625 - val_loss: 0.7266\nEpoch 671/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0624 - val_loss: 0.7240\nEpoch 672/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0623 - val_loss: 0.7214\nEpoch 673/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0621 - val_loss: 0.7188\nEpoch 674/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0620 - val_loss: 0.7162\nEpoch 675/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0618 - val_loss: 0.7136\nEpoch 676/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0617 - val_loss: 0.7108\nEpoch 677/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0615 - val_loss: 0.7081\nEpoch 678/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0614 - val_loss: 0.7053\nEpoch 679/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0613 - val_loss: 0.7027\nEpoch 680/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0611 - val_loss: 0.7002\nEpoch 681/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0610 - val_loss: 0.6976\nEpoch 682/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0609 - val_loss: 0.6952\nEpoch 683/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0607 - val_loss: 0.6927\nEpoch 684/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0606 - val_loss: 0.6903\nEpoch 685/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0605 - val_loss: 0.6877\nEpoch 686/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0603 - val_loss: 0.6851\nEpoch 687/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0602 - val_loss: 0.6827\nEpoch 688/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0601 - val_loss: 0.6801\nEpoch 689/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0600 - val_loss: 0.6775\nEpoch 690/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0598 - val_loss: 0.6750\nEpoch 691/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0597 - val_loss: 0.6725\nEpoch 692/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0596 - val_loss: 0.6700\nEpoch 693/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0595 - val_loss: 0.6674\nEpoch 694/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0593 - val_loss: 0.6648\nEpoch 695/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0592 - val_loss: 0.6624\nEpoch 696/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0591 - val_loss: 0.6600\nEpoch 697/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0590 - val_loss: 0.6576\nEpoch 698/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0589 - val_loss: 0.6551\nEpoch 699/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0588 - val_loss: 0.6525\nEpoch 700/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0586 - val_loss: 0.6500\nEpoch 701/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0585 - val_loss: 0.6474\nEpoch 702/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0584 - val_loss: 0.6449\nEpoch 703/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0583 - val_loss: 0.6424\nEpoch 704/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0582 - val_loss: 0.6398\nEpoch 705/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0581 - val_loss: 0.6376\nEpoch 706/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0580 - val_loss: 0.6352\nEpoch 707/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0578 - val_loss: 0.6328\nEpoch 708/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0577 - val_loss: 0.6303\nEpoch 709/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0576 - val_loss: 0.6278\nEpoch 710/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0575 - val_loss: 0.6253\nEpoch 711/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0574 - val_loss: 0.6231\nEpoch 712/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0573 - val_loss: 0.6207\nEpoch 713/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0572 - val_loss: 0.6183\nEpoch 714/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0571 - val_loss: 0.6157\nEpoch 715/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0570 - val_loss: 0.6132\nEpoch 716/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0569 - val_loss: 0.6107\nEpoch 717/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0568 - val_loss: 0.6083\nEpoch 718/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0566 - val_loss: 0.6059\nEpoch 719/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0566 - val_loss: 0.6035\nEpoch 720/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0564 - val_loss: 0.6012\nEpoch 721/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0563 - val_loss: 0.5989\nEpoch 722/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0562 - val_loss: 0.5964\nEpoch 723/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0561 - val_loss: 0.5941\nEpoch 724/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0560 - val_loss: 0.5915\nEpoch 725/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0559 - val_loss: 0.5893\nEpoch 726/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0558 - val_loss: 0.5869\nEpoch 727/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0557 - val_loss: 0.5844\nEpoch 728/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0556 - val_loss: 0.5821\nEpoch 729/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0555 - val_loss: 0.5799\nEpoch 730/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0554 - val_loss: 0.5777\nEpoch 731/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0553 - val_loss: 0.5753\nEpoch 732/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0552 - val_loss: 0.5728\nEpoch 733/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0551 - val_loss: 0.5701\nEpoch 734/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0550 - val_loss: 0.5676\nEpoch 735/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0549 - val_loss: 0.5653\nEpoch 736/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0548 - val_loss: 0.5630\nEpoch 737/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0547 - val_loss: 0.5606\nEpoch 738/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0546 - val_loss: 0.5584\nEpoch 739/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0545 - val_loss: 0.5560\nEpoch 740/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0544 - val_loss: 0.5537\nEpoch 741/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0543 - val_loss: 0.5514\nEpoch 742/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0542 - val_loss: 0.5491\nEpoch 743/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0541 - val_loss: 0.5468\nEpoch 744/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0540 - val_loss: 0.5446\nEpoch 745/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0540 - val_loss: 0.5421\nEpoch 746/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0539 - val_loss: 0.5399\nEpoch 747/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0538 - val_loss: 0.5374\nEpoch 748/2000\n6/6 [==============================] - 0s 6ms/step - loss: 0.0537 - val_loss: 0.5350\nEpoch 749/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0536 - val_loss: 0.5327\nEpoch 750/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0535 - val_loss: 0.5305\nEpoch 751/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0534 - val_loss: 0.5284\nEpoch 752/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0533 - val_loss: 0.5262\nEpoch 753/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0532 - val_loss: 0.5242\nEpoch 754/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0531 - val_loss: 0.5220\nEpoch 755/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0530 - val_loss: 0.5198\nEpoch 756/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0529 - val_loss: 0.5177\nEpoch 757/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0529 - val_loss: 0.5156\nEpoch 758/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0528 - val_loss: 0.5135\nEpoch 759/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0527 - val_loss: 0.5112\nEpoch 760/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0526 - val_loss: 0.5089\nEpoch 761/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0525 - val_loss: 0.5067\nEpoch 762/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0524 - val_loss: 0.5046\nEpoch 763/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0523 - val_loss: 0.5024\nEpoch 764/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0522 - val_loss: 0.5002\nEpoch 765/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0521 - val_loss: 0.4980\nEpoch 766/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0520 - val_loss: 0.4958\nEpoch 767/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0520 - val_loss: 0.4936\nEpoch 768/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0519 - val_loss: 0.4915\nEpoch 769/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0518 - val_loss: 0.4895\nEpoch 770/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0517 - val_loss: 0.4875\nEpoch 771/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0516 - val_loss: 0.4855\nEpoch 772/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0515 - val_loss: 0.4833\nEpoch 773/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0514 - val_loss: 0.4813\nEpoch 774/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0514 - val_loss: 0.4791\nEpoch 775/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0513 - val_loss: 0.4772\nEpoch 776/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0512 - val_loss: 0.4752\nEpoch 777/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0511 - val_loss: 0.4731\nEpoch 778/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0510 - val_loss: 0.4709\nEpoch 779/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0509 - val_loss: 0.4689\nEpoch 780/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0509 - val_loss: 0.4670\nEpoch 781/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0508 - val_loss: 0.4652\nEpoch 782/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0507 - val_loss: 0.4631\nEpoch 783/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0506 - val_loss: 0.4610\nEpoch 784/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0505 - val_loss: 0.4586\nEpoch 785/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0504 - val_loss: 0.4565\nEpoch 786/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0504 - val_loss: 0.4545\nEpoch 787/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0503 - val_loss: 0.4524\nEpoch 788/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0502 - val_loss: 0.4504\nEpoch 789/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0501 - val_loss: 0.4484\nEpoch 790/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0500 - val_loss: 0.4464\nEpoch 791/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0500 - val_loss: 0.4442\nEpoch 792/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0499 - val_loss: 0.4421\nEpoch 793/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0498 - val_loss: 0.4401\nEpoch 794/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0497 - val_loss: 0.4380\nEpoch 795/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0496 - val_loss: 0.4359\nEpoch 796/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0495 - val_loss: 0.4339\nEpoch 797/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0495 - val_loss: 0.4318\nEpoch 798/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0494 - val_loss: 0.4299\nEpoch 799/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0493 - val_loss: 0.4280\nEpoch 800/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0492 - val_loss: 0.4262\nEpoch 801/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0491 - val_loss: 0.4242\nEpoch 802/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0491 - val_loss: 0.4221\nEpoch 803/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0490 - val_loss: 0.4202\nEpoch 804/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0489 - val_loss: 0.4183\nEpoch 805/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0488 - val_loss: 0.4164\nEpoch 806/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0488 - val_loss: 0.4144\nEpoch 807/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0487 - val_loss: 0.4125\nEpoch 808/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0486 - val_loss: 0.4106\nEpoch 809/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0485 - val_loss: 0.4085\nEpoch 810/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0484 - val_loss: 0.4066\nEpoch 811/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0484 - val_loss: 0.4047\nEpoch 812/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0483 - val_loss: 0.4027\nEpoch 813/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0482 - val_loss: 0.4008\nEpoch 814/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0481 - val_loss: 0.3989\nEpoch 815/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0481 - val_loss: 0.3970\nEpoch 816/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0480 - val_loss: 0.3949\nEpoch 817/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0479 - val_loss: 0.3930\nEpoch 818/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0478 - val_loss: 0.3912\nEpoch 819/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0478 - val_loss: 0.3894\nEpoch 820/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0477 - val_loss: 0.3874\nEpoch 821/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0476 - val_loss: 0.3857\nEpoch 822/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0475 - val_loss: 0.3841\nEpoch 823/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0475 - val_loss: 0.3824\nEpoch 824/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0474 - val_loss: 0.3807\nEpoch 825/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0473 - val_loss: 0.3787\nEpoch 826/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0472 - val_loss: 0.3770\nEpoch 827/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0472 - val_loss: 0.3750\nEpoch 828/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0471 - val_loss: 0.3731\nEpoch 829/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0470 - val_loss: 0.3715\nEpoch 830/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0469 - val_loss: 0.3696\nEpoch 831/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0469 - val_loss: 0.3677\nEpoch 832/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0468 - val_loss: 0.3660\nEpoch 833/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0467 - val_loss: 0.3641\nEpoch 834/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0466 - val_loss: 0.3624\nEpoch 835/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0466 - val_loss: 0.3604\nEpoch 836/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0465 - val_loss: 0.3585\nEpoch 837/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0464 - val_loss: 0.3564\nEpoch 838/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0464 - val_loss: 0.3546\nEpoch 839/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0463 - val_loss: 0.3528\nEpoch 840/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0462 - val_loss: 0.3510\nEpoch 841/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0461 - val_loss: 0.3492\nEpoch 842/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0461 - val_loss: 0.3473\nEpoch 843/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0460 - val_loss: 0.3457\nEpoch 844/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0459 - val_loss: 0.3439\nEpoch 845/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0459 - val_loss: 0.3422\nEpoch 846/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0458 - val_loss: 0.3405\nEpoch 847/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0457 - val_loss: 0.3388\nEpoch 848/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0457 - val_loss: 0.3372\nEpoch 849/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0456 - val_loss: 0.3356\nEpoch 850/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0455 - val_loss: 0.3339\nEpoch 851/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0455 - val_loss: 0.3322\nEpoch 852/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0454 - val_loss: 0.3303\nEpoch 853/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0453 - val_loss: 0.3286\nEpoch 854/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0453 - val_loss: 0.3270\nEpoch 855/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0452 - val_loss: 0.3253\nEpoch 856/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0451 - val_loss: 0.3237\nEpoch 857/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0451 - val_loss: 0.3220\nEpoch 858/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0450 - val_loss: 0.3204\nEpoch 859/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0449 - val_loss: 0.3187\nEpoch 860/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0449 - val_loss: 0.3169\nEpoch 861/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0448 - val_loss: 0.3150\nEpoch 862/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0447 - val_loss: 0.3133\nEpoch 863/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0447 - val_loss: 0.3115\nEpoch 864/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0446 - val_loss: 0.3098\nEpoch 865/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0445 - val_loss: 0.3080\nEpoch 866/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0445 - val_loss: 0.3063\nEpoch 867/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0444 - val_loss: 0.3048\nEpoch 868/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0443 - val_loss: 0.3032\nEpoch 869/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0443 - val_loss: 0.3017\nEpoch 870/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0442 - val_loss: 0.3002\nEpoch 871/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0441 - val_loss: 0.2985\nEpoch 872/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0441 - val_loss: 0.2970\nEpoch 873/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0440 - val_loss: 0.2955\nEpoch 874/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0440 - val_loss: 0.2940\nEpoch 875/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0439 - val_loss: 0.2925\nEpoch 876/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0438 - val_loss: 0.2908\nEpoch 877/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0438 - val_loss: 0.2892\nEpoch 878/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0437 - val_loss: 0.2877\nEpoch 879/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0436 - val_loss: 0.2861\nEpoch 880/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0436 - val_loss: 0.2846\nEpoch 881/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0435 - val_loss: 0.2829\nEpoch 882/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0435 - val_loss: 0.2812\nEpoch 883/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0434 - val_loss: 0.2797\nEpoch 884/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0433 - val_loss: 0.2782\nEpoch 885/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0433 - val_loss: 0.2769\nEpoch 886/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0432 - val_loss: 0.2753\nEpoch 887/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0432 - val_loss: 0.2739\nEpoch 888/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0431 - val_loss: 0.2724\nEpoch 889/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0430 - val_loss: 0.2710\nEpoch 890/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0430 - val_loss: 0.2694\nEpoch 891/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0429 - val_loss: 0.2679\nEpoch 892/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0429 - val_loss: 0.2663\nEpoch 893/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0428 - val_loss: 0.2647\nEpoch 894/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0427 - val_loss: 0.2633\nEpoch 895/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0427 - val_loss: 0.2617\nEpoch 896/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0426 - val_loss: 0.2602\nEpoch 897/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0426 - val_loss: 0.2587\nEpoch 898/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0425 - val_loss: 0.2571\nEpoch 899/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0424 - val_loss: 0.2558\nEpoch 900/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0424 - val_loss: 0.2544\nEpoch 901/2000\n6/6 [==============================] - 0s 5ms/step - loss: 0.0423 - val_loss: 0.2529\nEpoch 902/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0423 - val_loss: 0.2514\nEpoch 903/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0422 - val_loss: 0.2499\nEpoch 904/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0422 - val_loss: 0.2484\nEpoch 905/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0421 - val_loss: 0.2470\nEpoch 906/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0420 - val_loss: 0.2456\nEpoch 907/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0420 - val_loss: 0.2441\nEpoch 908/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0419 - val_loss: 0.2425\nEpoch 909/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0419 - val_loss: 0.2410\nEpoch 910/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0418 - val_loss: 0.2395\nEpoch 911/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0418 - val_loss: 0.2381\nEpoch 912/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0417 - val_loss: 0.2366\nEpoch 913/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0416 - val_loss: 0.2355\nEpoch 914/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0416 - val_loss: 0.2341\nEpoch 915/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0415 - val_loss: 0.2328\nEpoch 916/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0415 - val_loss: 0.2314\nEpoch 917/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0414 - val_loss: 0.2301\nEpoch 918/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0414 - val_loss: 0.2288\nEpoch 919/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0413 - val_loss: 0.2274\nEpoch 920/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0413 - val_loss: 0.2262\nEpoch 921/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0412 - val_loss: 0.2251\nEpoch 922/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0412 - val_loss: 0.2237\nEpoch 923/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0411 - val_loss: 0.2223\nEpoch 924/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0411 - val_loss: 0.2210\nEpoch 925/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0410 - val_loss: 0.2197\nEpoch 926/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0410 - val_loss: 0.2184\nEpoch 927/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0409 - val_loss: 0.2171\nEpoch 928/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0409 - val_loss: 0.2157\nEpoch 929/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0408 - val_loss: 0.2144\nEpoch 930/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0408 - val_loss: 0.2131\nEpoch 931/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0407 - val_loss: 0.2119\nEpoch 932/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0407 - val_loss: 0.2106\nEpoch 933/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0406 - val_loss: 0.2094\nEpoch 934/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0406 - val_loss: 0.2080\nEpoch 935/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0405 - val_loss: 0.2067\nEpoch 936/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0405 - val_loss: 0.2056\nEpoch 937/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0404 - val_loss: 0.2044\nEpoch 938/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0404 - val_loss: 0.2032\nEpoch 939/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0403 - val_loss: 0.2021\nEpoch 940/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0403 - val_loss: 0.2008\nEpoch 941/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0402 - val_loss: 0.1997\nEpoch 942/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0402 - val_loss: 0.1985\nEpoch 943/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0401 - val_loss: 0.1976\nEpoch 944/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0401 - val_loss: 0.1964\nEpoch 945/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0400 - val_loss: 0.1952\nEpoch 946/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0400 - val_loss: 0.1941\nEpoch 947/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0399 - val_loss: 0.1928\nEpoch 948/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0399 - val_loss: 0.1916\nEpoch 949/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0399 - val_loss: 0.1903\nEpoch 950/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0398 - val_loss: 0.1892\nEpoch 951/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0398 - val_loss: 0.1882\nEpoch 952/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0397 - val_loss: 0.1870\nEpoch 953/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0397 - val_loss: 0.1859\nEpoch 954/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0396 - val_loss: 0.1847\nEpoch 955/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0396 - val_loss: 0.1835\nEpoch 956/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0395 - val_loss: 0.1824\nEpoch 957/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0395 - val_loss: 0.1814\nEpoch 958/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0394 - val_loss: 0.1803\nEpoch 959/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0394 - val_loss: 0.1794\nEpoch 960/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0394 - val_loss: 0.1783\nEpoch 961/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0393 - val_loss: 0.1771\nEpoch 962/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0393 - val_loss: 0.1760\nEpoch 963/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0392 - val_loss: 0.1749\nEpoch 964/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0392 - val_loss: 0.1738\nEpoch 965/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0392 - val_loss: 0.1726\nEpoch 966/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0391 - val_loss: 0.1716\nEpoch 967/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0391 - val_loss: 0.1705\nEpoch 968/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0390 - val_loss: 0.1694\nEpoch 969/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0390 - val_loss: 0.1685\nEpoch 970/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0389 - val_loss: 0.1674\nEpoch 971/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0389 - val_loss: 0.1662\nEpoch 972/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0389 - val_loss: 0.1650\nEpoch 973/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0388 - val_loss: 0.1639\nEpoch 974/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0388 - val_loss: 0.1628\nEpoch 975/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0387 - val_loss: 0.1618\nEpoch 976/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0387 - val_loss: 0.1609\nEpoch 977/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0387 - val_loss: 0.1600\nEpoch 978/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0386 - val_loss: 0.1590\nEpoch 979/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0386 - val_loss: 0.1579\nEpoch 980/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0385 - val_loss: 0.1569\nEpoch 981/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0385 - val_loss: 0.1560\nEpoch 982/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0385 - val_loss: 0.1552\nEpoch 983/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0384 - val_loss: 0.1543\nEpoch 984/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0384 - val_loss: 0.1533\nEpoch 985/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0384 - val_loss: 0.1523\nEpoch 986/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0383 - val_loss: 0.1514\nEpoch 987/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0383 - val_loss: 0.1503\nEpoch 988/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0382 - val_loss: 0.1493\nEpoch 989/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0382 - val_loss: 0.1484\nEpoch 990/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0382 - val_loss: 0.1475\nEpoch 991/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0381 - val_loss: 0.1465\nEpoch 992/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0381 - val_loss: 0.1457\nEpoch 993/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0381 - val_loss: 0.1449\nEpoch 994/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0380 - val_loss: 0.1439\nEpoch 995/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0380 - val_loss: 0.1430\nEpoch 996/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0380 - val_loss: 0.1420\nEpoch 997/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0379 - val_loss: 0.1411\nEpoch 998/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0379 - val_loss: 0.1401\nEpoch 999/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0379 - val_loss: 0.1392\nEpoch 1000/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0378 - val_loss: 0.1382\nEpoch 1001/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0378 - val_loss: 0.1374\nEpoch 1002/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0378 - val_loss: 0.1364\nEpoch 1003/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0377 - val_loss: 0.1357\nEpoch 1004/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0377 - val_loss: 0.1349\nEpoch 1005/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0377 - val_loss: 0.1341\nEpoch 1006/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0376 - val_loss: 0.1333\nEpoch 1007/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0376 - val_loss: 0.1325\nEpoch 1008/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0376 - val_loss: 0.1318\nEpoch 1009/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0375 - val_loss: 0.1311\nEpoch 1010/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0375 - val_loss: 0.1301\nEpoch 1011/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0375 - val_loss: 0.1293\nEpoch 1012/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0374 - val_loss: 0.1285\nEpoch 1013/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0374 - val_loss: 0.1277\nEpoch 1014/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0374 - val_loss: 0.1269\nEpoch 1015/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0373 - val_loss: 0.1262\nEpoch 1016/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0373 - val_loss: 0.1254\nEpoch 1017/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0373 - val_loss: 0.1246\nEpoch 1018/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0373 - val_loss: 0.1239\nEpoch 1019/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0372 - val_loss: 0.1231\nEpoch 1020/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0372 - val_loss: 0.1223\nEpoch 1021/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0372 - val_loss: 0.1216\nEpoch 1022/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0371 - val_loss: 0.1209\nEpoch 1023/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0371 - val_loss: 0.1202\nEpoch 1024/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0371 - val_loss: 0.1194\nEpoch 1025/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0371 - val_loss: 0.1187\nEpoch 1026/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0370 - val_loss: 0.1179\nEpoch 1027/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0370 - val_loss: 0.1170\nEpoch 1028/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0370 - val_loss: 0.1162\nEpoch 1029/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0369 - val_loss: 0.1155\nEpoch 1030/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0369 - val_loss: 0.1148\nEpoch 1031/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0369 - val_loss: 0.1141\nEpoch 1032/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0369 - val_loss: 0.1135\nEpoch 1033/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0368 - val_loss: 0.1128\nEpoch 1034/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0368 - val_loss: 0.1120\nEpoch 1035/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0368 - val_loss: 0.1114\nEpoch 1036/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0367 - val_loss: 0.1107\nEpoch 1037/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0367 - val_loss: 0.1102\nEpoch 1038/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0367 - val_loss: 0.1096\nEpoch 1039/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0367 - val_loss: 0.1090\nEpoch 1040/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0366 - val_loss: 0.1084\nEpoch 1041/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0366 - val_loss: 0.1077\nEpoch 1042/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0366 - val_loss: 0.1071\nEpoch 1043/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0366 - val_loss: 0.1065\nEpoch 1044/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0365 - val_loss: 0.1059\nEpoch 1045/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0365 - val_loss: 0.1052\nEpoch 1046/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0365 - val_loss: 0.1045\nEpoch 1047/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0365 - val_loss: 0.1039\nEpoch 1048/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0364 - val_loss: 0.1031\nEpoch 1049/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0364 - val_loss: 0.1025\nEpoch 1050/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0364 - val_loss: 0.1018\nEpoch 1051/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0364 - val_loss: 0.1013\nEpoch 1052/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0364 - val_loss: 0.1006\nEpoch 1053/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0363 - val_loss: 0.0999\nEpoch 1054/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0363 - val_loss: 0.0992\nEpoch 1055/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0363 - val_loss: 0.0985\nEpoch 1056/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0363 - val_loss: 0.0980\nEpoch 1057/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0362 - val_loss: 0.0974\nEpoch 1058/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0362 - val_loss: 0.0968\nEpoch 1059/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0362 - val_loss: 0.0962\nEpoch 1060/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0362 - val_loss: 0.0957\nEpoch 1061/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0362 - val_loss: 0.0950\nEpoch 1062/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0361 - val_loss: 0.0945\nEpoch 1063/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0361 - val_loss: 0.0939\nEpoch 1064/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0361 - val_loss: 0.0933\nEpoch 1065/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0361 - val_loss: 0.0926\nEpoch 1066/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0360 - val_loss: 0.0921\nEpoch 1067/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0360 - val_loss: 0.0916\nEpoch 1068/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0360 - val_loss: 0.0911\nEpoch 1069/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0360 - val_loss: 0.0905\nEpoch 1070/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0360 - val_loss: 0.0900\nEpoch 1071/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0359 - val_loss: 0.0895\nEpoch 1072/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0359 - val_loss: 0.0890\nEpoch 1073/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0359 - val_loss: 0.0885\nEpoch 1074/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0359 - val_loss: 0.0880\nEpoch 1075/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0359 - val_loss: 0.0876\nEpoch 1076/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0358 - val_loss: 0.0871\nEpoch 1077/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0358 - val_loss: 0.0865\nEpoch 1078/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0358 - val_loss: 0.0860\nEpoch 1079/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0358 - val_loss: 0.0857\nEpoch 1080/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0358 - val_loss: 0.0852\nEpoch 1081/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0358 - val_loss: 0.0846\nEpoch 1082/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0357 - val_loss: 0.0842\nEpoch 1083/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0357 - val_loss: 0.0836\nEpoch 1084/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0357 - val_loss: 0.0830\nEpoch 1085/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0357 - val_loss: 0.0825\nEpoch 1086/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0357 - val_loss: 0.0821\nEpoch 1087/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0356 - val_loss: 0.0816\nEpoch 1088/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0356 - val_loss: 0.0811\nEpoch 1089/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0356 - val_loss: 0.0806\nEpoch 1090/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0356 - val_loss: 0.0801\nEpoch 1091/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0356 - val_loss: 0.0796\nEpoch 1092/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0356 - val_loss: 0.0792\nEpoch 1093/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0355 - val_loss: 0.0787\nEpoch 1094/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0355 - val_loss: 0.0782\nEpoch 1095/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0355 - val_loss: 0.0778\nEpoch 1096/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0355 - val_loss: 0.0774\nEpoch 1097/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0355 - val_loss: 0.0770\nEpoch 1098/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0355 - val_loss: 0.0766\nEpoch 1099/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0354 - val_loss: 0.0762\nEpoch 1100/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0354 - val_loss: 0.0759\nEpoch 1101/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0354 - val_loss: 0.0755\nEpoch 1102/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0354 - val_loss: 0.0751\nEpoch 1103/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0354 - val_loss: 0.0747\nEpoch 1104/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0354 - val_loss: 0.0743\nEpoch 1105/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0354 - val_loss: 0.0739\nEpoch 1106/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0353 - val_loss: 0.0735\nEpoch 1107/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0353 - val_loss: 0.0731\nEpoch 1108/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0353 - val_loss: 0.0727\nEpoch 1109/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0353 - val_loss: 0.0724\nEpoch 1110/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0353 - val_loss: 0.0721\nEpoch 1111/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0353 - val_loss: 0.0717\nEpoch 1112/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0353 - val_loss: 0.0714\nEpoch 1113/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0710\nEpoch 1114/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0706\nEpoch 1115/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0702\nEpoch 1116/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0698\nEpoch 1117/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0352 - val_loss: 0.0694\nEpoch 1118/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0690\nEpoch 1119/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0686\nEpoch 1120/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0683\nEpoch 1121/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0679\nEpoch 1122/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0675\nEpoch 1123/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0673\nEpoch 1124/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0669\nEpoch 1125/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0664\nEpoch 1126/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0660\nEpoch 1127/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0656\nEpoch 1128/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0652\nEpoch 1129/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0649\nEpoch 1130/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0646\nEpoch 1131/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0643\nEpoch 1132/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0640\nEpoch 1133/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0637\nEpoch 1134/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0634\nEpoch 1135/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0631\nEpoch 1136/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0628\nEpoch 1137/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0626\nEpoch 1138/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0623\nEpoch 1139/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0620\nEpoch 1140/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0617\nEpoch 1141/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0615\nEpoch 1142/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0613\nEpoch 1143/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0610\nEpoch 1144/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0607\nEpoch 1145/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0603\nEpoch 1146/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0601\nEpoch 1147/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0598\nEpoch 1148/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0596\nEpoch 1149/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0594\nEpoch 1150/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0591\nEpoch 1151/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0588\nEpoch 1152/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0585\nEpoch 1153/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0582\nEpoch 1154/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0581\nEpoch 1155/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0579\nEpoch 1156/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0576\nEpoch 1157/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0574\nEpoch 1158/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0572\nEpoch 1159/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0570\nEpoch 1160/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0568\nEpoch 1161/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0566\nEpoch 1162/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0564\nEpoch 1163/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0563\nEpoch 1164/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0560\nEpoch 1165/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0558\nEpoch 1166/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0556\nEpoch 1167/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0553\nEpoch 1168/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0551\nEpoch 1169/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0549\nEpoch 1170/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0548\nEpoch 1171/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0546\nEpoch 1172/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0544\nEpoch 1173/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0542\nEpoch 1174/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0540\nEpoch 1175/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0538\nEpoch 1176/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0536\nEpoch 1177/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0346 - val_loss: 0.0534\nEpoch 1178/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0533\nEpoch 1179/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0346 - val_loss: 0.0531\nEpoch 1180/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0346 - val_loss: 0.0530\nEpoch 1181/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0529\nEpoch 1182/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0527\nEpoch 1183/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0525\nEpoch 1184/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0524\nEpoch 1185/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0522\nEpoch 1186/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0520\nEpoch 1187/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0519\nEpoch 1188/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0517\nEpoch 1189/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0346 - val_loss: 0.0516\nEpoch 1190/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0514\nEpoch 1191/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0512\nEpoch 1192/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0510\nEpoch 1193/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0508\nEpoch 1194/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0508\nEpoch 1195/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0506\nEpoch 1196/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0504\nEpoch 1197/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0502\nEpoch 1198/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0501\nEpoch 1199/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0499\nEpoch 1200/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0497\nEpoch 1201/2000\n6/6 [==============================] - 0s 5ms/step - loss: 0.0345 - val_loss: 0.0496\nEpoch 1202/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0494\nEpoch 1203/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0492\nEpoch 1204/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0491\nEpoch 1205/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0345 - val_loss: 0.0490\nEpoch 1206/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0489\nEpoch 1207/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0345 - val_loss: 0.0487\nEpoch 1208/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0486\nEpoch 1209/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0345 - val_loss: 0.0483\nEpoch 1210/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0482\nEpoch 1211/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0344 - val_loss: 0.0481\nEpoch 1212/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0479\nEpoch 1213/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0478\nEpoch 1214/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0477\nEpoch 1215/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0476\nEpoch 1216/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0475\nEpoch 1217/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0473\nEpoch 1218/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0344 - val_loss: 0.0472\nEpoch 1219/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0471\nEpoch 1220/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0344 - val_loss: 0.0470\nEpoch 1221/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0469\nEpoch 1222/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0468\nEpoch 1223/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0467\nEpoch 1224/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0466\nEpoch 1225/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0465\nEpoch 1226/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0464\nEpoch 1227/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0463\nEpoch 1228/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0462\nEpoch 1229/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0461\nEpoch 1230/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0344 - val_loss: 0.0460\nEpoch 1231/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0459\nEpoch 1232/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0458\nEpoch 1233/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0456\nEpoch 1234/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0455\nEpoch 1235/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0454\nEpoch 1236/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0454\nEpoch 1237/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0453\nEpoch 1238/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0452\nEpoch 1239/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0451\nEpoch 1240/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0343 - val_loss: 0.0450\nEpoch 1241/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0449\nEpoch 1242/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0448\nEpoch 1243/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0447\nEpoch 1244/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0446\nEpoch 1245/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0446\nEpoch 1246/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0343 - val_loss: 0.0444\nEpoch 1247/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0444\nEpoch 1248/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0443\nEpoch 1249/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0443\nEpoch 1250/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0442\nEpoch 1251/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0343 - val_loss: 0.0441\nEpoch 1252/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0440\nEpoch 1253/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0439\nEpoch 1254/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0438\nEpoch 1255/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0343 - val_loss: 0.0437\nEpoch 1256/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0436\nEpoch 1257/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0435\nEpoch 1258/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0434\nEpoch 1259/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0433\nEpoch 1260/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0432\nEpoch 1261/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0431\nEpoch 1262/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0431\nEpoch 1263/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0431\nEpoch 1264/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0430\nEpoch 1265/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0429\nEpoch 1266/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0428\nEpoch 1267/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0428\nEpoch 1268/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0427\nEpoch 1269/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0427\nEpoch 1270/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0426\nEpoch 1271/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0426\nEpoch 1272/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0425\nEpoch 1273/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0425\nEpoch 1274/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0424\nEpoch 1275/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0424\nEpoch 1276/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0423\nEpoch 1277/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0422\nEpoch 1278/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0422\nEpoch 1279/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0421\nEpoch 1280/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0421\nEpoch 1281/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0420\nEpoch 1282/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0419\nEpoch 1283/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0419\nEpoch 1284/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0419\nEpoch 1285/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0419\nEpoch 1286/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0418\nEpoch 1287/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0418\nEpoch 1288/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0417\nEpoch 1289/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0417\nEpoch 1290/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0416\nEpoch 1291/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0416\nEpoch 1292/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0415\nEpoch 1293/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0415\nEpoch 1294/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0414\nEpoch 1295/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0414\nEpoch 1296/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0413\nEpoch 1297/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0412\nEpoch 1298/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0412\nEpoch 1299/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0412\nEpoch 1300/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0412\nEpoch 1301/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0411\nEpoch 1302/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0411\nEpoch 1303/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0411\nEpoch 1304/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0410\nEpoch 1305/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0410\nEpoch 1306/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0409\nEpoch 1307/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0409\nEpoch 1308/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0409\nEpoch 1309/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0409\nEpoch 1310/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0408\nEpoch 1311/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0408\nEpoch 1312/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0408\nEpoch 1313/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0408\nEpoch 1314/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0407\nEpoch 1315/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0406\nEpoch 1316/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0406\nEpoch 1317/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0406\nEpoch 1318/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0405\nEpoch 1319/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0405\nEpoch 1320/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0405\nEpoch 1321/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0404\nEpoch 1322/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0404\nEpoch 1323/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0404\nEpoch 1324/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0403\nEpoch 1325/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0403\nEpoch 1326/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0403\nEpoch 1327/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0403\nEpoch 1328/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0403\nEpoch 1329/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0402\nEpoch 1330/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0402\nEpoch 1331/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0402\nEpoch 1332/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0401\nEpoch 1333/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0401\nEpoch 1334/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0401\nEpoch 1335/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0401\nEpoch 1336/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0401\nEpoch 1337/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0400\nEpoch 1338/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0400\nEpoch 1339/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0399\nEpoch 1340/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0399\nEpoch 1341/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0399\nEpoch 1342/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0399\nEpoch 1343/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0399\nEpoch 1344/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0398\nEpoch 1345/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0398\nEpoch 1346/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0398\nEpoch 1347/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0397\nEpoch 1348/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0397\nEpoch 1349/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0397\nEpoch 1350/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0397\nEpoch 1351/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0397\nEpoch 1352/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0397\nEpoch 1353/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0397\nEpoch 1354/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0397\nEpoch 1355/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0397\nEpoch 1356/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0397\nEpoch 1357/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0396\nEpoch 1358/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0397\nEpoch 1359/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0396\nEpoch 1360/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0396\nEpoch 1361/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0396\nEpoch 1362/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0396\nEpoch 1363/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0396\nEpoch 1364/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0396\nEpoch 1365/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0396\nEpoch 1366/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0396\nEpoch 1367/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0395\nEpoch 1368/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0395\nEpoch 1369/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0395\nEpoch 1370/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1371/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1372/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1373/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1374/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1375/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1376/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1377/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1378/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1379/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1380/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1381/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1382/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1383/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1384/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1385/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1386/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1387/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1388/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1389/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1390/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1391/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1392/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1393/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0392\nEpoch 1394/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0392\nEpoch 1395/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0392\nEpoch 1396/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1397/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1398/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1399/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1400/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1401/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1402/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1403/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1404/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1405/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1406/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1407/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1408/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1409/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1410/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1411/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1412/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1413/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1414/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1415/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1416/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1417/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1418/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1419/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1420/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1421/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1422/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1423/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1424/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1425/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1426/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1427/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1428/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1429/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1430/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1431/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1432/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1433/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1434/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1435/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1436/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1437/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1438/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1439/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1440/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1441/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1442/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1443/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1444/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1445/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1446/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1447/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1448/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1449/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1450/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1451/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1452/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1453/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1454/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1455/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1456/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1457/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1458/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1459/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1460/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1461/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1462/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1463/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1464/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1465/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1466/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1467/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1468/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1469/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1470/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1471/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1472/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1473/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1474/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1475/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1476/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1477/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1478/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1479/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1480/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1481/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1482/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1483/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1484/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1485/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1486/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1487/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1488/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1489/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1490/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1491/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1492/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1493/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1494/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1495/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1496/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1497/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1498/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1499/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1500/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1501/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1502/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1503/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1504/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1505/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1506/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1507/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1508/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1509/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1510/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1511/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1512/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1513/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1514/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1515/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1516/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1517/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1518/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1519/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1520/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1521/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1522/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1523/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1524/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1525/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1526/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1527/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1528/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1529/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1530/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1531/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1532/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1533/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1534/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1535/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1536/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1537/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1538/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1539/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1540/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1541/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1542/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1543/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1544/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1545/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1546/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1547/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1548/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1549/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1550/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1551/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1552/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1553/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1554/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1555/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1556/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1557/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1558/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1559/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1560/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1561/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1562/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1563/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1564/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1565/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1566/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1567/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1568/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1569/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1570/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1571/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1572/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1573/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1574/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1575/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1576/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1577/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1578/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1579/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1580/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1581/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1582/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1583/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1584/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1585/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1586/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1587/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1588/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1589/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1590/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1591/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1592/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1593/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1594/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1595/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1596/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1597/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1598/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1599/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1600/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1601/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1602/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1603/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1604/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1605/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1606/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1607/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1608/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1609/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1610/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1611/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1612/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1613/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1614/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1615/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1616/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1617/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1618/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1619/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1620/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1621/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1622/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1623/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1624/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1625/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1626/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1627/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1628/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1629/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1630/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1631/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1632/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1633/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1634/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1635/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1636/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1637/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1638/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1639/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1640/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1641/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1642/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1643/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1644/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1645/2000\n6/6 [==============================] - 0s 2ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1646/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1647/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1648/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1649/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1650/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1651/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1652/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1653/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1654/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1655/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1656/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1657/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1658/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1659/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1660/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1661/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1662/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1663/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1664/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1665/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1666/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1667/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1668/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1669/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1670/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1671/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1672/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1673/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1674/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1675/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1676/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1677/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1678/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1679/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1680/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1681/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1682/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1683/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1684/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1685/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1686/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1687/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1688/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1689/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1690/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1691/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1692/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1693/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1694/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1695/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1696/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1697/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1698/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1699/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1700/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1701/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1702/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1703/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1704/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1705/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1706/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1707/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1708/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1709/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1710/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1711/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1712/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1713/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1714/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1715/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1716/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1717/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1718/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1719/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1720/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1721/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1722/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1723/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1724/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1725/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1726/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1727/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1728/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1729/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1730/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1731/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1732/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1733/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1734/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1735/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1736/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1737/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1738/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1739/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1740/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1741/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1742/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1743/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1744/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1745/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1746/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1747/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1748/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1749/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1750/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1751/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1752/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1753/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1754/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1755/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1756/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1757/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1758/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1759/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1760/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1761/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1762/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1763/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1764/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1765/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1766/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1767/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1768/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1769/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1770/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1771/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1772/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1773/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1774/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1775/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1776/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1777/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1778/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1779/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1780/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1781/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1782/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1783/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1784/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1785/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1786/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1787/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1788/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1789/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1790/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1791/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1792/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1793/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1794/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1795/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1796/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1797/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1798/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1799/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1800/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1801/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1802/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1803/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1804/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1805/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1806/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1807/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1808/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1809/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1810/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1811/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1812/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1813/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1814/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1815/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1816/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1817/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1818/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1819/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1820/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1821/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1822/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1823/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1824/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1825/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1826/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1827/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1828/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1829/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1830/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1831/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1832/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1833/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1834/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1835/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1836/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1837/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1838/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1839/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1840/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1841/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1842/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1843/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1844/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1845/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1846/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1847/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1848/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1849/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1850/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1851/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1852/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1853/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1854/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1855/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1856/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1857/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1858/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1859/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1860/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1861/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1862/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1863/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1864/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1865/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1866/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1867/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1868/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1869/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1870/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1871/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1872/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1873/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1874/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1875/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1876/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1877/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1878/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1879/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1880/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1881/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1882/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1883/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1884/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1885/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1886/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1887/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1888/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1889/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1890/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1891/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1892/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1893/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1894/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1895/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1896/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1897/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1898/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1899/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1900/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1901/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1902/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1903/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1904/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1905/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1906/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1907/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1908/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1909/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1910/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1911/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1912/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1913/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1914/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1915/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1916/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1917/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1918/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1919/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1920/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1921/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1922/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1923/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1924/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1925/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1926/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1927/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1928/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1929/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1930/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1931/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1932/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1933/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1934/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1935/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1936/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1937/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1938/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1939/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1940/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1941/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1942/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1943/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1944/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1945/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1946/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1947/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1948/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1949/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1950/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1951/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1952/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1953/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1954/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1955/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1956/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1957/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1958/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1959/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1960/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1961/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1962/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1963/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1964/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1965/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1966/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1967/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1968/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1969/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1970/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1971/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1972/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1973/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1974/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1975/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1976/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1977/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1978/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1979/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1980/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1981/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1982/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1983/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1984/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1985/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1986/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1987/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1988/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1989/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1990/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1991/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1992/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1993/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1994/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1995/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1996/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1997/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1998/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1999/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 2000/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\n\n\n&lt;keras.callbacks.History at 0x7f708335a740&gt;\n\n\n\nnet.weights\n\n[&lt;tf.Variable 'dense_9/kernel:0' shape=(5, 1) dtype=float32, numpy=\n array([[ 2.9966218e+00],\n        [ 1.0097879e+00],\n        [-1.4235497e-02],\n        [ 3.8510602e-04],\n        [ 4.8625717e-01]], dtype=float32)&gt;,\n &lt;tf.Variable 'dense_9/bias:0' shape=(1,) dtype=float32, numpy=array([-2.0080342], dtype=float32)&gt;]\n\n\n\nnet.summary()\n\nModel: \"sequential_9\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense_9 (Dense)             (None, 1)                 6         \n                                                                 \n=================================================================\nTotal params: 6\nTrainable params: 6\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\n#\n#%tensorboard --logdir logs --host 0.0.0.0\n\n\n\n텐서보드: 사용자지정그림 에폭별로 시각화 (1)\n- 100에폭마다 적합결과를 시각화 하고 싶다 + 가중치와 같이!! - https://www.tensorflow.org/guide/keras/custom_callback\n- 일단 “100에폭마다 가중치적합과정 시각화 + 최종적합곡선 시각화” 까지 구현\n\n#collapse_output\n!rm -rf logs\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(1))\nnet.compile(loss='mse',optimizer='adam')\ncb1= tf.keras.callbacks.TensorBoard(update_freq='epoch',histogram_freq=100)\nnet.fit(X,y,epochs=2000, batch_size=100, validation_split=0.45,callbacks=cb1)\n\nEpoch 1/2000\n6/6 [==============================] - 0s 11ms/step - loss: 6.6990 - val_loss: 14.1016\nEpoch 2/2000\n6/6 [==============================] - 0s 4ms/step - loss: 6.6442 - val_loss: 14.0394\nEpoch 3/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.5913 - val_loss: 13.9775\nEpoch 4/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.5361 - val_loss: 13.9172\nEpoch 5/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.4823 - val_loss: 13.8573\nEpoch 6/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.4314 - val_loss: 13.7958\nEpoch 7/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.3769 - val_loss: 13.7367\nEpoch 8/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.3262 - val_loss: 13.6770\nEpoch 9/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.2734 - val_loss: 13.6171\nEpoch 10/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.2212 - val_loss: 13.5591\nEpoch 11/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.1701 - val_loss: 13.5023\nEpoch 12/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.1201 - val_loss: 13.4448\nEpoch 13/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.0705 - val_loss: 13.3874\nEpoch 14/2000\n6/6 [==============================] - 0s 4ms/step - loss: 6.0197 - val_loss: 13.3311\nEpoch 15/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.9705 - val_loss: 13.2742\nEpoch 16/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.9195 - val_loss: 13.2184\nEpoch 17/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.8713 - val_loss: 13.1627\nEpoch 18/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.8223 - val_loss: 13.1072\nEpoch 19/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.7729 - val_loss: 13.0523\nEpoch 20/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.7259 - val_loss: 12.9976\nEpoch 21/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.6771 - val_loss: 12.9449\nEpoch 22/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.6299 - val_loss: 12.8918\nEpoch 23/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.5827 - val_loss: 12.8385\nEpoch 24/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.5371 - val_loss: 12.7853\nEpoch 25/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.4892 - val_loss: 12.7333\nEpoch 26/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.4440 - val_loss: 12.6799\nEpoch 27/2000\n6/6 [==============================] - 0s 4ms/step - loss: 5.3974 - val_loss: 12.6281\nEpoch 28/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.3519 - val_loss: 12.5757\nEpoch 29/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.3069 - val_loss: 12.5253\nEpoch 30/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.2628 - val_loss: 12.4748\nEpoch 31/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.2181 - val_loss: 12.4244\nEpoch 32/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.1746 - val_loss: 12.3737\nEpoch 33/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.1311 - val_loss: 12.3234\nEpoch 34/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.0871 - val_loss: 12.2756\nEpoch 35/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.0448 - val_loss: 12.2277\nEpoch 36/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.0017 - val_loss: 12.1806\nEpoch 37/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.9595 - val_loss: 12.1327\nEpoch 38/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.9173 - val_loss: 12.0851\nEpoch 39/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.8764 - val_loss: 12.0371\nEpoch 40/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.8342 - val_loss: 11.9906\nEpoch 41/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.7936 - val_loss: 11.9438\nEpoch 42/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.7533 - val_loss: 11.8970\nEpoch 43/2000\n6/6 [==============================] - 0s 4ms/step - loss: 4.7121 - val_loss: 11.8512\nEpoch 44/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.6723 - val_loss: 11.8053\nEpoch 45/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.6318 - val_loss: 11.7611\nEpoch 46/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.5923 - val_loss: 11.7176\nEpoch 47/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.5532 - val_loss: 11.6732\nEpoch 48/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.5130 - val_loss: 11.6284\nEpoch 49/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.4748 - val_loss: 11.5833\nEpoch 50/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.4350 - val_loss: 11.5400\nEpoch 51/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.3965 - val_loss: 11.4966\nEpoch 52/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.3589 - val_loss: 11.4541\nEpoch 53/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.3203 - val_loss: 11.4124\nEpoch 54/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.2825 - val_loss: 11.3700\nEpoch 55/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.2451 - val_loss: 11.3271\nEpoch 56/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.2078 - val_loss: 11.2843\nEpoch 57/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.1703 - val_loss: 11.2416\nEpoch 58/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.1330 - val_loss: 11.1995\nEpoch 59/2000\n6/6 [==============================] - 0s 4ms/step - loss: 4.0959 - val_loss: 11.1581\nEpoch 60/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.0600 - val_loss: 11.1171\nEpoch 61/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.0237 - val_loss: 11.0773\nEpoch 62/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.9879 - val_loss: 11.0384\nEpoch 63/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.9532 - val_loss: 10.9997\nEpoch 64/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.9196 - val_loss: 10.9601\nEpoch 65/2000\n6/6 [==============================] - 0s 4ms/step - loss: 3.8845 - val_loss: 10.9219\nEpoch 66/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.8504 - val_loss: 10.8832\nEpoch 67/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.8163 - val_loss: 10.8449\nEpoch 68/2000\n6/6 [==============================] - 0s 4ms/step - loss: 3.7821 - val_loss: 10.8072\nEpoch 69/2000\n6/6 [==============================] - 0s 4ms/step - loss: 3.7485 - val_loss: 10.7692\nEpoch 70/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.7154 - val_loss: 10.7316\nEpoch 71/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.6823 - val_loss: 10.6955\nEpoch 72/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.6498 - val_loss: 10.6591\nEpoch 73/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.6170 - val_loss: 10.6233\nEpoch 74/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.5846 - val_loss: 10.5880\nEpoch 75/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.5533 - val_loss: 10.5517\nEpoch 76/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.5209 - val_loss: 10.5146\nEpoch 77/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.4886 - val_loss: 10.4796\nEpoch 78/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.4579 - val_loss: 10.4437\nEpoch 79/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.4264 - val_loss: 10.4078\nEpoch 80/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.3956 - val_loss: 10.3729\nEpoch 81/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.3637 - val_loss: 10.3384\nEpoch 82/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.3335 - val_loss: 10.3034\nEpoch 83/2000\n6/6 [==============================] - 0s 4ms/step - loss: 3.3032 - val_loss: 10.2686\nEpoch 84/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.2735 - val_loss: 10.2348\nEpoch 85/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.2434 - val_loss: 10.2016\nEpoch 86/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.2150 - val_loss: 10.1684\nEpoch 87/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.1850 - val_loss: 10.1359\nEpoch 88/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.1563 - val_loss: 10.1026\nEpoch 89/2000\n6/6 [==============================] - 0s 4ms/step - loss: 3.1270 - val_loss: 10.0714\nEpoch 90/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.0990 - val_loss: 10.0392\nEpoch 91/2000\n6/6 [==============================] - 0s 4ms/step - loss: 3.0712 - val_loss: 10.0072\nEpoch 92/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.0426 - val_loss: 9.9763\nEpoch 93/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.0153 - val_loss: 9.9454\nEpoch 94/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.9878 - val_loss: 9.9147\nEpoch 95/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.9603 - val_loss: 9.8845\nEpoch 96/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.9336 - val_loss: 9.8549\nEpoch 97/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.9068 - val_loss: 9.8259\nEpoch 98/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.8804 - val_loss: 9.7963\nEpoch 99/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.8538 - val_loss: 9.7673\nEpoch 100/2000\n6/6 [==============================] - 0s 4ms/step - loss: 2.8276 - val_loss: 9.7382\nEpoch 101/2000\n6/6 [==============================] - 0s 4ms/step - loss: 2.8017 - val_loss: 9.7100\nEpoch 102/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.7755 - val_loss: 9.6811\nEpoch 103/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.7499 - val_loss: 9.6516\nEpoch 104/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.7244 - val_loss: 9.6223\nEpoch 105/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.6994 - val_loss: 9.5934\nEpoch 106/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.6744 - val_loss: 9.5652\nEpoch 107/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.6495 - val_loss: 9.5379\nEpoch 108/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.6246 - val_loss: 9.5108\nEpoch 109/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.6008 - val_loss: 9.4831\nEpoch 110/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.5761 - val_loss: 9.4555\nEpoch 111/2000\n6/6 [==============================] - 0s 4ms/step - loss: 2.5520 - val_loss: 9.4283\nEpoch 112/2000\n6/6 [==============================] - 0s 4ms/step - loss: 2.5287 - val_loss: 9.4010\nEpoch 113/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.5052 - val_loss: 9.3744\nEpoch 114/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.4813 - val_loss: 9.3473\nEpoch 115/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.4583 - val_loss: 9.3206\nEpoch 116/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.4353 - val_loss: 9.2952\nEpoch 117/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.4123 - val_loss: 9.2694\nEpoch 118/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.3897 - val_loss: 9.2431\nEpoch 119/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.3671 - val_loss: 9.2181\nEpoch 120/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.3451 - val_loss: 9.1919\nEpoch 121/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.3226 - val_loss: 9.1674\nEpoch 122/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.3013 - val_loss: 9.1414\nEpoch 123/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.2790 - val_loss: 9.1171\nEpoch 124/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.2575 - val_loss: 9.0930\nEpoch 125/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.2361 - val_loss: 9.0694\nEpoch 126/2000\n6/6 [==============================] - 0s 4ms/step - loss: 2.2154 - val_loss: 9.0448\nEpoch 127/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.1942 - val_loss: 9.0199\nEpoch 128/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.1733 - val_loss: 8.9960\nEpoch 129/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.1526 - val_loss: 8.9720\nEpoch 130/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.1320 - val_loss: 8.9488\nEpoch 131/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.1119 - val_loss: 8.9255\nEpoch 132/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.0918 - val_loss: 8.9022\nEpoch 133/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.0718 - val_loss: 8.8786\nEpoch 134/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.0517 - val_loss: 8.8560\nEpoch 135/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.0321 - val_loss: 8.8331\nEpoch 136/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.0127 - val_loss: 8.8099\nEpoch 137/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.9936 - val_loss: 8.7871\nEpoch 138/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.9746 - val_loss: 8.7652\nEpoch 139/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.9557 - val_loss: 8.7429\nEpoch 140/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.9368 - val_loss: 8.7217\nEpoch 141/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.9192 - val_loss: 8.7001\nEpoch 142/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.9003 - val_loss: 8.6779\nEpoch 143/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.8826 - val_loss: 8.6553\nEpoch 144/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.8645 - val_loss: 8.6343\nEpoch 145/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.8470 - val_loss: 8.6119\nEpoch 146/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.8295 - val_loss: 8.5907\nEpoch 147/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.8117 - val_loss: 8.5700\nEpoch 148/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.7949 - val_loss: 8.5491\nEpoch 149/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.7779 - val_loss: 8.5288\nEpoch 150/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.7607 - val_loss: 8.5087\nEpoch 151/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.7440 - val_loss: 8.4884\nEpoch 152/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.7277 - val_loss: 8.4686\nEpoch 153/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.7107 - val_loss: 8.4488\nEpoch 154/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.6944 - val_loss: 8.4297\nEpoch 155/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.6783 - val_loss: 8.4099\nEpoch 156/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.6626 - val_loss: 8.3904\nEpoch 157/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.6466 - val_loss: 8.3712\nEpoch 158/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.6305 - val_loss: 8.3510\nEpoch 159/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.6148 - val_loss: 8.3309\nEpoch 160/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.5992 - val_loss: 8.3106\nEpoch 161/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.5839 - val_loss: 8.2924\nEpoch 162/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.5687 - val_loss: 8.2737\nEpoch 163/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.5537 - val_loss: 8.2558\nEpoch 164/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.5393 - val_loss: 8.2368\nEpoch 165/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.5241 - val_loss: 8.2189\nEpoch 166/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.5098 - val_loss: 8.2008\nEpoch 167/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.4951 - val_loss: 8.1831\nEpoch 168/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.4808 - val_loss: 8.1653\nEpoch 169/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.4669 - val_loss: 8.1461\nEpoch 170/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.4523 - val_loss: 8.1289\nEpoch 171/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.4385 - val_loss: 8.1115\nEpoch 172/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.4247 - val_loss: 8.0941\nEpoch 173/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.4108 - val_loss: 8.0772\nEpoch 174/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.3972 - val_loss: 8.0592\nEpoch 175/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.3837 - val_loss: 8.0412\nEpoch 176/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.3700 - val_loss: 8.0239\nEpoch 177/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.3571 - val_loss: 8.0066\nEpoch 178/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.3440 - val_loss: 7.9895\nEpoch 179/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.3308 - val_loss: 7.9722\nEpoch 180/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.3181 - val_loss: 7.9550\nEpoch 181/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.3055 - val_loss: 7.9387\nEpoch 182/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.2926 - val_loss: 7.9229\nEpoch 183/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.2805 - val_loss: 7.9067\nEpoch 184/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.2681 - val_loss: 7.8900\nEpoch 185/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.2556 - val_loss: 7.8737\nEpoch 186/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.2437 - val_loss: 7.8576\nEpoch 187/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.2318 - val_loss: 7.8403\nEpoch 188/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.2196 - val_loss: 7.8245\nEpoch 189/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.2079 - val_loss: 7.8084\nEpoch 190/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.1965 - val_loss: 7.7924\nEpoch 191/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.1850 - val_loss: 7.7777\nEpoch 192/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.1735 - val_loss: 7.7618\nEpoch 193/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.1625 - val_loss: 7.7459\nEpoch 194/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.1512 - val_loss: 7.7310\nEpoch 195/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.1403 - val_loss: 7.7166\nEpoch 196/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.1294 - val_loss: 7.7019\nEpoch 197/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.1186 - val_loss: 7.6871\nEpoch 198/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.1078 - val_loss: 7.6719\nEpoch 199/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0973 - val_loss: 7.6572\nEpoch 200/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0866 - val_loss: 7.6426\nEpoch 201/2000\n6/6 [==============================] - 0s 4ms/step - loss: 1.0762 - val_loss: 7.6278\nEpoch 202/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0657 - val_loss: 7.6135\nEpoch 203/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0558 - val_loss: 7.5977\nEpoch 204/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0456 - val_loss: 7.5829\nEpoch 205/2000\n6/6 [==============================] - 0s 4ms/step - loss: 1.0357 - val_loss: 7.5681\nEpoch 206/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0256 - val_loss: 7.5545\nEpoch 207/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0160 - val_loss: 7.5397\nEpoch 208/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0064 - val_loss: 7.5247\nEpoch 209/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9968 - val_loss: 7.5098\nEpoch 210/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9873 - val_loss: 7.4949\nEpoch 211/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9781 - val_loss: 7.4804\nEpoch 212/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9685 - val_loss: 7.4660\nEpoch 213/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.9593 - val_loss: 7.4515\nEpoch 214/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9503 - val_loss: 7.4373\nEpoch 215/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9412 - val_loss: 7.4229\nEpoch 216/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9323 - val_loss: 7.4090\nEpoch 217/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9237 - val_loss: 7.3950\nEpoch 218/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9149 - val_loss: 7.3820\nEpoch 219/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9065 - val_loss: 7.3686\nEpoch 220/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8979 - val_loss: 7.3558\nEpoch 221/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8896 - val_loss: 7.3422\nEpoch 222/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8814 - val_loss: 7.3290\nEpoch 223/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8731 - val_loss: 7.3155\nEpoch 224/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8654 - val_loss: 7.3014\nEpoch 225/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8572 - val_loss: 7.2881\nEpoch 226/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8492 - val_loss: 7.2753\nEpoch 227/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.8416 - val_loss: 7.2624\nEpoch 228/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8338 - val_loss: 7.2492\nEpoch 229/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8262 - val_loss: 7.2358\nEpoch 230/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.8185 - val_loss: 7.2236\nEpoch 231/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.8110 - val_loss: 7.2112\nEpoch 232/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8036 - val_loss: 7.1979\nEpoch 233/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.7962 - val_loss: 7.1849\nEpoch 234/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7890 - val_loss: 7.1720\nEpoch 235/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7817 - val_loss: 7.1592\nEpoch 236/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7746 - val_loss: 7.1468\nEpoch 237/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7674 - val_loss: 7.1337\nEpoch 238/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7605 - val_loss: 7.1196\nEpoch 239/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7536 - val_loss: 7.1064\nEpoch 240/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7467 - val_loss: 7.0940\nEpoch 241/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7399 - val_loss: 7.0820\nEpoch 242/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7333 - val_loss: 7.0690\nEpoch 243/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7267 - val_loss: 7.0568\nEpoch 244/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7202 - val_loss: 7.0443\nEpoch 245/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7139 - val_loss: 7.0311\nEpoch 246/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7075 - val_loss: 7.0185\nEpoch 247/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7012 - val_loss: 7.0054\nEpoch 248/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6948 - val_loss: 6.9938\nEpoch 249/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6887 - val_loss: 6.9809\nEpoch 250/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6827 - val_loss: 6.9690\nEpoch 251/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6768 - val_loss: 6.9565\nEpoch 252/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6709 - val_loss: 6.9443\nEpoch 253/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6650 - val_loss: 6.9322\nEpoch 254/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6590 - val_loss: 6.9206\nEpoch 255/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6533 - val_loss: 6.9084\nEpoch 256/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6477 - val_loss: 6.8959\nEpoch 257/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6422 - val_loss: 6.8833\nEpoch 258/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6366 - val_loss: 6.8716\nEpoch 259/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6312 - val_loss: 6.8591\nEpoch 260/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6258 - val_loss: 6.8477\nEpoch 261/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6206 - val_loss: 6.8359\nEpoch 262/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6153 - val_loss: 6.8237\nEpoch 263/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6101 - val_loss: 6.8123\nEpoch 264/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6050 - val_loss: 6.8006\nEpoch 265/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6000 - val_loss: 6.7892\nEpoch 266/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5949 - val_loss: 6.7776\nEpoch 267/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5900 - val_loss: 6.7652\nEpoch 268/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5849 - val_loss: 6.7529\nEpoch 269/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5801 - val_loss: 6.7406\nEpoch 270/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5753 - val_loss: 6.7281\nEpoch 271/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5705 - val_loss: 6.7161\nEpoch 272/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5658 - val_loss: 6.7042\nEpoch 273/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5612 - val_loss: 6.6921\nEpoch 274/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5567 - val_loss: 6.6803\nEpoch 275/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5521 - val_loss: 6.6690\nEpoch 276/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5478 - val_loss: 6.6569\nEpoch 277/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5433 - val_loss: 6.6454\nEpoch 278/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5391 - val_loss: 6.6337\nEpoch 279/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5346 - val_loss: 6.6222\nEpoch 280/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5306 - val_loss: 6.6105\nEpoch 281/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.5264 - val_loss: 6.5989\nEpoch 282/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5223 - val_loss: 6.5878\nEpoch 283/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5183 - val_loss: 6.5764\nEpoch 284/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5143 - val_loss: 6.5650\nEpoch 285/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5103 - val_loss: 6.5537\nEpoch 286/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5065 - val_loss: 6.5421\nEpoch 287/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5025 - val_loss: 6.5305\nEpoch 288/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4988 - val_loss: 6.5192\nEpoch 289/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4951 - val_loss: 6.5072\nEpoch 290/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4914 - val_loss: 6.4958\nEpoch 291/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4876 - val_loss: 6.4849\nEpoch 292/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4840 - val_loss: 6.4732\nEpoch 293/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4805 - val_loss: 6.4623\nEpoch 294/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.4768 - val_loss: 6.4509\nEpoch 295/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4733 - val_loss: 6.4391\nEpoch 296/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.4698 - val_loss: 6.4276\nEpoch 297/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4664 - val_loss: 6.4161\nEpoch 298/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4630 - val_loss: 6.4040\nEpoch 299/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4595 - val_loss: 6.3927\nEpoch 300/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4562 - val_loss: 6.3814\nEpoch 301/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.4529 - val_loss: 6.3698\nEpoch 302/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4497 - val_loss: 6.3581\nEpoch 303/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.4463 - val_loss: 6.3466\nEpoch 304/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4432 - val_loss: 6.3346\nEpoch 305/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4402 - val_loss: 6.3230\nEpoch 306/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4370 - val_loss: 6.3121\nEpoch 307/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4340 - val_loss: 6.3007\nEpoch 308/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4310 - val_loss: 6.2892\nEpoch 309/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4280 - val_loss: 6.2780\nEpoch 310/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4251 - val_loss: 6.2666\nEpoch 311/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4222 - val_loss: 6.2554\nEpoch 312/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4194 - val_loss: 6.2441\nEpoch 313/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4165 - val_loss: 6.2326\nEpoch 314/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4137 - val_loss: 6.2205\nEpoch 315/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.4110 - val_loss: 6.2087\nEpoch 316/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4082 - val_loss: 6.1973\nEpoch 317/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.4056 - val_loss: 6.1859\nEpoch 318/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4030 - val_loss: 6.1745\nEpoch 319/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4004 - val_loss: 6.1624\nEpoch 320/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3977 - val_loss: 6.1507\nEpoch 321/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3952 - val_loss: 6.1391\nEpoch 322/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3927 - val_loss: 6.1269\nEpoch 323/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3902 - val_loss: 6.1151\nEpoch 324/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3878 - val_loss: 6.1037\nEpoch 325/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3854 - val_loss: 6.0923\nEpoch 326/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3829 - val_loss: 6.0804\nEpoch 327/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3805 - val_loss: 6.0692\nEpoch 328/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3782 - val_loss: 6.0571\nEpoch 329/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.3759 - val_loss: 6.0447\nEpoch 330/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3735 - val_loss: 6.0329\nEpoch 331/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3713 - val_loss: 6.0203\nEpoch 332/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3690 - val_loss: 6.0089\nEpoch 333/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3668 - val_loss: 5.9969\nEpoch 334/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3645 - val_loss: 5.9852\nEpoch 335/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3625 - val_loss: 5.9734\nEpoch 336/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3603 - val_loss: 5.9614\nEpoch 337/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3581 - val_loss: 5.9501\nEpoch 338/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3561 - val_loss: 5.9386\nEpoch 339/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3541 - val_loss: 5.9270\nEpoch 340/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3521 - val_loss: 5.9156\nEpoch 341/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3501 - val_loss: 5.9042\nEpoch 342/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3481 - val_loss: 5.8924\nEpoch 343/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3463 - val_loss: 5.8808\nEpoch 344/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3442 - val_loss: 5.8688\nEpoch 345/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3424 - val_loss: 5.8569\nEpoch 346/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3405 - val_loss: 5.8452\nEpoch 347/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3386 - val_loss: 5.8337\nEpoch 348/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3369 - val_loss: 5.8213\nEpoch 349/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3350 - val_loss: 5.8097\nEpoch 350/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.3332 - val_loss: 5.7979\nEpoch 351/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3315 - val_loss: 5.7865\nEpoch 352/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.3297 - val_loss: 5.7747\nEpoch 353/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3280 - val_loss: 5.7631\nEpoch 354/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3263 - val_loss: 5.7515\nEpoch 355/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3246 - val_loss: 5.7400\nEpoch 356/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.3230 - val_loss: 5.7283\nEpoch 357/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3213 - val_loss: 5.7165\nEpoch 358/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.3197 - val_loss: 5.7047\nEpoch 359/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3181 - val_loss: 5.6931\nEpoch 360/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3165 - val_loss: 5.6812\nEpoch 361/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3150 - val_loss: 5.6690\nEpoch 362/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3134 - val_loss: 5.6579\nEpoch 363/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3118 - val_loss: 5.6461\nEpoch 364/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.3104 - val_loss: 5.6340\nEpoch 365/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3089 - val_loss: 5.6223\nEpoch 366/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3074 - val_loss: 5.6100\nEpoch 367/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3060 - val_loss: 5.5979\nEpoch 368/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3045 - val_loss: 5.5858\nEpoch 369/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3030 - val_loss: 5.5742\nEpoch 370/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3016 - val_loss: 5.5622\nEpoch 371/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3002 - val_loss: 5.5502\nEpoch 372/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2989 - val_loss: 5.5378\nEpoch 373/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2975 - val_loss: 5.5254\nEpoch 374/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2961 - val_loss: 5.5131\nEpoch 375/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2948 - val_loss: 5.5012\nEpoch 376/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2936 - val_loss: 5.4891\nEpoch 377/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2923 - val_loss: 5.4766\nEpoch 378/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2910 - val_loss: 5.4642\nEpoch 379/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2897 - val_loss: 5.4516\nEpoch 380/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2885 - val_loss: 5.4399\nEpoch 381/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2872 - val_loss: 5.4283\nEpoch 382/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2860 - val_loss: 5.4164\nEpoch 383/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2848 - val_loss: 5.4039\nEpoch 384/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2836 - val_loss: 5.3920\nEpoch 385/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2824 - val_loss: 5.3798\nEpoch 386/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2812 - val_loss: 5.3679\nEpoch 387/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2801 - val_loss: 5.3552\nEpoch 388/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2790 - val_loss: 5.3430\nEpoch 389/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2778 - val_loss: 5.3307\nEpoch 390/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2767 - val_loss: 5.3186\nEpoch 391/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2756 - val_loss: 5.3061\nEpoch 392/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2745 - val_loss: 5.2937\nEpoch 393/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2734 - val_loss: 5.2816\nEpoch 394/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2724 - val_loss: 5.2698\nEpoch 395/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2713 - val_loss: 5.2575\nEpoch 396/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2702 - val_loss: 5.2452\nEpoch 397/2000\n6/6 [==============================] - 0s 2ms/step - loss: 0.2692 - val_loss: 5.2331\nEpoch 398/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2681 - val_loss: 5.2213\nEpoch 399/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2671 - val_loss: 5.2088\nEpoch 400/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2661 - val_loss: 5.1963\nEpoch 401/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2651 - val_loss: 5.1842\nEpoch 402/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2641 - val_loss: 5.1719\nEpoch 403/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2631 - val_loss: 5.1593\nEpoch 404/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2620 - val_loss: 5.1472\nEpoch 405/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2611 - val_loss: 5.1346\nEpoch 406/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2601 - val_loss: 5.1224\nEpoch 407/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2592 - val_loss: 5.1101\nEpoch 408/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2583 - val_loss: 5.0975\nEpoch 409/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2573 - val_loss: 5.0852\nEpoch 410/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2564 - val_loss: 5.0724\nEpoch 411/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2555 - val_loss: 5.0598\nEpoch 412/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2546 - val_loss: 5.0475\nEpoch 413/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2536 - val_loss: 5.0351\nEpoch 414/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2528 - val_loss: 5.0226\nEpoch 415/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2519 - val_loss: 5.0102\nEpoch 416/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2510 - val_loss: 4.9976\nEpoch 417/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2501 - val_loss: 4.9857\nEpoch 418/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2492 - val_loss: 4.9732\nEpoch 419/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2483 - val_loss: 4.9606\nEpoch 420/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2475 - val_loss: 4.9481\nEpoch 421/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2466 - val_loss: 4.9357\nEpoch 422/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2458 - val_loss: 4.9231\nEpoch 423/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2450 - val_loss: 4.9108\nEpoch 424/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2442 - val_loss: 4.8982\nEpoch 425/2000\n6/6 [==============================] - 0s 2ms/step - loss: 0.2433 - val_loss: 4.8859\nEpoch 426/2000\n6/6 [==============================] - 0s 2ms/step - loss: 0.2425 - val_loss: 4.8733\nEpoch 427/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2417 - val_loss: 4.8607\nEpoch 428/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2409 - val_loss: 4.8482\nEpoch 429/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2402 - val_loss: 4.8354\nEpoch 430/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2393 - val_loss: 4.8233\nEpoch 431/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2386 - val_loss: 4.8107\nEpoch 432/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2378 - val_loss: 4.7986\nEpoch 433/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2371 - val_loss: 4.7858\nEpoch 434/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2363 - val_loss: 4.7728\nEpoch 435/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2355 - val_loss: 4.7603\nEpoch 436/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2347 - val_loss: 4.7470\nEpoch 437/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2340 - val_loss: 4.7336\nEpoch 438/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2331 - val_loss: 4.7212\nEpoch 439/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2324 - val_loss: 4.7081\nEpoch 440/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2317 - val_loss: 4.6952\nEpoch 441/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2309 - val_loss: 4.6824\nEpoch 442/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2301 - val_loss: 4.6697\nEpoch 443/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2294 - val_loss: 4.6565\nEpoch 444/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2286 - val_loss: 4.6438\nEpoch 445/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2280 - val_loss: 4.6312\nEpoch 446/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2272 - val_loss: 4.6187\nEpoch 447/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2265 - val_loss: 4.6062\nEpoch 448/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2258 - val_loss: 4.5933\nEpoch 449/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2251 - val_loss: 4.5809\nEpoch 450/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2244 - val_loss: 4.5682\nEpoch 451/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2237 - val_loss: 4.5557\nEpoch 452/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2230 - val_loss: 4.5432\nEpoch 453/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2223 - val_loss: 4.5304\nEpoch 454/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2216 - val_loss: 4.5172\nEpoch 455/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2209 - val_loss: 4.5045\nEpoch 456/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2203 - val_loss: 4.4913\nEpoch 457/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2196 - val_loss: 4.4780\nEpoch 458/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2189 - val_loss: 4.4653\nEpoch 459/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2182 - val_loss: 4.4529\nEpoch 460/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2176 - val_loss: 4.4401\nEpoch 461/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2169 - val_loss: 4.4275\nEpoch 462/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2163 - val_loss: 4.4144\nEpoch 463/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2156 - val_loss: 4.4018\nEpoch 464/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2150 - val_loss: 4.3893\nEpoch 465/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2143 - val_loss: 4.3762\nEpoch 466/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2137 - val_loss: 4.3634\nEpoch 467/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2131 - val_loss: 4.3504\nEpoch 468/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2125 - val_loss: 4.3378\nEpoch 469/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2118 - val_loss: 4.3251\nEpoch 470/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2112 - val_loss: 4.3126\nEpoch 471/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2106 - val_loss: 4.2997\nEpoch 472/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2100 - val_loss: 4.2867\nEpoch 473/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2093 - val_loss: 4.2739\nEpoch 474/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2086 - val_loss: 4.2609\nEpoch 475/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2080 - val_loss: 4.2479\nEpoch 476/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2074 - val_loss: 4.2348\nEpoch 477/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2068 - val_loss: 4.2221\nEpoch 478/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2062 - val_loss: 4.2093\nEpoch 479/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2056 - val_loss: 4.1964\nEpoch 480/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2050 - val_loss: 4.1831\nEpoch 481/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2044 - val_loss: 4.1703\nEpoch 482/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2038 - val_loss: 4.1576\nEpoch 483/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2031 - val_loss: 4.1450\nEpoch 484/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2025 - val_loss: 4.1318\nEpoch 485/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2019 - val_loss: 4.1189\nEpoch 486/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2013 - val_loss: 4.1055\nEpoch 487/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2007 - val_loss: 4.0927\nEpoch 488/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2001 - val_loss: 4.0799\nEpoch 489/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1995 - val_loss: 4.0670\nEpoch 490/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1989 - val_loss: 4.0543\nEpoch 491/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1983 - val_loss: 4.0412\nEpoch 492/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1977 - val_loss: 4.0288\nEpoch 493/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1971 - val_loss: 4.0160\nEpoch 494/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1965 - val_loss: 4.0029\nEpoch 495/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1960 - val_loss: 3.9893\nEpoch 496/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1954 - val_loss: 3.9760\nEpoch 497/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1948 - val_loss: 3.9628\nEpoch 498/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1942 - val_loss: 3.9498\nEpoch 499/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1936 - val_loss: 3.9373\nEpoch 500/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1931 - val_loss: 3.9246\nEpoch 501/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1925 - val_loss: 3.9113\nEpoch 502/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1919 - val_loss: 3.8987\nEpoch 503/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1914 - val_loss: 3.8862\nEpoch 504/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1907 - val_loss: 3.8739\nEpoch 505/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1901 - val_loss: 3.8608\nEpoch 506/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1896 - val_loss: 3.8478\nEpoch 507/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1889 - val_loss: 3.8350\nEpoch 508/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1884 - val_loss: 3.8218\nEpoch 509/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1878 - val_loss: 3.8091\nEpoch 510/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1872 - val_loss: 3.7960\nEpoch 511/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1867 - val_loss: 3.7827\nEpoch 512/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1861 - val_loss: 3.7695\nEpoch 513/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1855 - val_loss: 3.7564\nEpoch 514/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1849 - val_loss: 3.7437\nEpoch 515/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1843 - val_loss: 3.7308\nEpoch 516/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1837 - val_loss: 3.7178\nEpoch 517/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1832 - val_loss: 3.7048\nEpoch 518/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1826 - val_loss: 3.6917\nEpoch 519/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1821 - val_loss: 3.6781\nEpoch 520/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1815 - val_loss: 3.6648\nEpoch 521/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1809 - val_loss: 3.6515\nEpoch 522/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1804 - val_loss: 3.6382\nEpoch 523/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1799 - val_loss: 3.6252\nEpoch 524/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1793 - val_loss: 3.6128\nEpoch 525/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1787 - val_loss: 3.5998\nEpoch 526/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1781 - val_loss: 3.5868\nEpoch 527/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1776 - val_loss: 3.5735\nEpoch 528/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1770 - val_loss: 3.5601\nEpoch 529/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1765 - val_loss: 3.5467\nEpoch 530/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1759 - val_loss: 3.5337\nEpoch 531/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1753 - val_loss: 3.5207\nEpoch 532/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1748 - val_loss: 3.5074\nEpoch 533/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1743 - val_loss: 3.4943\nEpoch 534/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1737 - val_loss: 3.4814\nEpoch 535/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1731 - val_loss: 3.4686\nEpoch 536/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1726 - val_loss: 3.4557\nEpoch 537/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1721 - val_loss: 3.4433\nEpoch 538/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1715 - val_loss: 3.4307\nEpoch 539/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1710 - val_loss: 3.4174\nEpoch 540/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1704 - val_loss: 3.4044\nEpoch 541/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1699 - val_loss: 3.3917\nEpoch 542/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1693 - val_loss: 3.3786\nEpoch 543/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1688 - val_loss: 3.3658\nEpoch 544/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1683 - val_loss: 3.3522\nEpoch 545/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1677 - val_loss: 3.3393\nEpoch 546/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1672 - val_loss: 3.3264\nEpoch 547/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1667 - val_loss: 3.3137\nEpoch 548/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1661 - val_loss: 3.3012\nEpoch 549/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1656 - val_loss: 3.2885\nEpoch 550/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1651 - val_loss: 3.2761\nEpoch 551/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1646 - val_loss: 3.2636\nEpoch 552/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1641 - val_loss: 3.2506\nEpoch 553/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1635 - val_loss: 3.2386\nEpoch 554/2000\n6/6 [==============================] - 0s 2ms/step - loss: 0.1630 - val_loss: 3.2257\nEpoch 555/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1625 - val_loss: 3.2130\nEpoch 556/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1619 - val_loss: 3.2007\nEpoch 557/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1614 - val_loss: 3.1881\nEpoch 558/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1609 - val_loss: 3.1755\nEpoch 559/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1604 - val_loss: 3.1625\nEpoch 560/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1598 - val_loss: 3.1501\nEpoch 561/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1593 - val_loss: 3.1372\nEpoch 562/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1588 - val_loss: 3.1248\nEpoch 563/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1582 - val_loss: 3.1119\nEpoch 564/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1577 - val_loss: 3.0991\nEpoch 565/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1572 - val_loss: 3.0867\nEpoch 566/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1567 - val_loss: 3.0744\nEpoch 567/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1562 - val_loss: 3.0622\nEpoch 568/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1557 - val_loss: 3.0497\nEpoch 569/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1552 - val_loss: 3.0377\nEpoch 570/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1547 - val_loss: 3.0254\nEpoch 571/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1541 - val_loss: 3.0136\nEpoch 572/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1536 - val_loss: 3.0013\nEpoch 573/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1531 - val_loss: 2.9889\nEpoch 574/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1526 - val_loss: 2.9764\nEpoch 575/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1521 - val_loss: 2.9640\nEpoch 576/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1516 - val_loss: 2.9517\nEpoch 577/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1511 - val_loss: 2.9392\nEpoch 578/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1505 - val_loss: 2.9271\nEpoch 579/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1500 - val_loss: 2.9147\nEpoch 580/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1495 - val_loss: 2.9022\nEpoch 581/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1490 - val_loss: 2.8904\nEpoch 582/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1485 - val_loss: 2.8783\nEpoch 583/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1480 - val_loss: 2.8657\nEpoch 584/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1475 - val_loss: 2.8539\nEpoch 585/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1470 - val_loss: 2.8414\nEpoch 586/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1465 - val_loss: 2.8292\nEpoch 587/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1460 - val_loss: 2.8170\nEpoch 588/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1455 - val_loss: 2.8047\nEpoch 589/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1451 - val_loss: 2.7925\nEpoch 590/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1445 - val_loss: 2.7802\nEpoch 591/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1440 - val_loss: 2.7680\nEpoch 592/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1435 - val_loss: 2.7559\nEpoch 593/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1430 - val_loss: 2.7437\nEpoch 594/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1426 - val_loss: 2.7318\nEpoch 595/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1420 - val_loss: 2.7202\nEpoch 596/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1415 - val_loss: 2.7084\nEpoch 597/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1411 - val_loss: 2.6963\nEpoch 598/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1405 - val_loss: 2.6843\nEpoch 599/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1400 - val_loss: 2.6728\nEpoch 600/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1396 - val_loss: 2.6609\nEpoch 601/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1391 - val_loss: 2.6491\nEpoch 602/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1386 - val_loss: 2.6370\nEpoch 603/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1381 - val_loss: 2.6249\nEpoch 604/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1376 - val_loss: 2.6129\nEpoch 605/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1371 - val_loss: 2.6011\nEpoch 606/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1366 - val_loss: 2.5889\nEpoch 607/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1361 - val_loss: 2.5768\nEpoch 608/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1356 - val_loss: 2.5650\nEpoch 609/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1351 - val_loss: 2.5529\nEpoch 610/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1347 - val_loss: 2.5414\nEpoch 611/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1342 - val_loss: 2.5297\nEpoch 612/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1337 - val_loss: 2.5178\nEpoch 613/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1332 - val_loss: 2.5065\nEpoch 614/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1327 - val_loss: 2.4948\nEpoch 615/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1323 - val_loss: 2.4828\nEpoch 616/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1318 - val_loss: 2.4708\nEpoch 617/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1313 - val_loss: 2.4591\nEpoch 618/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1308 - val_loss: 2.4475\nEpoch 619/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1304 - val_loss: 2.4358\nEpoch 620/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1299 - val_loss: 2.4243\nEpoch 621/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1294 - val_loss: 2.4127\nEpoch 622/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1290 - val_loss: 2.4011\nEpoch 623/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1285 - val_loss: 2.3899\nEpoch 624/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1280 - val_loss: 2.3783\nEpoch 625/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1275 - val_loss: 2.3666\nEpoch 626/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1271 - val_loss: 2.3553\nEpoch 627/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1266 - val_loss: 2.3438\nEpoch 628/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1262 - val_loss: 2.3323\nEpoch 629/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1258 - val_loss: 2.3208\nEpoch 630/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1253 - val_loss: 2.3097\nEpoch 631/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1248 - val_loss: 2.2984\nEpoch 632/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1244 - val_loss: 2.2874\nEpoch 633/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1239 - val_loss: 2.2767\nEpoch 634/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1235 - val_loss: 2.2654\nEpoch 635/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1230 - val_loss: 2.2545\nEpoch 636/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1225 - val_loss: 2.2435\nEpoch 637/2000\n6/6 [==============================] - 0s 2ms/step - loss: 0.1221 - val_loss: 2.2321\nEpoch 638/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1217 - val_loss: 2.2217\nEpoch 639/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1213 - val_loss: 2.2106\nEpoch 640/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1208 - val_loss: 2.2000\nEpoch 641/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1204 - val_loss: 2.1890\nEpoch 642/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1199 - val_loss: 2.1783\nEpoch 643/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1194 - val_loss: 2.1677\nEpoch 644/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1190 - val_loss: 2.1568\nEpoch 645/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1186 - val_loss: 2.1462\nEpoch 646/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1182 - val_loss: 2.1353\nEpoch 647/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1177 - val_loss: 2.1245\nEpoch 648/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1173 - val_loss: 2.1141\nEpoch 649/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1168 - val_loss: 2.1036\nEpoch 650/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1164 - val_loss: 2.0928\nEpoch 651/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1160 - val_loss: 2.0823\nEpoch 652/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1155 - val_loss: 2.0714\nEpoch 653/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1151 - val_loss: 2.0609\nEpoch 654/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1147 - val_loss: 2.0502\nEpoch 655/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1143 - val_loss: 2.0392\nEpoch 656/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1138 - val_loss: 2.0286\nEpoch 657/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1134 - val_loss: 2.0182\nEpoch 658/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1130 - val_loss: 2.0077\nEpoch 659/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1125 - val_loss: 1.9972\nEpoch 660/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1121 - val_loss: 1.9869\nEpoch 661/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1117 - val_loss: 1.9767\nEpoch 662/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1113 - val_loss: 1.9659\nEpoch 663/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1109 - val_loss: 1.9554\nEpoch 664/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1104 - val_loss: 1.9449\nEpoch 665/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1100 - val_loss: 1.9350\nEpoch 666/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1096 - val_loss: 1.9250\nEpoch 667/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1091 - val_loss: 1.9149\nEpoch 668/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1087 - val_loss: 1.9043\nEpoch 669/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1083 - val_loss: 1.8937\nEpoch 670/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1079 - val_loss: 1.8833\nEpoch 671/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1075 - val_loss: 1.8730\nEpoch 672/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1071 - val_loss: 1.8626\nEpoch 673/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1066 - val_loss: 1.8525\nEpoch 674/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1062 - val_loss: 1.8425\nEpoch 675/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1058 - val_loss: 1.8325\nEpoch 676/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1054 - val_loss: 1.8222\nEpoch 677/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1050 - val_loss: 1.8121\nEpoch 678/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1045 - val_loss: 1.8023\nEpoch 679/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1042 - val_loss: 1.7921\nEpoch 680/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1038 - val_loss: 1.7819\nEpoch 681/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1033 - val_loss: 1.7721\nEpoch 682/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1029 - val_loss: 1.7620\nEpoch 683/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1025 - val_loss: 1.7520\nEpoch 684/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1021 - val_loss: 1.7421\nEpoch 685/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1018 - val_loss: 1.7322\nEpoch 686/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1013 - val_loss: 1.7224\nEpoch 687/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1010 - val_loss: 1.7127\nEpoch 688/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1006 - val_loss: 1.7031\nEpoch 689/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1002 - val_loss: 1.6937\nEpoch 690/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0998 - val_loss: 1.6841\nEpoch 691/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0994 - val_loss: 1.6741\nEpoch 692/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0990 - val_loss: 1.6640\nEpoch 693/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0986 - val_loss: 1.6543\nEpoch 694/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0982 - val_loss: 1.6454\nEpoch 695/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0979 - val_loss: 1.6361\nEpoch 696/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0975 - val_loss: 1.6266\nEpoch 697/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0971 - val_loss: 1.6170\nEpoch 698/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0967 - val_loss: 1.6073\nEpoch 699/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0963 - val_loss: 1.5979\nEpoch 700/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0960 - val_loss: 1.5886\nEpoch 701/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0956 - val_loss: 1.5796\nEpoch 702/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0952 - val_loss: 1.5706\nEpoch 703/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0949 - val_loss: 1.5613\nEpoch 704/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0945 - val_loss: 1.5523\nEpoch 705/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0941 - val_loss: 1.5429\nEpoch 706/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0938 - val_loss: 1.5336\nEpoch 707/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0934 - val_loss: 1.5247\nEpoch 708/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0930 - val_loss: 1.5156\nEpoch 709/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0926 - val_loss: 1.5068\nEpoch 710/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0923 - val_loss: 1.4980\nEpoch 711/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0919 - val_loss: 1.4888\nEpoch 712/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0915 - val_loss: 1.4796\nEpoch 713/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0912 - val_loss: 1.4702\nEpoch 714/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0908 - val_loss: 1.4612\nEpoch 715/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0904 - val_loss: 1.4523\nEpoch 716/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0901 - val_loss: 1.4433\nEpoch 717/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0897 - val_loss: 1.4345\nEpoch 718/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0894 - val_loss: 1.4258\nEpoch 719/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0890 - val_loss: 1.4173\nEpoch 720/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0886 - val_loss: 1.4085\nEpoch 721/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0883 - val_loss: 1.3997\nEpoch 722/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0879 - val_loss: 1.3913\nEpoch 723/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0876 - val_loss: 1.3827\nEpoch 724/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0873 - val_loss: 1.3742\nEpoch 725/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0869 - val_loss: 1.3656\nEpoch 726/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0866 - val_loss: 1.3571\nEpoch 727/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0862 - val_loss: 1.3485\nEpoch 728/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0859 - val_loss: 1.3402\nEpoch 729/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0856 - val_loss: 1.3317\nEpoch 730/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0852 - val_loss: 1.3236\nEpoch 731/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0849 - val_loss: 1.3152\nEpoch 732/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0845 - val_loss: 1.3069\nEpoch 733/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0842 - val_loss: 1.2986\nEpoch 734/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0839 - val_loss: 1.2901\nEpoch 735/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0835 - val_loss: 1.2821\nEpoch 736/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0832 - val_loss: 1.2735\nEpoch 737/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0828 - val_loss: 1.2655\nEpoch 738/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0825 - val_loss: 1.2574\nEpoch 739/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0822 - val_loss: 1.2491\nEpoch 740/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0819 - val_loss: 1.2410\nEpoch 741/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0815 - val_loss: 1.2326\nEpoch 742/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0812 - val_loss: 1.2245\nEpoch 743/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0809 - val_loss: 1.2162\nEpoch 744/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0806 - val_loss: 1.2082\nEpoch 745/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0802 - val_loss: 1.2005\nEpoch 746/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0799 - val_loss: 1.1927\nEpoch 747/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0796 - val_loss: 1.1848\nEpoch 748/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0793 - val_loss: 1.1771\nEpoch 749/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0790 - val_loss: 1.1692\nEpoch 750/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0787 - val_loss: 1.1613\nEpoch 751/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0784 - val_loss: 1.1534\nEpoch 752/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0780 - val_loss: 1.1456\nEpoch 753/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0778 - val_loss: 1.1380\nEpoch 754/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0774 - val_loss: 1.1301\nEpoch 755/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0771 - val_loss: 1.1222\nEpoch 756/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0768 - val_loss: 1.1146\nEpoch 757/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0765 - val_loss: 1.1073\nEpoch 758/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0762 - val_loss: 1.0998\nEpoch 759/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0759 - val_loss: 1.0928\nEpoch 760/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0756 - val_loss: 1.0854\nEpoch 761/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0754 - val_loss: 1.0781\nEpoch 762/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0751 - val_loss: 1.0707\nEpoch 763/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0748 - val_loss: 1.0630\nEpoch 764/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0745 - val_loss: 1.0560\nEpoch 765/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0742 - val_loss: 1.0486\nEpoch 766/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0739 - val_loss: 1.0412\nEpoch 767/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0736 - val_loss: 1.0341\nEpoch 768/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0733 - val_loss: 1.0270\nEpoch 769/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0730 - val_loss: 1.0200\nEpoch 770/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0727 - val_loss: 1.0131\nEpoch 771/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0725 - val_loss: 1.0066\nEpoch 772/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0722 - val_loss: 0.9998\nEpoch 773/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0719 - val_loss: 0.9929\nEpoch 774/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0716 - val_loss: 0.9858\nEpoch 775/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0713 - val_loss: 0.9790\nEpoch 776/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0711 - val_loss: 0.9719\nEpoch 777/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0708 - val_loss: 0.9653\nEpoch 778/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0705 - val_loss: 0.9583\nEpoch 779/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0702 - val_loss: 0.9515\nEpoch 780/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0700 - val_loss: 0.9449\nEpoch 781/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0697 - val_loss: 0.9380\nEpoch 782/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0694 - val_loss: 0.9312\nEpoch 783/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0692 - val_loss: 0.9246\nEpoch 784/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0689 - val_loss: 0.9182\nEpoch 785/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0686 - val_loss: 0.9118\nEpoch 786/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0684 - val_loss: 0.9054\nEpoch 787/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0681 - val_loss: 0.8986\nEpoch 788/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0679 - val_loss: 0.8915\nEpoch 789/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0676 - val_loss: 0.8849\nEpoch 790/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0673 - val_loss: 0.8786\nEpoch 791/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0671 - val_loss: 0.8724\nEpoch 792/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0668 - val_loss: 0.8663\nEpoch 793/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0666 - val_loss: 0.8602\nEpoch 794/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0663 - val_loss: 0.8539\nEpoch 795/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0661 - val_loss: 0.8475\nEpoch 796/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0658 - val_loss: 0.8416\nEpoch 797/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0656 - val_loss: 0.8354\nEpoch 798/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0653 - val_loss: 0.8295\nEpoch 799/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0651 - val_loss: 0.8233\nEpoch 800/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0648 - val_loss: 0.8172\nEpoch 801/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0646 - val_loss: 0.8115\nEpoch 802/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0644 - val_loss: 0.8055\nEpoch 803/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0641 - val_loss: 0.7994\nEpoch 804/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0639 - val_loss: 0.7937\nEpoch 805/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0636 - val_loss: 0.7877\nEpoch 806/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0634 - val_loss: 0.7819\nEpoch 807/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0632 - val_loss: 0.7761\nEpoch 808/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0629 - val_loss: 0.7702\nEpoch 809/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0627 - val_loss: 0.7640\nEpoch 810/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0625 - val_loss: 0.7584\nEpoch 811/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0622 - val_loss: 0.7529\nEpoch 812/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0620 - val_loss: 0.7469\nEpoch 813/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0618 - val_loss: 0.7411\nEpoch 814/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0615 - val_loss: 0.7350\nEpoch 815/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0613 - val_loss: 0.7292\nEpoch 816/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0610 - val_loss: 0.7238\nEpoch 817/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0609 - val_loss: 0.7180\nEpoch 818/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0606 - val_loss: 0.7123\nEpoch 819/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0604 - val_loss: 0.7069\nEpoch 820/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0601 - val_loss: 0.7016\nEpoch 821/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0599 - val_loss: 0.6959\nEpoch 822/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0597 - val_loss: 0.6902\nEpoch 823/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0595 - val_loss: 0.6845\nEpoch 824/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0592 - val_loss: 0.6790\nEpoch 825/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0590 - val_loss: 0.6735\nEpoch 826/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0588 - val_loss: 0.6681\nEpoch 827/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0586 - val_loss: 0.6627\nEpoch 828/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0584 - val_loss: 0.6575\nEpoch 829/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0582 - val_loss: 0.6524\nEpoch 830/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0580 - val_loss: 0.6472\nEpoch 831/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0578 - val_loss: 0.6421\nEpoch 832/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0576 - val_loss: 0.6370\nEpoch 833/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0573 - val_loss: 0.6318\nEpoch 834/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0572 - val_loss: 0.6265\nEpoch 835/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0569 - val_loss: 0.6211\nEpoch 836/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0567 - val_loss: 0.6162\nEpoch 837/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0565 - val_loss: 0.6111\nEpoch 838/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0563 - val_loss: 0.6061\nEpoch 839/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0561 - val_loss: 0.6013\nEpoch 840/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0559 - val_loss: 0.5965\nEpoch 841/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0557 - val_loss: 0.5917\nEpoch 842/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0556 - val_loss: 0.5869\nEpoch 843/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0554 - val_loss: 0.5819\nEpoch 844/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0552 - val_loss: 0.5771\nEpoch 845/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0550 - val_loss: 0.5722\nEpoch 846/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0548 - val_loss: 0.5672\nEpoch 847/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0546 - val_loss: 0.5627\nEpoch 848/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0544 - val_loss: 0.5580\nEpoch 849/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0542 - val_loss: 0.5534\nEpoch 850/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0540 - val_loss: 0.5487\nEpoch 851/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0539 - val_loss: 0.5441\nEpoch 852/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0537 - val_loss: 0.5395\nEpoch 853/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0535 - val_loss: 0.5350\nEpoch 854/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0533 - val_loss: 0.5304\nEpoch 855/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0531 - val_loss: 0.5259\nEpoch 856/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0529 - val_loss: 0.5214\nEpoch 857/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0528 - val_loss: 0.5170\nEpoch 858/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0526 - val_loss: 0.5125\nEpoch 859/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0524 - val_loss: 0.5082\nEpoch 860/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0522 - val_loss: 0.5039\nEpoch 861/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0521 - val_loss: 0.4995\nEpoch 862/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0519 - val_loss: 0.4952\nEpoch 863/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0517 - val_loss: 0.4909\nEpoch 864/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0515 - val_loss: 0.4865\nEpoch 865/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0514 - val_loss: 0.4823\nEpoch 866/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0512 - val_loss: 0.4780\nEpoch 867/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0510 - val_loss: 0.4738\nEpoch 868/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0509 - val_loss: 0.4696\nEpoch 869/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0507 - val_loss: 0.4655\nEpoch 870/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0505 - val_loss: 0.4615\nEpoch 871/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0504 - val_loss: 0.4575\nEpoch 872/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0502 - val_loss: 0.4533\nEpoch 873/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0501 - val_loss: 0.4495\nEpoch 874/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0499 - val_loss: 0.4459\nEpoch 875/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0498 - val_loss: 0.4420\nEpoch 876/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0496 - val_loss: 0.4379\nEpoch 877/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0494 - val_loss: 0.4340\nEpoch 878/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0493 - val_loss: 0.4303\nEpoch 879/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0492 - val_loss: 0.4264\nEpoch 880/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0490 - val_loss: 0.4226\nEpoch 881/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0489 - val_loss: 0.4190\nEpoch 882/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0487 - val_loss: 0.4154\nEpoch 883/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0486 - val_loss: 0.4118\nEpoch 884/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0484 - val_loss: 0.4082\nEpoch 885/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0483 - val_loss: 0.4049\nEpoch 886/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0481 - val_loss: 0.4012\nEpoch 887/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0480 - val_loss: 0.3976\nEpoch 888/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0479 - val_loss: 0.3939\nEpoch 889/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0477 - val_loss: 0.3903\nEpoch 890/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0476 - val_loss: 0.3869\nEpoch 891/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0474 - val_loss: 0.3831\nEpoch 892/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0473 - val_loss: 0.3796\nEpoch 893/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0471 - val_loss: 0.3762\nEpoch 894/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0470 - val_loss: 0.3729\nEpoch 895/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0469 - val_loss: 0.3695\nEpoch 896/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0467 - val_loss: 0.3662\nEpoch 897/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0466 - val_loss: 0.3629\nEpoch 898/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0465 - val_loss: 0.3596\nEpoch 899/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0463 - val_loss: 0.3563\nEpoch 900/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0462 - val_loss: 0.3530\nEpoch 901/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0461 - val_loss: 0.3499\nEpoch 902/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0460 - val_loss: 0.3468\nEpoch 903/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0458 - val_loss: 0.3435\nEpoch 904/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0457 - val_loss: 0.3403\nEpoch 905/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0456 - val_loss: 0.3371\nEpoch 906/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0455 - val_loss: 0.3340\nEpoch 907/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0453 - val_loss: 0.3310\nEpoch 908/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0452 - val_loss: 0.3280\nEpoch 909/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0451 - val_loss: 0.3251\nEpoch 910/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0450 - val_loss: 0.3219\nEpoch 911/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0449 - val_loss: 0.3189\nEpoch 912/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0448 - val_loss: 0.3159\nEpoch 913/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0446 - val_loss: 0.3130\nEpoch 914/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0445 - val_loss: 0.3100\nEpoch 915/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0444 - val_loss: 0.3072\nEpoch 916/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0443 - val_loss: 0.3044\nEpoch 917/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0442 - val_loss: 0.3015\nEpoch 918/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0441 - val_loss: 0.2987\nEpoch 919/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0440 - val_loss: 0.2960\nEpoch 920/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0439 - val_loss: 0.2933\nEpoch 921/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0438 - val_loss: 0.2905\nEpoch 922/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0437 - val_loss: 0.2878\nEpoch 923/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0435 - val_loss: 0.2853\nEpoch 924/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0434 - val_loss: 0.2827\nEpoch 925/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0433 - val_loss: 0.2801\nEpoch 926/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0432 - val_loss: 0.2777\nEpoch 927/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0431 - val_loss: 0.2752\nEpoch 928/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0430 - val_loss: 0.2727\nEpoch 929/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0429 - val_loss: 0.2703\nEpoch 930/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0428 - val_loss: 0.2679\nEpoch 931/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0427 - val_loss: 0.2655\nEpoch 932/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0427 - val_loss: 0.2633\nEpoch 933/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0426 - val_loss: 0.2607\nEpoch 934/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0425 - val_loss: 0.2584\nEpoch 935/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0424 - val_loss: 0.2559\nEpoch 936/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0423 - val_loss: 0.2534\nEpoch 937/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0422 - val_loss: 0.2510\nEpoch 938/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0421 - val_loss: 0.2486\nEpoch 939/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0420 - val_loss: 0.2461\nEpoch 940/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0419 - val_loss: 0.2437\nEpoch 941/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0418 - val_loss: 0.2413\nEpoch 942/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0417 - val_loss: 0.2390\nEpoch 943/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0416 - val_loss: 0.2367\nEpoch 944/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0416 - val_loss: 0.2345\nEpoch 945/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0415 - val_loss: 0.2322\nEpoch 946/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0414 - val_loss: 0.2300\nEpoch 947/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0413 - val_loss: 0.2276\nEpoch 948/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0412 - val_loss: 0.2254\nEpoch 949/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0411 - val_loss: 0.2232\nEpoch 950/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0410 - val_loss: 0.2211\nEpoch 951/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0409 - val_loss: 0.2190\nEpoch 952/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0409 - val_loss: 0.2170\nEpoch 953/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0408 - val_loss: 0.2151\nEpoch 954/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0407 - val_loss: 0.2131\nEpoch 955/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0406 - val_loss: 0.2111\nEpoch 956/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0405 - val_loss: 0.2091\nEpoch 957/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0405 - val_loss: 0.2071\nEpoch 958/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0404 - val_loss: 0.2051\nEpoch 959/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0403 - val_loss: 0.2030\nEpoch 960/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0402 - val_loss: 0.2010\nEpoch 961/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0401 - val_loss: 0.1989\nEpoch 962/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0401 - val_loss: 0.1972\nEpoch 963/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0400 - val_loss: 0.1953\nEpoch 964/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0399 - val_loss: 0.1932\nEpoch 965/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0399 - val_loss: 0.1913\nEpoch 966/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0398 - val_loss: 0.1894\nEpoch 967/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0397 - val_loss: 0.1875\nEpoch 968/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0396 - val_loss: 0.1856\nEpoch 969/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0396 - val_loss: 0.1839\nEpoch 970/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0395 - val_loss: 0.1821\nEpoch 971/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0394 - val_loss: 0.1802\nEpoch 972/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0394 - val_loss: 0.1784\nEpoch 973/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0393 - val_loss: 0.1767\nEpoch 974/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0392 - val_loss: 0.1751\nEpoch 975/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0392 - val_loss: 0.1734\nEpoch 976/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0391 - val_loss: 0.1719\nEpoch 977/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0390 - val_loss: 0.1703\nEpoch 978/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0390 - val_loss: 0.1687\nEpoch 979/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0389 - val_loss: 0.1670\nEpoch 980/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0388 - val_loss: 0.1656\nEpoch 981/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0388 - val_loss: 0.1639\nEpoch 982/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0387 - val_loss: 0.1624\nEpoch 983/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0387 - val_loss: 0.1608\nEpoch 984/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0386 - val_loss: 0.1592\nEpoch 985/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0385 - val_loss: 0.1577\nEpoch 986/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0385 - val_loss: 0.1563\nEpoch 987/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0384 - val_loss: 0.1548\nEpoch 988/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0384 - val_loss: 0.1533\nEpoch 989/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0383 - val_loss: 0.1518\nEpoch 990/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0383 - val_loss: 0.1504\nEpoch 991/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0382 - val_loss: 0.1488\nEpoch 992/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0382 - val_loss: 0.1475\nEpoch 993/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0381 - val_loss: 0.1462\nEpoch 994/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0381 - val_loss: 0.1448\nEpoch 995/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0380 - val_loss: 0.1434\nEpoch 996/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0379 - val_loss: 0.1421\nEpoch 997/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0379 - val_loss: 0.1407\nEpoch 998/2000\n6/6 [==============================] - 0s 2ms/step - loss: 0.0378 - val_loss: 0.1393\nEpoch 999/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0378 - val_loss: 0.1380\nEpoch 1000/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0377 - val_loss: 0.1367\nEpoch 1001/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0377 - val_loss: 0.1355\nEpoch 1002/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0376 - val_loss: 0.1343\nEpoch 1003/2000\n6/6 [==============================] - ETA: 0s - loss: 0.032 - 0s 4ms/step - loss: 0.0376 - val_loss: 0.1331\nEpoch 1004/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0375 - val_loss: 0.1320\nEpoch 1005/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0375 - val_loss: 0.1308\nEpoch 1006/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0374 - val_loss: 0.1296\nEpoch 1007/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0374 - val_loss: 0.1284\nEpoch 1008/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0374 - val_loss: 0.1272\nEpoch 1009/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0373 - val_loss: 0.1261\nEpoch 1010/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0373 - val_loss: 0.1250\nEpoch 1011/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0372 - val_loss: 0.1239\nEpoch 1012/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0372 - val_loss: 0.1229\nEpoch 1013/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0371 - val_loss: 0.1218\nEpoch 1014/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0371 - val_loss: 0.1207\nEpoch 1015/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0371 - val_loss: 0.1196\nEpoch 1016/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0370 - val_loss: 0.1185\nEpoch 1017/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0370 - val_loss: 0.1174\nEpoch 1018/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0369 - val_loss: 0.1163\nEpoch 1019/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0369 - val_loss: 0.1153\nEpoch 1020/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0369 - val_loss: 0.1143\nEpoch 1021/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0368 - val_loss: 0.1133\nEpoch 1022/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0368 - val_loss: 0.1123\nEpoch 1023/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0368 - val_loss: 0.1114\nEpoch 1024/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0367 - val_loss: 0.1104\nEpoch 1025/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0367 - val_loss: 0.1093\nEpoch 1026/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0366 - val_loss: 0.1082\nEpoch 1027/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0366 - val_loss: 0.1073\nEpoch 1028/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0366 - val_loss: 0.1063\nEpoch 1029/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0365 - val_loss: 0.1054\nEpoch 1030/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0365 - val_loss: 0.1044\nEpoch 1031/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0365 - val_loss: 0.1036\nEpoch 1032/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0364 - val_loss: 0.1027\nEpoch 1033/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0364 - val_loss: 0.1019\nEpoch 1034/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0364 - val_loss: 0.1009\nEpoch 1035/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0363 - val_loss: 0.1001\nEpoch 1036/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0363 - val_loss: 0.0992\nEpoch 1037/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0363 - val_loss: 0.0983\nEpoch 1038/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0362 - val_loss: 0.0975\nEpoch 1039/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0362 - val_loss: 0.0965\nEpoch 1040/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0362 - val_loss: 0.0956\nEpoch 1041/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0361 - val_loss: 0.0948\nEpoch 1042/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0361 - val_loss: 0.0940\nEpoch 1043/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0361 - val_loss: 0.0932\nEpoch 1044/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0360 - val_loss: 0.0925\nEpoch 1045/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0360 - val_loss: 0.0917\nEpoch 1046/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0360 - val_loss: 0.0910\nEpoch 1047/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0360 - val_loss: 0.0902\nEpoch 1048/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0359 - val_loss: 0.0894\nEpoch 1049/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0359 - val_loss: 0.0886\nEpoch 1050/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0359 - val_loss: 0.0878\nEpoch 1051/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0359 - val_loss: 0.0870\nEpoch 1052/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0358 - val_loss: 0.0863\nEpoch 1053/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0358 - val_loss: 0.0857\nEpoch 1054/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0358 - val_loss: 0.0851\nEpoch 1055/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0357 - val_loss: 0.0844\nEpoch 1056/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0357 - val_loss: 0.0838\nEpoch 1057/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0357 - val_loss: 0.0831\nEpoch 1058/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0357 - val_loss: 0.0825\nEpoch 1059/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0357 - val_loss: 0.0819\nEpoch 1060/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0356 - val_loss: 0.0813\nEpoch 1061/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0356 - val_loss: 0.0807\nEpoch 1062/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0356 - val_loss: 0.0801\nEpoch 1063/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0355 - val_loss: 0.0795\nEpoch 1064/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0355 - val_loss: 0.0789\nEpoch 1065/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0355 - val_loss: 0.0783\nEpoch 1066/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0355 - val_loss: 0.0777\nEpoch 1067/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0355 - val_loss: 0.0771\nEpoch 1068/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0354 - val_loss: 0.0765\nEpoch 1069/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0354 - val_loss: 0.0759\nEpoch 1070/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0354 - val_loss: 0.0753\nEpoch 1071/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0354 - val_loss: 0.0748\nEpoch 1072/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0354 - val_loss: 0.0742\nEpoch 1073/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0353 - val_loss: 0.0736\nEpoch 1074/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0353 - val_loss: 0.0730\nEpoch 1075/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0353 - val_loss: 0.0724\nEpoch 1076/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0353 - val_loss: 0.0719\nEpoch 1077/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0353 - val_loss: 0.0714\nEpoch 1078/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0710\nEpoch 1079/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0705\nEpoch 1080/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0700\nEpoch 1081/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0695\nEpoch 1082/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0690\nEpoch 1083/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0351 - val_loss: 0.0685\nEpoch 1084/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0351 - val_loss: 0.0680\nEpoch 1085/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0675\nEpoch 1086/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0671\nEpoch 1087/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0667\nEpoch 1088/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0662\nEpoch 1089/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0659\nEpoch 1090/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0350 - val_loss: 0.0655\nEpoch 1091/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0651\nEpoch 1092/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0646\nEpoch 1093/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0642\nEpoch 1094/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0637\nEpoch 1095/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0633\nEpoch 1096/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0629\nEpoch 1097/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0625\nEpoch 1098/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0622\nEpoch 1099/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0618\nEpoch 1100/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0614\nEpoch 1101/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0611\nEpoch 1102/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0349 - val_loss: 0.0607\nEpoch 1103/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0604\nEpoch 1104/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0601\nEpoch 1105/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0598\nEpoch 1106/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0595\nEpoch 1107/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0591\nEpoch 1108/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0587\nEpoch 1109/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0585\nEpoch 1110/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0582\nEpoch 1111/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0579\nEpoch 1112/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0575\nEpoch 1113/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0572\nEpoch 1114/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0570\nEpoch 1115/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0567\nEpoch 1116/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0563\nEpoch 1117/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0560\nEpoch 1118/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0557\nEpoch 1119/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0555\nEpoch 1120/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0551\nEpoch 1121/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0548\nEpoch 1122/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0545\nEpoch 1123/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0542\nEpoch 1124/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0539\nEpoch 1125/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0538\nEpoch 1126/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0536\nEpoch 1127/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0533\nEpoch 1128/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0531\nEpoch 1129/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0528\nEpoch 1130/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0526\nEpoch 1131/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0346 - val_loss: 0.0524\nEpoch 1132/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0522\nEpoch 1133/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0520\nEpoch 1134/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0346 - val_loss: 0.0518\nEpoch 1135/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0515\nEpoch 1136/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0513\nEpoch 1137/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0345 - val_loss: 0.0511\nEpoch 1138/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0345 - val_loss: 0.0509\nEpoch 1139/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0507\nEpoch 1140/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0505\nEpoch 1141/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0503\nEpoch 1142/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0502\nEpoch 1143/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0500\nEpoch 1144/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0498\nEpoch 1145/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0496\nEpoch 1146/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0494\nEpoch 1147/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0492\nEpoch 1148/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0490\nEpoch 1149/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0345 - val_loss: 0.0488\nEpoch 1150/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0345 - val_loss: 0.0486\nEpoch 1151/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0344 - val_loss: 0.0484\nEpoch 1152/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0482\nEpoch 1153/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0481\nEpoch 1154/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0480\nEpoch 1155/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0478\nEpoch 1156/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0476\nEpoch 1157/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0474\nEpoch 1158/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0472\nEpoch 1159/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0471\nEpoch 1160/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0470\nEpoch 1161/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0468\nEpoch 1162/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0467\nEpoch 1163/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0465\nEpoch 1164/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0463\nEpoch 1165/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0462\nEpoch 1166/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0461\nEpoch 1167/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0460\nEpoch 1168/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0459\nEpoch 1169/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0458\nEpoch 1170/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0456\nEpoch 1171/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0455\nEpoch 1172/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0454\nEpoch 1173/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0343 - val_loss: 0.0453\nEpoch 1174/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0452\nEpoch 1175/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0451\nEpoch 1176/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0450\nEpoch 1177/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0449\nEpoch 1178/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0448\nEpoch 1179/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0447\nEpoch 1180/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0446\nEpoch 1181/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0444\nEpoch 1182/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0443\nEpoch 1183/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0442\nEpoch 1184/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0441\nEpoch 1185/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0440\nEpoch 1186/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0439\nEpoch 1187/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0438\nEpoch 1188/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0436\nEpoch 1189/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0435\nEpoch 1190/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0435\nEpoch 1191/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0434\nEpoch 1192/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0433\nEpoch 1193/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0432\nEpoch 1194/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0431\nEpoch 1195/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0430\nEpoch 1196/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0430\nEpoch 1197/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0343 - val_loss: 0.0429\nEpoch 1198/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0428\nEpoch 1199/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0343 - val_loss: 0.0428\nEpoch 1200/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0427\nEpoch 1201/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0426\nEpoch 1202/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0425\nEpoch 1203/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0425\nEpoch 1204/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0424\nEpoch 1205/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0423\nEpoch 1206/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0422\nEpoch 1207/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0422\nEpoch 1208/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0421\nEpoch 1209/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0421\nEpoch 1210/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0420\nEpoch 1211/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0419\nEpoch 1212/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0419\nEpoch 1213/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0419\nEpoch 1214/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0418\nEpoch 1215/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0418\nEpoch 1216/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0417\nEpoch 1217/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0417\nEpoch 1218/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0416\nEpoch 1219/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0415\nEpoch 1220/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0415\nEpoch 1221/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0414\nEpoch 1222/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0414\nEpoch 1223/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0413\nEpoch 1224/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0412\nEpoch 1225/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0412\nEpoch 1226/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0411\nEpoch 1227/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0411\nEpoch 1228/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0411\nEpoch 1229/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0410\nEpoch 1230/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0410\nEpoch 1231/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0409\nEpoch 1232/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0409\nEpoch 1233/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0408\nEpoch 1234/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0408\nEpoch 1235/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0407\nEpoch 1236/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0407\nEpoch 1237/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0406\nEpoch 1238/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0406\nEpoch 1239/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0405\nEpoch 1240/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0405\nEpoch 1241/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0405\nEpoch 1242/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0404\nEpoch 1243/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0404\nEpoch 1244/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0404\nEpoch 1245/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0403\nEpoch 1246/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0403\nEpoch 1247/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0402\nEpoch 1248/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0402\nEpoch 1249/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0402\nEpoch 1250/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0402\nEpoch 1251/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0402\nEpoch 1252/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0401\nEpoch 1253/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0401\nEpoch 1254/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0401\nEpoch 1255/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0400\nEpoch 1256/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0400\nEpoch 1257/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0400\nEpoch 1258/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0400\nEpoch 1259/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0399\nEpoch 1260/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0399\nEpoch 1261/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0399\nEpoch 1262/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0399\nEpoch 1263/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0399\nEpoch 1264/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0398\nEpoch 1265/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0398\nEpoch 1266/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0398\nEpoch 1267/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0397\nEpoch 1268/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0397\nEpoch 1269/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0397\nEpoch 1270/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0396\nEpoch 1271/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0396\nEpoch 1272/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0396\nEpoch 1273/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0396\nEpoch 1274/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0395\nEpoch 1275/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0395\nEpoch 1276/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0395\nEpoch 1277/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0395\nEpoch 1278/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0395\nEpoch 1279/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1280/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1281/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1282/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1283/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1284/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1285/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1286/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1287/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1288/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1289/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1290/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1291/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1292/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1293/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1294/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1295/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1296/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1297/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1298/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1299/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1300/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1301/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1302/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0392\nEpoch 1303/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0392\nEpoch 1304/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0392\nEpoch 1305/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1306/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1307/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1308/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1309/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1310/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1311/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1312/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1313/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1314/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1315/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1316/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1317/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1318/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1319/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1320/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1321/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1322/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1323/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1324/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1325/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1326/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1327/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1328/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1329/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1330/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1331/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1332/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1333/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1334/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1335/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1336/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1337/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1338/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1339/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1340/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1341/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1342/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1343/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1344/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1345/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1346/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1347/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1348/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1349/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1350/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1351/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1352/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1353/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1354/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1355/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1356/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1357/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1358/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1359/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1360/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1361/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1362/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1363/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1364/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1365/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1366/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1367/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1368/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1369/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1370/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1371/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1372/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1373/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1374/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1375/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1376/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1377/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1378/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1379/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1380/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1381/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1382/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1383/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1384/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1385/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1386/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1387/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1388/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1389/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1390/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1391/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1392/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1393/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1394/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1395/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1396/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1397/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1398/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1399/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1400/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1401/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1402/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1403/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1404/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1405/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1406/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1407/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1408/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1409/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1410/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1411/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1412/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1413/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1414/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1415/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1416/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1417/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1418/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1419/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1420/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1421/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1422/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1423/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1424/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1425/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1426/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1427/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1428/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1429/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1430/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1431/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1432/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1433/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1434/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1435/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1436/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1437/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1438/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1439/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1440/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1441/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1442/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1443/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1444/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1445/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1446/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1447/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1448/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1449/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1450/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1451/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1452/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1453/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1454/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1455/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1456/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1457/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1458/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1459/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1460/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1461/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1462/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1463/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1464/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1465/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1466/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1467/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1468/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1469/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1470/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1471/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1472/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1473/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1474/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1475/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1476/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1477/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1478/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1479/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1480/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1481/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1482/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1483/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1484/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1485/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1486/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1487/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1488/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1489/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1490/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1491/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1492/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1493/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1494/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1495/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1496/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1497/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1498/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1499/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1500/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1501/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1502/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1503/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1504/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1505/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1506/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1507/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1508/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1509/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1510/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1511/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1512/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1513/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1514/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1515/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1516/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1517/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1518/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1519/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1520/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1521/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1522/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1523/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1524/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1525/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1526/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1527/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1528/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1529/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1530/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1531/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1532/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1533/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1534/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1535/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1536/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1537/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1538/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1539/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1540/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1541/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1542/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1543/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1544/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1545/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1546/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1547/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1548/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1549/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1550/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1551/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1552/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1553/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1554/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1555/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1556/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1557/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1558/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1559/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1560/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1561/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1562/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1563/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1564/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1565/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1566/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1567/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1568/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1569/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1570/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1571/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1572/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1573/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1574/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1575/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1576/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1577/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1578/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1579/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1580/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1581/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1582/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1583/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1584/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1585/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1586/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1587/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1588/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1589/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1590/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1591/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1592/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1593/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1594/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1595/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1596/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1597/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1598/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1599/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1600/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1601/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1602/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1603/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1604/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1605/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1606/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1607/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1608/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1609/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1610/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1611/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1612/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1613/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1614/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1615/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1616/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1617/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1618/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1619/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1620/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1621/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1622/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1623/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1624/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1625/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1626/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1627/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1628/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1629/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1630/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1631/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1632/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1633/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1634/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1635/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1636/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1637/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1638/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1639/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1640/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1641/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1642/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1643/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1644/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1645/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1646/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1647/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1648/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1649/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1650/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1651/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1652/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1653/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1654/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1655/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1656/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1657/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1658/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1659/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1660/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1661/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1662/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1663/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1664/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1665/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1666/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1667/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1668/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1669/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1670/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1671/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1672/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1673/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1674/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1675/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1676/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1677/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1678/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1679/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1680/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1681/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1682/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1683/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1684/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1685/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1686/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1687/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1688/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1689/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1690/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1691/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1692/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1693/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1694/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1695/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1696/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1697/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1698/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1699/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1700/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1701/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1702/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1703/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1704/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1705/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1706/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1707/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1708/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1709/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1710/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1711/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1712/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1713/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1714/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1715/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1716/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1717/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1718/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1719/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1720/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1721/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1722/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1723/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1724/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1725/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1726/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1727/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1728/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1729/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1730/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1731/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1732/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1733/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1734/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1735/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1736/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1737/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1738/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1739/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1740/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1741/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1742/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1743/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1744/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1745/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1746/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1747/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1748/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1749/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1750/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1751/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1752/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1753/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1754/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1755/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1756/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1757/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1758/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1759/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1760/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1761/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1762/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1763/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1764/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1765/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1766/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1767/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1768/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1769/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1770/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1771/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1772/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1773/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1774/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1775/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1776/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1777/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1778/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1779/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1780/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1781/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1782/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1783/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1784/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1785/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1786/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1787/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1788/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1789/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1790/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1791/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1792/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1793/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1794/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1795/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1796/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1797/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1798/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1799/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1800/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1801/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1802/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1803/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1804/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1805/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1806/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1807/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1808/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1809/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1810/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1811/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1812/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1813/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1814/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1815/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1816/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1817/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1818/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1819/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1820/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1821/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1822/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1823/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1824/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1825/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1826/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1827/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1828/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1829/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1830/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1831/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1832/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1833/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1834/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1835/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1836/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1837/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1838/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1839/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1840/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1841/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1842/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1843/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1844/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1845/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1846/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1847/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1848/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1849/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1850/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1851/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1852/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1853/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1854/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1855/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1856/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1857/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1858/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1859/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1860/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1861/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1862/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1863/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1864/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1865/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1866/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1867/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1868/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1869/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1870/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1871/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1872/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1873/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1874/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1875/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1876/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1877/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1878/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1879/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1880/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1881/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1882/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1883/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1884/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1885/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1886/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1887/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1888/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1889/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1890/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1891/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1892/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1893/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1894/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1895/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1896/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1897/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1898/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1899/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1900/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1901/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1902/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1903/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1904/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1905/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1906/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1907/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1908/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1909/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1910/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1911/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1912/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1913/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1914/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1915/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1916/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1917/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1918/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1919/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1920/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1921/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1922/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1923/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1924/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1925/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1926/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1927/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1928/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1929/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1930/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1931/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1932/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1933/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1934/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1935/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1936/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1937/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1938/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1939/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1940/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1941/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1942/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1943/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1944/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1945/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1946/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1947/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1948/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1949/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1950/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1951/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1952/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1953/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1954/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1955/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1956/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1957/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1958/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1959/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1960/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1961/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1962/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1963/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1964/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1965/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1966/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1967/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1968/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1969/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1970/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1971/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1972/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1973/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1974/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1975/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1976/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1977/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1978/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1979/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1980/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1981/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1982/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1983/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1984/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1985/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1986/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1987/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1988/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1989/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1990/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1991/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1992/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1993/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1994/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1995/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1996/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1997/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1998/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1999/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 2000/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\n\n\n&lt;keras.callbacks.History at 0x7f708353a650&gt;\n\n\n\nfig, ax = plt.subplots()\nax.plot(y,'.',alpha=0.2)\nax.plot(net(X),'--')\nwith tf.summary.create_file_writer(logdir).as_default():\n    tf.summary.image(\"적합결과시각화\", plot_to_image(fig), step=0)\n\n\n#\n#%tensorboard --logdir logs --host 0.0.0.0\n\n- 아래의 코드를 100에폭마다 실행하고 싶다.\nfig, ax = plt.subplots()\nax.plot(y,'.',alpha=0.2)\nax.plot(net(X),'--')\nwith tf.summary.create_file_writer(logdir).as_default():\n    tf.summary.image(\"적합결과시각화\", plot_to_image(fig), step=0)\n- 일단 net.fit직전까지의 코드를 구현\n\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(1))\nnet.compile(loss='mse',optimizer='adam')\n\n- 사용자정의 콜백클래스를 만듬\n\nclass PlotYhat(tf.keras.callbacks.Callback):\n    def on_epoch_begin(self,epoch,logs): # 입력은 무조건 self, epoch, logs를 써야합니다 --&gt; 이 함수안에 에폭이 끝날때마다 할 동작을 정의한다.\n        if epoch % 100 ==0:\n            fig, ax = plt.subplots()\n            ax.plot(y,'.',alpha=0.2)\n            ax.plot(net(X),'--')\n            with tf.summary.create_file_writer(logdir).as_default():\n                tf.summary.image(\"적합결과시각화\"+str(epoch), plot_to_image(fig), step=0)\n\n- 내가 만든 클래스에서 cb2를 생성\n\n#collapse_output\n!rm -rf logs\ncb1= tf.keras.callbacks.TensorBoard(update_freq='epoch',histogram_freq=100)\ncb2= PlotYhat()\nnet.fit(X,y,epochs=2000, batch_size=100, validation_split=0.45,callbacks=[cb1,cb2])\n\nEpoch 1/2000\n1/6 [====&gt;.........................] - ETA: 0s - loss: 2.9239WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0007s vs `on_train_batch_end` time: 0.0019s). Check your callbacks.\n6/6 [==============================] - 0s 6ms/step - loss: 2.8708 - val_loss: 9.1608\nEpoch 2/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.8444 - val_loss: 9.1250\nEpoch 3/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.8186 - val_loss: 9.0901\nEpoch 4/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.7930 - val_loss: 9.0545\nEpoch 5/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.7673 - val_loss: 9.0188\nEpoch 6/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.7417 - val_loss: 8.9828\nEpoch 7/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.7169 - val_loss: 8.9476\nEpoch 8/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.6913 - val_loss: 8.9134\nEpoch 9/2000\n6/6 [==============================] - 0s 4ms/step - loss: 2.6667 - val_loss: 8.8786\nEpoch 10/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.6420 - val_loss: 8.8450\nEpoch 11/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.6175 - val_loss: 8.8107\nEpoch 12/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.5932 - val_loss: 8.7766\nEpoch 13/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.5690 - val_loss: 8.7433\nEpoch 14/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.5453 - val_loss: 8.7103\nEpoch 15/2000\n6/6 [==============================] - 0s 4ms/step - loss: 2.5214 - val_loss: 8.6774\nEpoch 16/2000\n6/6 [==============================] - 0s 4ms/step - loss: 2.4980 - val_loss: 8.6453\nEpoch 17/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.4749 - val_loss: 8.6120\nEpoch 18/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.4520 - val_loss: 8.5786\nEpoch 19/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.4293 - val_loss: 8.5454\nEpoch 20/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.4065 - val_loss: 8.5130\nEpoch 21/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.3840 - val_loss: 8.4813\nEpoch 22/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.3618 - val_loss: 8.4501\nEpoch 23/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.3398 - val_loss: 8.4176\nEpoch 24/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.3182 - val_loss: 8.3857\nEpoch 25/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.2960 - val_loss: 8.3532\nEpoch 26/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.2748 - val_loss: 8.3213\nEpoch 27/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.2534 - val_loss: 8.2902\nEpoch 28/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.2321 - val_loss: 8.2592\nEpoch 29/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.2113 - val_loss: 8.2272\nEpoch 30/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.1904 - val_loss: 8.1962\nEpoch 31/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.1696 - val_loss: 8.1648\nEpoch 32/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.1493 - val_loss: 8.1346\nEpoch 33/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.1288 - val_loss: 8.1042\nEpoch 34/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.1088 - val_loss: 8.0739\nEpoch 35/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.0887 - val_loss: 8.0442\nEpoch 36/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.0688 - val_loss: 8.0146\nEpoch 37/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.0491 - val_loss: 7.9854\nEpoch 38/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.0297 - val_loss: 7.9553\nEpoch 39/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.0102 - val_loss: 7.9261\nEpoch 40/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.9913 - val_loss: 7.8968\nEpoch 41/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.9720 - val_loss: 7.8684\nEpoch 42/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.9533 - val_loss: 7.8398\nEpoch 43/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.9346 - val_loss: 7.8111\nEpoch 44/2000\n6/6 [==============================] - 0s 2ms/step - loss: 1.9162 - val_loss: 7.7827\nEpoch 45/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.8976 - val_loss: 7.7536\nEpoch 46/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.8796 - val_loss: 7.7262\nEpoch 47/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.8613 - val_loss: 7.6986\nEpoch 48/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.8437 - val_loss: 7.6714\nEpoch 49/2000\n6/6 [==============================] - 0s 4ms/step - loss: 1.8257 - val_loss: 7.6441\nEpoch 50/2000\n6/6 [==============================] - 0s 4ms/step - loss: 1.8083 - val_loss: 7.6171\nEpoch 51/2000\n6/6 [==============================] - 0s 4ms/step - loss: 1.7911 - val_loss: 7.5898\nEpoch 52/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.7739 - val_loss: 7.5620\nEpoch 53/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.7568 - val_loss: 7.5342\nEpoch 54/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.7399 - val_loss: 7.5075\nEpoch 55/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.7231 - val_loss: 7.4809\nEpoch 56/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.7066 - val_loss: 7.4545\nEpoch 57/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.6898 - val_loss: 7.4279\nEpoch 58/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.6734 - val_loss: 7.4019\nEpoch 59/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.6573 - val_loss: 7.3760\nEpoch 60/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.6410 - val_loss: 7.3502\nEpoch 61/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.6251 - val_loss: 7.3247\nEpoch 62/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.6095 - val_loss: 7.2996\nEpoch 63/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.5937 - val_loss: 7.2744\nEpoch 64/2000\n6/6 [==============================] - 0s 4ms/step - loss: 1.5782 - val_loss: 7.2490\nEpoch 65/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.5626 - val_loss: 7.2237\nEpoch 66/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.5476 - val_loss: 7.1979\nEpoch 67/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.5324 - val_loss: 7.1726\nEpoch 68/2000\n6/6 [==============================] - 0s 4ms/step - loss: 1.5175 - val_loss: 7.1478\nEpoch 69/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.5027 - val_loss: 7.1222\nEpoch 70/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.4878 - val_loss: 7.0974\nEpoch 71/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.4732 - val_loss: 7.0722\nEpoch 72/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.4588 - val_loss: 7.0463\nEpoch 73/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.4445 - val_loss: 7.0214\nEpoch 74/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.4301 - val_loss: 6.9963\nEpoch 75/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.4162 - val_loss: 6.9712\nEpoch 76/2000\n6/6 [==============================] - 0s 4ms/step - loss: 1.4024 - val_loss: 6.9467\nEpoch 77/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.3885 - val_loss: 6.9229\nEpoch 78/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.3749 - val_loss: 6.8992\nEpoch 79/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.3612 - val_loss: 6.8755\nEpoch 80/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.3479 - val_loss: 6.8522\nEpoch 81/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.3345 - val_loss: 6.8282\nEpoch 82/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.3213 - val_loss: 6.8048\nEpoch 83/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.3083 - val_loss: 6.7817\nEpoch 84/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.2955 - val_loss: 6.7583\nEpoch 85/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.2826 - val_loss: 6.7351\nEpoch 86/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.2700 - val_loss: 6.7123\nEpoch 87/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.2574 - val_loss: 6.6893\nEpoch 88/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.2449 - val_loss: 6.6665\nEpoch 89/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.2327 - val_loss: 6.6433\nEpoch 90/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.2205 - val_loss: 6.6207\nEpoch 91/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.2085 - val_loss: 6.5983\nEpoch 92/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.1964 - val_loss: 6.5757\nEpoch 93/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.1844 - val_loss: 6.5534\nEpoch 94/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.1727 - val_loss: 6.5304\nEpoch 95/2000\n6/6 [==============================] - 0s 4ms/step - loss: 1.1610 - val_loss: 6.5080\nEpoch 96/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.1494 - val_loss: 6.4857\nEpoch 97/2000\n6/6 [==============================] - 0s 4ms/step - loss: 1.1381 - val_loss: 6.4647\nEpoch 98/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.1267 - val_loss: 6.4424\nEpoch 99/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.1155 - val_loss: 6.4206\nEpoch 100/2000\n6/6 [==============================] - 0s 4ms/step - loss: 1.1044 - val_loss: 6.3984\nEpoch 101/2000\n6/6 [==============================] - 0s 4ms/step - loss: 1.0934 - val_loss: 6.3769\nEpoch 102/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0824 - val_loss: 6.3556\nEpoch 103/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0716 - val_loss: 6.3349\nEpoch 104/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0609 - val_loss: 6.3136\nEpoch 105/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0506 - val_loss: 6.2930\nEpoch 106/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0399 - val_loss: 6.2721\nEpoch 107/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0296 - val_loss: 6.2521\nEpoch 108/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0194 - val_loss: 6.2312\nEpoch 109/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0093 - val_loss: 6.2111\nEpoch 110/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9992 - val_loss: 6.1916\nEpoch 111/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9893 - val_loss: 6.1708\nEpoch 112/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.9792 - val_loss: 6.1501\nEpoch 113/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9696 - val_loss: 6.1293\nEpoch 114/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9600 - val_loss: 6.1098\nEpoch 115/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9503 - val_loss: 6.0898\nEpoch 116/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9407 - val_loss: 6.0703\nEpoch 117/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9313 - val_loss: 6.0498\nEpoch 118/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9219 - val_loss: 6.0300\nEpoch 119/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9129 - val_loss: 6.0099\nEpoch 120/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9036 - val_loss: 5.9900\nEpoch 121/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.8947 - val_loss: 5.9708\nEpoch 122/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8857 - val_loss: 5.9519\nEpoch 123/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8769 - val_loss: 5.9320\nEpoch 124/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8682 - val_loss: 5.9135\nEpoch 125/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8595 - val_loss: 5.8947\nEpoch 126/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8508 - val_loss: 5.8760\nEpoch 127/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8425 - val_loss: 5.8576\nEpoch 128/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.8340 - val_loss: 5.8392\nEpoch 129/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8256 - val_loss: 5.8206\nEpoch 130/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8175 - val_loss: 5.8018\nEpoch 131/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8094 - val_loss: 5.7832\nEpoch 132/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8012 - val_loss: 5.7644\nEpoch 133/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7932 - val_loss: 5.7460\nEpoch 134/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7853 - val_loss: 5.7276\nEpoch 135/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7774 - val_loss: 5.7094\nEpoch 136/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7698 - val_loss: 5.6909\nEpoch 137/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7620 - val_loss: 5.6725\nEpoch 138/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7544 - val_loss: 5.6538\nEpoch 139/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7470 - val_loss: 5.6356\nEpoch 140/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7397 - val_loss: 5.6178\nEpoch 141/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7322 - val_loss: 5.6006\nEpoch 142/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7251 - val_loss: 5.5828\nEpoch 143/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7178 - val_loss: 5.5649\nEpoch 144/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7107 - val_loss: 5.5471\nEpoch 145/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7036 - val_loss: 5.5300\nEpoch 146/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.6967 - val_loss: 5.5130\nEpoch 147/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.6900 - val_loss: 5.4949\nEpoch 148/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.6831 - val_loss: 5.4773\nEpoch 149/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6765 - val_loss: 5.4599\nEpoch 150/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6698 - val_loss: 5.4428\nEpoch 151/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6633 - val_loss: 5.4258\nEpoch 152/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6568 - val_loss: 5.4088\nEpoch 153/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6504 - val_loss: 5.3916\nEpoch 154/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6440 - val_loss: 5.3746\nEpoch 155/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6376 - val_loss: 5.3580\nEpoch 156/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6313 - val_loss: 5.3409\nEpoch 157/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6252 - val_loss: 5.3243\nEpoch 158/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6191 - val_loss: 5.3078\nEpoch 159/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6132 - val_loss: 5.2917\nEpoch 160/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6071 - val_loss: 5.2757\nEpoch 161/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6012 - val_loss: 5.2593\nEpoch 162/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5954 - val_loss: 5.2427\nEpoch 163/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5896 - val_loss: 5.2265\nEpoch 164/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5839 - val_loss: 5.2100\nEpoch 165/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5783 - val_loss: 5.1937\nEpoch 166/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5726 - val_loss: 5.1774\nEpoch 167/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5672 - val_loss: 5.1611\nEpoch 168/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.5616 - val_loss: 5.1447\nEpoch 169/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5562 - val_loss: 5.1285\nEpoch 170/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5508 - val_loss: 5.1128\nEpoch 171/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5455 - val_loss: 5.0969\nEpoch 172/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5403 - val_loss: 5.0809\nEpoch 173/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5351 - val_loss: 5.0651\nEpoch 174/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5299 - val_loss: 5.0495\nEpoch 175/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5248 - val_loss: 5.0335\nEpoch 176/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5199 - val_loss: 5.0180\nEpoch 177/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5150 - val_loss: 5.0023\nEpoch 178/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5100 - val_loss: 4.9867\nEpoch 179/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5052 - val_loss: 4.9713\nEpoch 180/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5005 - val_loss: 4.9561\nEpoch 181/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4957 - val_loss: 4.9413\nEpoch 182/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4911 - val_loss: 4.9263\nEpoch 183/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4864 - val_loss: 4.9110\nEpoch 184/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4818 - val_loss: 4.8960\nEpoch 185/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4773 - val_loss: 4.8807\nEpoch 186/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4727 - val_loss: 4.8652\nEpoch 187/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.4684 - val_loss: 4.8500\nEpoch 188/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4640 - val_loss: 4.8352\nEpoch 189/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4597 - val_loss: 4.8199\nEpoch 190/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.4554 - val_loss: 4.8049\nEpoch 191/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4511 - val_loss: 4.7902\nEpoch 192/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4470 - val_loss: 4.7757\nEpoch 193/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4430 - val_loss: 4.7610\nEpoch 194/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4388 - val_loss: 4.7460\nEpoch 195/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4348 - val_loss: 4.7315\nEpoch 196/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4308 - val_loss: 4.7174\nEpoch 197/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4270 - val_loss: 4.7021\nEpoch 198/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4231 - val_loss: 4.6869\nEpoch 199/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4192 - val_loss: 4.6721\nEpoch 200/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.4154 - val_loss: 4.6581\nEpoch 201/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.4117 - val_loss: 4.6438\nEpoch 202/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4080 - val_loss: 4.6298\nEpoch 203/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4042 - val_loss: 4.6157\nEpoch 204/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4007 - val_loss: 4.6012\nEpoch 205/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3971 - val_loss: 4.5873\nEpoch 206/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3935 - val_loss: 4.5730\nEpoch 207/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3900 - val_loss: 4.5583\nEpoch 208/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3865 - val_loss: 4.5440\nEpoch 209/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3830 - val_loss: 4.5298\nEpoch 210/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3796 - val_loss: 4.5153\nEpoch 211/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3763 - val_loss: 4.5010\nEpoch 212/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.3730 - val_loss: 4.4872\nEpoch 213/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3697 - val_loss: 4.4732\nEpoch 214/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3665 - val_loss: 4.4590\nEpoch 215/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3633 - val_loss: 4.4456\nEpoch 216/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3602 - val_loss: 4.4320\nEpoch 217/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3571 - val_loss: 4.4182\nEpoch 218/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3540 - val_loss: 4.4044\nEpoch 219/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3510 - val_loss: 4.3908\nEpoch 220/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3480 - val_loss: 4.3772\nEpoch 221/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3451 - val_loss: 4.3643\nEpoch 222/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3422 - val_loss: 4.3514\nEpoch 223/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3392 - val_loss: 4.3379\nEpoch 224/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3364 - val_loss: 4.3246\nEpoch 225/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3336 - val_loss: 4.3112\nEpoch 226/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3309 - val_loss: 4.2979\nEpoch 227/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3281 - val_loss: 4.2849\nEpoch 228/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3255 - val_loss: 4.2719\nEpoch 229/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3228 - val_loss: 4.2590\nEpoch 230/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3202 - val_loss: 4.2463\nEpoch 231/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3176 - val_loss: 4.2340\nEpoch 232/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3150 - val_loss: 4.2210\nEpoch 233/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3125 - val_loss: 4.2083\nEpoch 234/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3100 - val_loss: 4.1954\nEpoch 235/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3075 - val_loss: 4.1827\nEpoch 236/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3051 - val_loss: 4.1695\nEpoch 237/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.3026 - val_loss: 4.1566\nEpoch 238/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3002 - val_loss: 4.1435\nEpoch 239/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2978 - val_loss: 4.1301\nEpoch 240/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2955 - val_loss: 4.1168\nEpoch 241/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2931 - val_loss: 4.1040\nEpoch 242/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2908 - val_loss: 4.0912\nEpoch 243/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2886 - val_loss: 4.0783\nEpoch 244/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2864 - val_loss: 4.0650\nEpoch 245/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2842 - val_loss: 4.0519\nEpoch 246/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2820 - val_loss: 4.0394\nEpoch 247/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2799 - val_loss: 4.0260\nEpoch 248/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2778 - val_loss: 4.0132\nEpoch 249/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2756 - val_loss: 4.0004\nEpoch 250/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2736 - val_loss: 3.9875\nEpoch 251/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2716 - val_loss: 3.9748\nEpoch 252/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2695 - val_loss: 3.9623\nEpoch 253/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2675 - val_loss: 3.9500\nEpoch 254/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2656 - val_loss: 3.9379\nEpoch 255/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2637 - val_loss: 3.9257\nEpoch 256/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2618 - val_loss: 3.9131\nEpoch 257/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2598 - val_loss: 3.9008\nEpoch 258/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2581 - val_loss: 3.8884\nEpoch 259/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2562 - val_loss: 3.8763\nEpoch 260/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2544 - val_loss: 3.8639\nEpoch 261/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2526 - val_loss: 3.8518\nEpoch 262/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2508 - val_loss: 3.8400\nEpoch 263/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2491 - val_loss: 3.8280\nEpoch 264/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2474 - val_loss: 3.8163\nEpoch 265/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2457 - val_loss: 3.8043\nEpoch 266/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2440 - val_loss: 3.7922\nEpoch 267/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2424 - val_loss: 3.7796\nEpoch 268/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2407 - val_loss: 3.7679\nEpoch 269/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2391 - val_loss: 3.7561\nEpoch 270/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2375 - val_loss: 3.7440\nEpoch 271/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2358 - val_loss: 3.7320\nEpoch 272/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2342 - val_loss: 3.7200\nEpoch 273/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2327 - val_loss: 3.7085\nEpoch 274/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2311 - val_loss: 3.6968\nEpoch 275/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2296 - val_loss: 3.6854\nEpoch 276/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2282 - val_loss: 3.6738\nEpoch 277/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2267 - val_loss: 3.6621\nEpoch 278/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2252 - val_loss: 3.6506\nEpoch 279/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2238 - val_loss: 3.6389\nEpoch 280/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2225 - val_loss: 3.6272\nEpoch 281/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2210 - val_loss: 3.6159\nEpoch 282/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2197 - val_loss: 3.6041\nEpoch 283/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2184 - val_loss: 3.5925\nEpoch 284/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2170 - val_loss: 3.5810\nEpoch 285/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2157 - val_loss: 3.5693\nEpoch 286/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2144 - val_loss: 3.5573\nEpoch 287/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2132 - val_loss: 3.5455\nEpoch 288/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2118 - val_loss: 3.5341\nEpoch 289/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2106 - val_loss: 3.5220\nEpoch 290/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2093 - val_loss: 3.5100\nEpoch 291/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2080 - val_loss: 3.4983\nEpoch 292/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2068 - val_loss: 3.4868\nEpoch 293/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2056 - val_loss: 3.4754\nEpoch 294/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2044 - val_loss: 3.4642\nEpoch 295/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2033 - val_loss: 3.4525\nEpoch 296/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2021 - val_loss: 3.4414\nEpoch 297/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2010 - val_loss: 3.4307\nEpoch 298/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1998 - val_loss: 3.4204\nEpoch 299/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1987 - val_loss: 3.4090\nEpoch 300/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1976 - val_loss: 3.3979\nEpoch 301/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1966 - val_loss: 3.3865\nEpoch 302/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1955 - val_loss: 3.3759\nEpoch 303/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1944 - val_loss: 3.3648\nEpoch 304/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1934 - val_loss: 3.3537\nEpoch 305/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1923 - val_loss: 3.3431\nEpoch 306/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1913 - val_loss: 3.3316\nEpoch 307/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1903 - val_loss: 3.3203\nEpoch 308/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1893 - val_loss: 3.3087\nEpoch 309/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1882 - val_loss: 3.2972\nEpoch 310/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1873 - val_loss: 3.2862\nEpoch 311/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1864 - val_loss: 3.2753\nEpoch 312/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1854 - val_loss: 3.2644\nEpoch 313/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1844 - val_loss: 3.2537\nEpoch 314/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1836 - val_loss: 3.2427\nEpoch 315/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1826 - val_loss: 3.2318\nEpoch 316/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1817 - val_loss: 3.2210\nEpoch 317/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1808 - val_loss: 3.2101\nEpoch 318/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1799 - val_loss: 3.1994\nEpoch 319/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1790 - val_loss: 3.1886\nEpoch 320/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1781 - val_loss: 3.1771\nEpoch 321/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1772 - val_loss: 3.1665\nEpoch 322/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1764 - val_loss: 3.1559\nEpoch 323/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1756 - val_loss: 3.1453\nEpoch 324/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1748 - val_loss: 3.1339\nEpoch 325/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1739 - val_loss: 3.1228\nEpoch 326/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1731 - val_loss: 3.1123\nEpoch 327/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1723 - val_loss: 3.1017\nEpoch 328/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1716 - val_loss: 3.0913\nEpoch 329/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1708 - val_loss: 3.0808\nEpoch 330/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1700 - val_loss: 3.0705\nEpoch 331/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1692 - val_loss: 3.0602\nEpoch 332/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1685 - val_loss: 3.0498\nEpoch 333/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1677 - val_loss: 3.0388\nEpoch 334/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1670 - val_loss: 3.0285\nEpoch 335/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1662 - val_loss: 3.0177\nEpoch 336/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1655 - val_loss: 3.0074\nEpoch 337/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1648 - val_loss: 2.9975\nEpoch 338/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1641 - val_loss: 2.9873\nEpoch 339/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1634 - val_loss: 2.9772\nEpoch 340/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1627 - val_loss: 2.9667\nEpoch 341/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1620 - val_loss: 2.9566\nEpoch 342/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1613 - val_loss: 2.9468\nEpoch 343/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1607 - val_loss: 2.9365\nEpoch 344/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1599 - val_loss: 2.9265\nEpoch 345/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1593 - val_loss: 2.9164\nEpoch 346/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1586 - val_loss: 2.9060\nEpoch 347/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1580 - val_loss: 2.8955\nEpoch 348/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1573 - val_loss: 2.8854\nEpoch 349/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1567 - val_loss: 2.8753\nEpoch 350/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1560 - val_loss: 2.8653\nEpoch 351/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1554 - val_loss: 2.8548\nEpoch 352/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1548 - val_loss: 2.8449\nEpoch 353/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1542 - val_loss: 2.8345\nEpoch 354/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1536 - val_loss: 2.8244\nEpoch 355/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1530 - val_loss: 2.8139\nEpoch 356/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1524 - val_loss: 2.8030\nEpoch 357/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1517 - val_loss: 2.7928\nEpoch 358/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1512 - val_loss: 2.7827\nEpoch 359/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1505 - val_loss: 2.7725\nEpoch 360/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1500 - val_loss: 2.7618\nEpoch 361/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1494 - val_loss: 2.7521\nEpoch 362/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1488 - val_loss: 2.7423\nEpoch 363/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1483 - val_loss: 2.7323\nEpoch 364/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1477 - val_loss: 2.7222\nEpoch 365/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1471 - val_loss: 2.7124\nEpoch 366/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1466 - val_loss: 2.7025\nEpoch 367/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1460 - val_loss: 2.6925\nEpoch 368/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1455 - val_loss: 2.6823\nEpoch 369/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1449 - val_loss: 2.6724\nEpoch 370/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1444 - val_loss: 2.6625\nEpoch 371/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1438 - val_loss: 2.6529\nEpoch 372/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1433 - val_loss: 2.6425\nEpoch 373/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1428 - val_loss: 2.6325\nEpoch 374/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1422 - val_loss: 2.6229\nEpoch 375/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1417 - val_loss: 2.6133\nEpoch 376/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1412 - val_loss: 2.6035\nEpoch 377/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1407 - val_loss: 2.5939\nEpoch 378/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1402 - val_loss: 2.5842\nEpoch 379/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1397 - val_loss: 2.5744\nEpoch 380/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1392 - val_loss: 2.5642\nEpoch 381/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1387 - val_loss: 2.5543\nEpoch 382/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1382 - val_loss: 2.5446\nEpoch 383/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1378 - val_loss: 2.5352\nEpoch 384/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1372 - val_loss: 2.5256\nEpoch 385/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1368 - val_loss: 2.5152\nEpoch 386/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1363 - val_loss: 2.5051\nEpoch 387/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1358 - val_loss: 2.4956\nEpoch 388/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1353 - val_loss: 2.4856\nEpoch 389/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1348 - val_loss: 2.4758\nEpoch 390/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1344 - val_loss: 2.4662\nEpoch 391/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1339 - val_loss: 2.4572\nEpoch 392/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1335 - val_loss: 2.4477\nEpoch 393/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1330 - val_loss: 2.4390\nEpoch 394/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1325 - val_loss: 2.4299\nEpoch 395/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1321 - val_loss: 2.4209\nEpoch 396/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1316 - val_loss: 2.4115\nEpoch 397/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1312 - val_loss: 2.4028\nEpoch 398/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1307 - val_loss: 2.3940\nEpoch 399/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1303 - val_loss: 2.3844\nEpoch 400/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1299 - val_loss: 2.3751\nEpoch 401/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1295 - val_loss: 2.3659\nEpoch 402/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1290 - val_loss: 2.3568\nEpoch 403/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1286 - val_loss: 2.3472\nEpoch 404/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1282 - val_loss: 2.3378\nEpoch 405/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1277 - val_loss: 2.3288\nEpoch 406/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1273 - val_loss: 2.3193\nEpoch 407/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1269 - val_loss: 2.3105\nEpoch 408/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1265 - val_loss: 2.3011\nEpoch 409/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1261 - val_loss: 2.2917\nEpoch 410/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1256 - val_loss: 2.2824\nEpoch 411/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1252 - val_loss: 2.2731\nEpoch 412/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1248 - val_loss: 2.2642\nEpoch 413/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1244 - val_loss: 2.2553\nEpoch 414/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1240 - val_loss: 2.2461\nEpoch 415/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1236 - val_loss: 2.2373\nEpoch 416/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1232 - val_loss: 2.2284\nEpoch 417/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1228 - val_loss: 2.2194\nEpoch 418/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1224 - val_loss: 2.2105\nEpoch 419/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1220 - val_loss: 2.2017\nEpoch 420/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1216 - val_loss: 2.1927\nEpoch 421/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1212 - val_loss: 2.1841\nEpoch 422/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1208 - val_loss: 2.1749\nEpoch 423/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1205 - val_loss: 2.1657\nEpoch 424/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1200 - val_loss: 2.1567\nEpoch 425/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1196 - val_loss: 2.1476\nEpoch 426/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1192 - val_loss: 2.1388\nEpoch 427/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1189 - val_loss: 2.1294\nEpoch 428/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1185 - val_loss: 2.1209\nEpoch 429/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1181 - val_loss: 2.1120\nEpoch 430/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1177 - val_loss: 2.1027\nEpoch 431/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1173 - val_loss: 2.0937\nEpoch 432/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1170 - val_loss: 2.0851\nEpoch 433/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1166 - val_loss: 2.0769\nEpoch 434/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1162 - val_loss: 2.0680\nEpoch 435/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1158 - val_loss: 2.0597\nEpoch 436/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1154 - val_loss: 2.0511\nEpoch 437/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1151 - val_loss: 2.0421\nEpoch 438/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1147 - val_loss: 2.0333\nEpoch 439/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1143 - val_loss: 2.0251\nEpoch 440/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1140 - val_loss: 2.0163\nEpoch 441/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1136 - val_loss: 2.0074\nEpoch 442/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1133 - val_loss: 1.9985\nEpoch 443/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1129 - val_loss: 1.9900\nEpoch 444/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1125 - val_loss: 1.9811\nEpoch 445/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1121 - val_loss: 1.9727\nEpoch 446/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1118 - val_loss: 1.9641\nEpoch 447/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1114 - val_loss: 1.9554\nEpoch 448/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1111 - val_loss: 1.9468\nEpoch 449/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1107 - val_loss: 1.9385\nEpoch 450/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1104 - val_loss: 1.9299\nEpoch 451/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1100 - val_loss: 1.9216\nEpoch 452/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1097 - val_loss: 1.9130\nEpoch 453/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1093 - val_loss: 1.9047\nEpoch 454/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1090 - val_loss: 1.8963\nEpoch 455/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1086 - val_loss: 1.8880\nEpoch 456/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1083 - val_loss: 1.8797\nEpoch 457/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1079 - val_loss: 1.8712\nEpoch 458/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1076 - val_loss: 1.8626\nEpoch 459/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1072 - val_loss: 1.8541\nEpoch 460/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1068 - val_loss: 1.8456\nEpoch 461/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1065 - val_loss: 1.8371\nEpoch 462/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1062 - val_loss: 1.8291\nEpoch 463/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1058 - val_loss: 1.8207\nEpoch 464/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1055 - val_loss: 1.8123\nEpoch 465/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1051 - val_loss: 1.8044\nEpoch 466/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1048 - val_loss: 1.7963\nEpoch 467/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1044 - val_loss: 1.7885\nEpoch 468/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1041 - val_loss: 1.7803\nEpoch 469/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1038 - val_loss: 1.7722\nEpoch 470/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1034 - val_loss: 1.7646\nEpoch 471/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1031 - val_loss: 1.7565\nEpoch 472/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1028 - val_loss: 1.7484\nEpoch 473/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1025 - val_loss: 1.7402\nEpoch 474/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1021 - val_loss: 1.7330\nEpoch 475/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1018 - val_loss: 1.7250\nEpoch 476/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1015 - val_loss: 1.7168\nEpoch 477/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1012 - val_loss: 1.7089\nEpoch 478/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1008 - val_loss: 1.7008\nEpoch 479/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1005 - val_loss: 1.6927\nEpoch 480/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1002 - val_loss: 1.6846\nEpoch 481/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0999 - val_loss: 1.6768\nEpoch 482/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0996 - val_loss: 1.6686\nEpoch 483/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0992 - val_loss: 1.6610\nEpoch 484/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0989 - val_loss: 1.6530\nEpoch 485/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0986 - val_loss: 1.6448\nEpoch 486/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0983 - val_loss: 1.6373\nEpoch 487/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0980 - val_loss: 1.6292\nEpoch 488/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0976 - val_loss: 1.6219\nEpoch 489/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0973 - val_loss: 1.6139\nEpoch 490/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0970 - val_loss: 1.6062\nEpoch 491/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0967 - val_loss: 1.5984\nEpoch 492/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0964 - val_loss: 1.5904\nEpoch 493/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0961 - val_loss: 1.5827\nEpoch 494/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0957 - val_loss: 1.5750\nEpoch 495/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0954 - val_loss: 1.5672\nEpoch 496/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0951 - val_loss: 1.5593\nEpoch 497/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0948 - val_loss: 1.5515\nEpoch 498/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0945 - val_loss: 1.5438\nEpoch 499/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0942 - val_loss: 1.5357\nEpoch 500/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0938 - val_loss: 1.5281\nEpoch 501/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0935 - val_loss: 1.5207\nEpoch 502/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0932 - val_loss: 1.5133\nEpoch 503/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0929 - val_loss: 1.5063\nEpoch 504/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0926 - val_loss: 1.4992\nEpoch 505/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0923 - val_loss: 1.4920\nEpoch 506/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0920 - val_loss: 1.4839\nEpoch 507/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0917 - val_loss: 1.4763\nEpoch 508/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0914 - val_loss: 1.4686\nEpoch 509/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0911 - val_loss: 1.4608\nEpoch 510/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0908 - val_loss: 1.4534\nEpoch 511/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0905 - val_loss: 1.4456\nEpoch 512/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0902 - val_loss: 1.4380\nEpoch 513/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0899 - val_loss: 1.4307\nEpoch 514/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0896 - val_loss: 1.4231\nEpoch 515/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0893 - val_loss: 1.4158\nEpoch 516/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0890 - val_loss: 1.4086\nEpoch 517/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0887 - val_loss: 1.4011\nEpoch 518/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0884 - val_loss: 1.3936\nEpoch 519/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0881 - val_loss: 1.3862\nEpoch 520/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0878 - val_loss: 1.3786\nEpoch 521/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0875 - val_loss: 1.3714\nEpoch 522/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0872 - val_loss: 1.3644\nEpoch 523/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0869 - val_loss: 1.3574\nEpoch 524/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0866 - val_loss: 1.3505\nEpoch 525/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0864 - val_loss: 1.3437\nEpoch 526/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0861 - val_loss: 1.3364\nEpoch 527/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0858 - val_loss: 1.3296\nEpoch 528/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0855 - val_loss: 1.3227\nEpoch 529/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0852 - val_loss: 1.3158\nEpoch 530/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0849 - val_loss: 1.3094\nEpoch 531/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0847 - val_loss: 1.3025\nEpoch 532/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0844 - val_loss: 1.2952\nEpoch 533/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0841 - val_loss: 1.2883\nEpoch 534/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0838 - val_loss: 1.2809\nEpoch 535/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0835 - val_loss: 1.2734\nEpoch 536/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0832 - val_loss: 1.2665\nEpoch 537/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0830 - val_loss: 1.2597\nEpoch 538/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0827 - val_loss: 1.2527\nEpoch 539/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0824 - val_loss: 1.2453\nEpoch 540/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0821 - val_loss: 1.2380\nEpoch 541/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0818 - val_loss: 1.2310\nEpoch 542/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0815 - val_loss: 1.2237\nEpoch 543/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0812 - val_loss: 1.2171\nEpoch 544/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0810 - val_loss: 1.2103\nEpoch 545/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0807 - val_loss: 1.2032\nEpoch 546/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0804 - val_loss: 1.1967\nEpoch 547/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0801 - val_loss: 1.1904\nEpoch 548/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0798 - val_loss: 1.1842\nEpoch 549/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0796 - val_loss: 1.1775\nEpoch 550/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0793 - val_loss: 1.1708\nEpoch 551/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0790 - val_loss: 1.1644\nEpoch 552/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0788 - val_loss: 1.1575\nEpoch 553/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0785 - val_loss: 1.1506\nEpoch 554/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0782 - val_loss: 1.1441\nEpoch 555/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0779 - val_loss: 1.1371\nEpoch 556/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0777 - val_loss: 1.1302\nEpoch 557/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0774 - val_loss: 1.1237\nEpoch 558/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0771 - val_loss: 1.1172\nEpoch 559/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0769 - val_loss: 1.1104\nEpoch 560/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0766 - val_loss: 1.1040\nEpoch 561/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0763 - val_loss: 1.0975\nEpoch 562/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0761 - val_loss: 1.0910\nEpoch 563/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0758 - val_loss: 1.0850\nEpoch 564/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0756 - val_loss: 1.0784\nEpoch 565/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0753 - val_loss: 1.0720\nEpoch 566/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0751 - val_loss: 1.0654\nEpoch 567/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0748 - val_loss: 1.0591\nEpoch 568/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0746 - val_loss: 1.0525\nEpoch 569/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0743 - val_loss: 1.0462\nEpoch 570/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0741 - val_loss: 1.0400\nEpoch 571/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0738 - val_loss: 1.0336\nEpoch 572/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0736 - val_loss: 1.0274\nEpoch 573/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0733 - val_loss: 1.0215\nEpoch 574/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0731 - val_loss: 1.0155\nEpoch 575/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0728 - val_loss: 1.0095\nEpoch 576/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0726 - val_loss: 1.0033\nEpoch 577/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0723 - val_loss: 0.9969\nEpoch 578/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0721 - val_loss: 0.9911\nEpoch 579/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0719 - val_loss: 0.9855\nEpoch 580/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0716 - val_loss: 0.9800\nEpoch 581/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0714 - val_loss: 0.9744\nEpoch 582/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0711 - val_loss: 0.9686\nEpoch 583/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0709 - val_loss: 0.9626\nEpoch 584/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0707 - val_loss: 0.9567\nEpoch 585/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0704 - val_loss: 0.9504\nEpoch 586/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0702 - val_loss: 0.9447\nEpoch 587/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0699 - val_loss: 0.9390\nEpoch 588/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0697 - val_loss: 0.9330\nEpoch 589/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0695 - val_loss: 0.9273\nEpoch 590/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0692 - val_loss: 0.9217\nEpoch 591/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0690 - val_loss: 0.9159\nEpoch 592/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0688 - val_loss: 0.9097\nEpoch 593/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0685 - val_loss: 0.9039\nEpoch 594/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0683 - val_loss: 0.8983\nEpoch 595/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0681 - val_loss: 0.8930\nEpoch 596/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0678 - val_loss: 0.8872\nEpoch 597/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0676 - val_loss: 0.8821\nEpoch 598/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0674 - val_loss: 0.8764\nEpoch 599/2000\n6/6 [==============================] - 0s 2ms/step - loss: 0.0672 - val_loss: 0.8709\nEpoch 600/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0669 - val_loss: 0.8655\nEpoch 601/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0667 - val_loss: 0.8599\nEpoch 602/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0665 - val_loss: 0.8545\nEpoch 603/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0662 - val_loss: 0.8489\nEpoch 604/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0660 - val_loss: 0.8432\nEpoch 605/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0658 - val_loss: 0.8379\nEpoch 606/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0656 - val_loss: 0.8323\nEpoch 607/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0654 - val_loss: 0.8270\nEpoch 608/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0651 - val_loss: 0.8218\nEpoch 609/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0649 - val_loss: 0.8166\nEpoch 610/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0647 - val_loss: 0.8109\nEpoch 611/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0645 - val_loss: 0.8056\nEpoch 612/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0643 - val_loss: 0.8000\nEpoch 613/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0641 - val_loss: 0.7947\nEpoch 614/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0639 - val_loss: 0.7896\nEpoch 615/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0637 - val_loss: 0.7842\nEpoch 616/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0635 - val_loss: 0.7790\nEpoch 617/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0632 - val_loss: 0.7744\nEpoch 618/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0630 - val_loss: 0.7692\nEpoch 619/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0628 - val_loss: 0.7644\nEpoch 620/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0626 - val_loss: 0.7597\nEpoch 621/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0624 - val_loss: 0.7546\nEpoch 622/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0622 - val_loss: 0.7497\nEpoch 623/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0620 - val_loss: 0.7450\nEpoch 624/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0618 - val_loss: 0.7403\nEpoch 625/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0616 - val_loss: 0.7351\nEpoch 626/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0614 - val_loss: 0.7301\nEpoch 627/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0612 - val_loss: 0.7254\nEpoch 628/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0610 - val_loss: 0.7203\nEpoch 629/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0608 - val_loss: 0.7157\nEpoch 630/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0607 - val_loss: 0.7109\nEpoch 631/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0605 - val_loss: 0.7062\nEpoch 632/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0603 - val_loss: 0.7016\nEpoch 633/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0601 - val_loss: 0.6969\nEpoch 634/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0599 - val_loss: 0.6921\nEpoch 635/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0597 - val_loss: 0.6874\nEpoch 636/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0595 - val_loss: 0.6825\nEpoch 637/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0593 - val_loss: 0.6778\nEpoch 638/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0591 - val_loss: 0.6729\nEpoch 639/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0590 - val_loss: 0.6679\nEpoch 640/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0588 - val_loss: 0.6632\nEpoch 641/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0586 - val_loss: 0.6585\nEpoch 642/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0584 - val_loss: 0.6537\nEpoch 643/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0582 - val_loss: 0.6489\nEpoch 644/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0580 - val_loss: 0.6440\nEpoch 645/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0578 - val_loss: 0.6392\nEpoch 646/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0576 - val_loss: 0.6343\nEpoch 647/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0574 - val_loss: 0.6300\nEpoch 648/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0573 - val_loss: 0.6258\nEpoch 649/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0571 - val_loss: 0.6210\nEpoch 650/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0569 - val_loss: 0.6163\nEpoch 651/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0567 - val_loss: 0.6116\nEpoch 652/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0565 - val_loss: 0.6071\nEpoch 653/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0564 - val_loss: 0.6025\nEpoch 654/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0562 - val_loss: 0.5979\nEpoch 655/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0560 - val_loss: 0.5934\nEpoch 656/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0558 - val_loss: 0.5893\nEpoch 657/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0557 - val_loss: 0.5852\nEpoch 658/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0555 - val_loss: 0.5812\nEpoch 659/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0553 - val_loss: 0.5772\nEpoch 660/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0552 - val_loss: 0.5731\nEpoch 661/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0550 - val_loss: 0.5690\nEpoch 662/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0548 - val_loss: 0.5651\nEpoch 663/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0547 - val_loss: 0.5608\nEpoch 664/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0545 - val_loss: 0.5567\nEpoch 665/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0543 - val_loss: 0.5525\nEpoch 666/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0542 - val_loss: 0.5482\nEpoch 667/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0540 - val_loss: 0.5440\nEpoch 668/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0538 - val_loss: 0.5399\nEpoch 669/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0537 - val_loss: 0.5359\nEpoch 670/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0535 - val_loss: 0.5321\nEpoch 671/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0533 - val_loss: 0.5282\nEpoch 672/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0532 - val_loss: 0.5240\nEpoch 673/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0530 - val_loss: 0.5203\nEpoch 674/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0529 - val_loss: 0.5162\nEpoch 675/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0527 - val_loss: 0.5126\nEpoch 676/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0526 - val_loss: 0.5090\nEpoch 677/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0524 - val_loss: 0.5054\nEpoch 678/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0523 - val_loss: 0.5018\nEpoch 679/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0521 - val_loss: 0.4978\nEpoch 680/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0519 - val_loss: 0.4943\nEpoch 681/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0518 - val_loss: 0.4907\nEpoch 682/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0517 - val_loss: 0.4869\nEpoch 683/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0515 - val_loss: 0.4836\nEpoch 684/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0513 - val_loss: 0.4795\nEpoch 685/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0512 - val_loss: 0.4756\nEpoch 686/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0510 - val_loss: 0.4716\nEpoch 687/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0509 - val_loss: 0.4680\nEpoch 688/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0507 - val_loss: 0.4640\nEpoch 689/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0506 - val_loss: 0.4605\nEpoch 690/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0504 - val_loss: 0.4566\nEpoch 691/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0503 - val_loss: 0.4532\nEpoch 692/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0501 - val_loss: 0.4497\nEpoch 693/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0500 - val_loss: 0.4460\nEpoch 694/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0499 - val_loss: 0.4424\nEpoch 695/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0497 - val_loss: 0.4388\nEpoch 696/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0496 - val_loss: 0.4354\nEpoch 697/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0494 - val_loss: 0.4322\nEpoch 698/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0493 - val_loss: 0.4287\nEpoch 699/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0492 - val_loss: 0.4255\nEpoch 700/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0490 - val_loss: 0.4220\nEpoch 701/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0489 - val_loss: 0.4185\nEpoch 702/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0488 - val_loss: 0.4151\nEpoch 703/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0486 - val_loss: 0.4117\nEpoch 704/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0485 - val_loss: 0.4084\nEpoch 705/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0484 - val_loss: 0.4050\nEpoch 706/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0482 - val_loss: 0.4019\nEpoch 707/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0481 - val_loss: 0.3990\nEpoch 708/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0480 - val_loss: 0.3958\nEpoch 709/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0479 - val_loss: 0.3925\nEpoch 710/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0477 - val_loss: 0.3891\nEpoch 711/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0476 - val_loss: 0.3859\nEpoch 712/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0475 - val_loss: 0.3827\nEpoch 713/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0474 - val_loss: 0.3799\nEpoch 714/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0472 - val_loss: 0.3766\nEpoch 715/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0471 - val_loss: 0.3737\nEpoch 716/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0470 - val_loss: 0.3704\nEpoch 717/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0469 - val_loss: 0.3674\nEpoch 718/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0467 - val_loss: 0.3644\nEpoch 719/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0466 - val_loss: 0.3615\nEpoch 720/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0465 - val_loss: 0.3587\nEpoch 721/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0464 - val_loss: 0.3559\nEpoch 722/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0463 - val_loss: 0.3527\nEpoch 723/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0462 - val_loss: 0.3499\nEpoch 724/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0460 - val_loss: 0.3468\nEpoch 725/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0459 - val_loss: 0.3439\nEpoch 726/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0458 - val_loss: 0.3409\nEpoch 727/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0457 - val_loss: 0.3379\nEpoch 728/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0456 - val_loss: 0.3352\nEpoch 729/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0455 - val_loss: 0.3323\nEpoch 730/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0454 - val_loss: 0.3293\nEpoch 731/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0452 - val_loss: 0.3264\nEpoch 732/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0451 - val_loss: 0.3236\nEpoch 733/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0450 - val_loss: 0.3209\nEpoch 734/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0449 - val_loss: 0.3183\nEpoch 735/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0448 - val_loss: 0.3156\nEpoch 736/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0447 - val_loss: 0.3129\nEpoch 737/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0446 - val_loss: 0.3105\nEpoch 738/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0445 - val_loss: 0.3079\nEpoch 739/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0444 - val_loss: 0.3053\nEpoch 740/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0443 - val_loss: 0.3026\nEpoch 741/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0442 - val_loss: 0.3001\nEpoch 742/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0441 - val_loss: 0.2975\nEpoch 743/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0440 - val_loss: 0.2950\nEpoch 744/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0439 - val_loss: 0.2923\nEpoch 745/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0438 - val_loss: 0.2897\nEpoch 746/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0437 - val_loss: 0.2873\nEpoch 747/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0436 - val_loss: 0.2851\nEpoch 748/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0435 - val_loss: 0.2827\nEpoch 749/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0434 - val_loss: 0.2804\nEpoch 750/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0433 - val_loss: 0.2780\nEpoch 751/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0432 - val_loss: 0.2757\nEpoch 752/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0431 - val_loss: 0.2732\nEpoch 753/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0430 - val_loss: 0.2710\nEpoch 754/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0429 - val_loss: 0.2687\nEpoch 755/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0428 - val_loss: 0.2662\nEpoch 756/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0427 - val_loss: 0.2639\nEpoch 757/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0426 - val_loss: 0.2617\nEpoch 758/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0425 - val_loss: 0.2595\nEpoch 759/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0425 - val_loss: 0.2574\nEpoch 760/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0424 - val_loss: 0.2552\nEpoch 761/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0423 - val_loss: 0.2530\nEpoch 762/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0422 - val_loss: 0.2508\nEpoch 763/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0421 - val_loss: 0.2487\nEpoch 764/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0420 - val_loss: 0.2465\nEpoch 765/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0420 - val_loss: 0.2444\nEpoch 766/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0419 - val_loss: 0.2422\nEpoch 767/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0418 - val_loss: 0.2400\nEpoch 768/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0417 - val_loss: 0.2380\nEpoch 769/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0416 - val_loss: 0.2356\nEpoch 770/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0415 - val_loss: 0.2336\nEpoch 771/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0415 - val_loss: 0.2313\nEpoch 772/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0414 - val_loss: 0.2293\nEpoch 773/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0413 - val_loss: 0.2273\nEpoch 774/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0412 - val_loss: 0.2255\nEpoch 775/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0411 - val_loss: 0.2236\nEpoch 776/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0411 - val_loss: 0.2217\nEpoch 777/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0410 - val_loss: 0.2200\nEpoch 778/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0409 - val_loss: 0.2178\nEpoch 779/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0408 - val_loss: 0.2158\nEpoch 780/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0408 - val_loss: 0.2139\nEpoch 781/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0407 - val_loss: 0.2120\nEpoch 782/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0406 - val_loss: 0.2101\nEpoch 783/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0405 - val_loss: 0.2082\nEpoch 784/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0405 - val_loss: 0.2063\nEpoch 785/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0404 - val_loss: 0.2045\nEpoch 786/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0403 - val_loss: 0.2026\nEpoch 787/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0403 - val_loss: 0.2007\nEpoch 788/2000\n6/6 [==============================] - 0s 2ms/step - loss: 0.0402 - val_loss: 0.1989\nEpoch 789/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0401 - val_loss: 0.1971\nEpoch 790/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0400 - val_loss: 0.1955\nEpoch 791/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0400 - val_loss: 0.1938\nEpoch 792/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0399 - val_loss: 0.1920\nEpoch 793/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0398 - val_loss: 0.1903\nEpoch 794/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0398 - val_loss: 0.1883\nEpoch 795/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0397 - val_loss: 0.1866\nEpoch 796/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0396 - val_loss: 0.1848\nEpoch 797/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0396 - val_loss: 0.1831\nEpoch 798/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0395 - val_loss: 0.1816\nEpoch 799/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0394 - val_loss: 0.1799\nEpoch 800/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0394 - val_loss: 0.1782\nEpoch 801/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0393 - val_loss: 0.1765\nEpoch 802/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0392 - val_loss: 0.1750\nEpoch 803/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0392 - val_loss: 0.1736\nEpoch 804/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0391 - val_loss: 0.1721\nEpoch 805/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0391 - val_loss: 0.1708\nEpoch 806/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0390 - val_loss: 0.1693\nEpoch 807/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0389 - val_loss: 0.1678\nEpoch 808/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0389 - val_loss: 0.1663\nEpoch 809/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0388 - val_loss: 0.1648\nEpoch 810/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0388 - val_loss: 0.1632\nEpoch 811/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0387 - val_loss: 0.1617\nEpoch 812/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0387 - val_loss: 0.1603\nEpoch 813/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0386 - val_loss: 0.1589\nEpoch 814/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0386 - val_loss: 0.1575\nEpoch 815/2000\n6/6 [==============================] - 0s 2ms/step - loss: 0.0385 - val_loss: 0.1562\nEpoch 816/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0384 - val_loss: 0.1547\nEpoch 817/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0384 - val_loss: 0.1534\nEpoch 818/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0383 - val_loss: 0.1521\nEpoch 819/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0383 - val_loss: 0.1507\nEpoch 820/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0382 - val_loss: 0.1493\nEpoch 821/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0382 - val_loss: 0.1479\nEpoch 822/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0381 - val_loss: 0.1466\nEpoch 823/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0381 - val_loss: 0.1453\nEpoch 824/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0381 - val_loss: 0.1441\nEpoch 825/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0380 - val_loss: 0.1429\nEpoch 826/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0379 - val_loss: 0.1416\nEpoch 827/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0379 - val_loss: 0.1404\nEpoch 828/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0378 - val_loss: 0.1392\nEpoch 829/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0378 - val_loss: 0.1378\nEpoch 830/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0377 - val_loss: 0.1365\nEpoch 831/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0377 - val_loss: 0.1353\nEpoch 832/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0377 - val_loss: 0.1342\nEpoch 833/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0376 - val_loss: 0.1330\nEpoch 834/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0376 - val_loss: 0.1319\nEpoch 835/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0375 - val_loss: 0.1309\nEpoch 836/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0375 - val_loss: 0.1297\nEpoch 837/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0374 - val_loss: 0.1287\nEpoch 838/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0374 - val_loss: 0.1276\nEpoch 839/2000\n6/6 [==============================] - 0s 2ms/step - loss: 0.0373 - val_loss: 0.1265\nEpoch 840/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0373 - val_loss: 0.1253\nEpoch 841/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0373 - val_loss: 0.1241\nEpoch 842/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0372 - val_loss: 0.1229\nEpoch 843/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0372 - val_loss: 0.1218\nEpoch 844/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0371 - val_loss: 0.1209\nEpoch 845/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0371 - val_loss: 0.1198\nEpoch 846/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0370 - val_loss: 0.1189\nEpoch 847/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0370 - val_loss: 0.1178\nEpoch 848/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0370 - val_loss: 0.1168\nEpoch 849/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0369 - val_loss: 0.1159\nEpoch 850/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0369 - val_loss: 0.1149\nEpoch 851/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0369 - val_loss: 0.1139\nEpoch 852/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0368 - val_loss: 0.1129\nEpoch 853/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0368 - val_loss: 0.1120\nEpoch 854/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0367 - val_loss: 0.1111\nEpoch 855/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0367 - val_loss: 0.1102\nEpoch 856/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0367 - val_loss: 0.1093\nEpoch 857/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0366 - val_loss: 0.1083\nEpoch 858/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0366 - val_loss: 0.1074\nEpoch 859/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0366 - val_loss: 0.1066\nEpoch 860/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0365 - val_loss: 0.1057\nEpoch 861/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0365 - val_loss: 0.1047\nEpoch 862/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0365 - val_loss: 0.1038\nEpoch 863/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0364 - val_loss: 0.1029\nEpoch 864/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0364 - val_loss: 0.1019\nEpoch 865/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0364 - val_loss: 0.1009\nEpoch 866/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0363 - val_loss: 0.1000\nEpoch 867/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0363 - val_loss: 0.0992\nEpoch 868/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0363 - val_loss: 0.0984\nEpoch 869/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0362 - val_loss: 0.0976\nEpoch 870/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0362 - val_loss: 0.0968\nEpoch 871/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0362 - val_loss: 0.0962\nEpoch 872/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0362 - val_loss: 0.0954\nEpoch 873/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0361 - val_loss: 0.0946\nEpoch 874/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0361 - val_loss: 0.0939\nEpoch 875/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0361 - val_loss: 0.0931\nEpoch 876/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0360 - val_loss: 0.0924\nEpoch 877/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0360 - val_loss: 0.0916\nEpoch 878/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0360 - val_loss: 0.0908\nEpoch 879/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0360 - val_loss: 0.0902\nEpoch 880/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0359 - val_loss: 0.0895\nEpoch 881/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0359 - val_loss: 0.0889\nEpoch 882/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0359 - val_loss: 0.0881\nEpoch 883/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0359 - val_loss: 0.0875\nEpoch 884/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0358 - val_loss: 0.0868\nEpoch 885/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0358 - val_loss: 0.0861\nEpoch 886/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0358 - val_loss: 0.0853\nEpoch 887/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0358 - val_loss: 0.0846\nEpoch 888/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0357 - val_loss: 0.0839\nEpoch 889/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0357 - val_loss: 0.0833\nEpoch 890/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0357 - val_loss: 0.0827\nEpoch 891/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0357 - val_loss: 0.0821\nEpoch 892/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0356 - val_loss: 0.0815\nEpoch 893/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0356 - val_loss: 0.0810\nEpoch 894/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0356 - val_loss: 0.0804\nEpoch 895/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0356 - val_loss: 0.0799\nEpoch 896/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0356 - val_loss: 0.0794\nEpoch 897/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0355 - val_loss: 0.0789\nEpoch 898/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0355 - val_loss: 0.0783\nEpoch 899/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0355 - val_loss: 0.0778\nEpoch 900/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0355 - val_loss: 0.0773\nEpoch 901/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0355 - val_loss: 0.0768\nEpoch 902/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0354 - val_loss: 0.0762\nEpoch 903/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0354 - val_loss: 0.0756\nEpoch 904/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0354 - val_loss: 0.0751\nEpoch 905/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0354 - val_loss: 0.0746\nEpoch 906/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0354 - val_loss: 0.0740\nEpoch 907/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0353 - val_loss: 0.0735\nEpoch 908/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0353 - val_loss: 0.0731\nEpoch 909/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0353 - val_loss: 0.0726\nEpoch 910/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0353 - val_loss: 0.0723\nEpoch 911/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0353 - val_loss: 0.0719\nEpoch 912/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0353 - val_loss: 0.0714\nEpoch 913/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0352 - val_loss: 0.0709\nEpoch 914/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0703\nEpoch 915/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0699\nEpoch 916/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0694\nEpoch 917/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0690\nEpoch 918/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0684\nEpoch 919/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0680\nEpoch 920/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0675\nEpoch 921/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0671\nEpoch 922/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0666\nEpoch 923/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0663\nEpoch 924/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0659\nEpoch 925/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0654\nEpoch 926/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0649\nEpoch 927/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0644\nEpoch 928/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0639\nEpoch 929/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0634\nEpoch 930/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0631\nEpoch 931/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0628\nEpoch 932/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0624\nEpoch 933/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0622\nEpoch 934/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0618\nEpoch 935/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0614\nEpoch 936/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0611\nEpoch 937/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0608\nEpoch 938/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0349 - val_loss: 0.0606\nEpoch 939/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0603\nEpoch 940/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0599\nEpoch 941/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0597\nEpoch 942/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0594\nEpoch 943/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0591\nEpoch 944/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0588\nEpoch 945/2000\n6/6 [==============================] - 0s 2ms/step - loss: 0.0348 - val_loss: 0.0584\nEpoch 946/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0580\nEpoch 947/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0577\nEpoch 948/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0575\nEpoch 949/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0571\nEpoch 950/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0569\nEpoch 951/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0565\nEpoch 952/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0563\nEpoch 953/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0560\nEpoch 954/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0557\nEpoch 955/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0554\nEpoch 956/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0552\nEpoch 957/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0549\nEpoch 958/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0347 - val_loss: 0.0547\nEpoch 959/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0544\nEpoch 960/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0542\nEpoch 961/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0540\nEpoch 962/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0346 - val_loss: 0.0537\nEpoch 963/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0534\nEpoch 964/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0532\nEpoch 965/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0529\nEpoch 966/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0527\nEpoch 967/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0524\nEpoch 968/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0522\nEpoch 969/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0520\nEpoch 970/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0519\nEpoch 971/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0517\nEpoch 972/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0515\nEpoch 973/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0513\nEpoch 974/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0511\nEpoch 975/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0509\nEpoch 976/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0507\nEpoch 977/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0505\nEpoch 978/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0503\nEpoch 979/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0501\nEpoch 980/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0499\nEpoch 981/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0497\nEpoch 982/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0495\nEpoch 983/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0494\nEpoch 984/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0492\nEpoch 985/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0490\nEpoch 986/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0488\nEpoch 987/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0486\nEpoch 988/2000\n6/6 [==============================] - 0s 2ms/step - loss: 0.0344 - val_loss: 0.0484\nEpoch 989/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0482\nEpoch 990/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0481\nEpoch 991/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0479\nEpoch 992/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0478\nEpoch 993/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0476\nEpoch 994/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0475\nEpoch 995/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0473\nEpoch 996/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0472\nEpoch 997/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0344 - val_loss: 0.0470\nEpoch 998/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0469\nEpoch 999/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0467\nEpoch 1000/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0466\nEpoch 1001/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0344 - val_loss: 0.0464\nEpoch 1002/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0463\nEpoch 1003/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0462\nEpoch 1004/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0461\nEpoch 1005/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0459\nEpoch 1006/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0458\nEpoch 1007/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0458\nEpoch 1008/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0457\nEpoch 1009/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0457\nEpoch 1010/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0456\nEpoch 1011/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0455\nEpoch 1012/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0454\nEpoch 1013/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0454\nEpoch 1014/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0343 - val_loss: 0.0452\nEpoch 1015/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0452\nEpoch 1016/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0450\nEpoch 1017/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0448\nEpoch 1018/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0446\nEpoch 1019/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0445\nEpoch 1020/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0444\nEpoch 1021/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0443\nEpoch 1022/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0442\nEpoch 1023/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0441\nEpoch 1024/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0441\nEpoch 1025/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0440\nEpoch 1026/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0439\nEpoch 1027/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0438\nEpoch 1028/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0437\nEpoch 1029/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0436\nEpoch 1030/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0434\nEpoch 1031/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0434\nEpoch 1032/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0433\nEpoch 1033/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0432\nEpoch 1034/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0431\nEpoch 1035/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0431\nEpoch 1036/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0343 - val_loss: 0.0430\nEpoch 1037/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0429\nEpoch 1038/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0428\nEpoch 1039/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0427\nEpoch 1040/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0427\nEpoch 1041/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0426\nEpoch 1042/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0425\nEpoch 1043/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0424\nEpoch 1044/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0423\nEpoch 1045/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0423\nEpoch 1046/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0422\nEpoch 1047/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0421\nEpoch 1048/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0421\nEpoch 1049/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0420\nEpoch 1050/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0419\nEpoch 1051/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0418\nEpoch 1052/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0418\nEpoch 1053/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0417\nEpoch 1054/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0417\nEpoch 1055/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0417\nEpoch 1056/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0416\nEpoch 1057/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0416\nEpoch 1058/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0416\nEpoch 1059/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0415\nEpoch 1060/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0415\nEpoch 1061/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0414\nEpoch 1062/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0414\nEpoch 1063/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0413\nEpoch 1064/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0412\nEpoch 1065/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0412\nEpoch 1066/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0411\nEpoch 1067/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0410\nEpoch 1068/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0410\nEpoch 1069/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0409\nEpoch 1070/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0409\nEpoch 1071/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0409\nEpoch 1072/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0409\nEpoch 1073/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0408\nEpoch 1074/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0408\nEpoch 1075/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0407\nEpoch 1076/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0407\nEpoch 1077/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0407\nEpoch 1078/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0407\nEpoch 1079/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0406\nEpoch 1080/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0406\nEpoch 1081/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0406\nEpoch 1082/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0405\nEpoch 1083/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0405\nEpoch 1084/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0405\nEpoch 1085/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0404\nEpoch 1086/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0404\nEpoch 1087/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0404\nEpoch 1088/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0403\nEpoch 1089/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0403\nEpoch 1090/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0403\nEpoch 1091/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0403\nEpoch 1092/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0403\nEpoch 1093/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0402\nEpoch 1094/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0402\nEpoch 1095/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0402\nEpoch 1096/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0401\nEpoch 1097/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0401\nEpoch 1098/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0401\nEpoch 1099/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0401\nEpoch 1100/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0401\nEpoch 1101/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0401\nEpoch 1102/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0400\nEpoch 1103/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0400\nEpoch 1104/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0400\nEpoch 1105/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0399\nEpoch 1106/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0399\nEpoch 1107/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0399\nEpoch 1108/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0399\nEpoch 1109/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0398\nEpoch 1110/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0398\nEpoch 1111/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0398\nEpoch 1112/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0398\nEpoch 1113/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0398\nEpoch 1114/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0398\nEpoch 1115/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0397\nEpoch 1116/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0397\nEpoch 1117/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0397\nEpoch 1118/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0396\nEpoch 1119/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0396\nEpoch 1120/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0396\nEpoch 1121/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0396\nEpoch 1122/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0395\nEpoch 1123/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0395\nEpoch 1124/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0395\nEpoch 1125/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0395\nEpoch 1126/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0395\nEpoch 1127/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1128/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0395\nEpoch 1129/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0395\nEpoch 1130/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0395\nEpoch 1131/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0395\nEpoch 1132/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0395\nEpoch 1133/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0395\nEpoch 1134/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1135/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1136/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1137/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1138/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1139/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1140/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1141/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1142/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1143/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1144/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1145/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1146/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1147/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1148/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1149/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1150/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1151/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1152/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1153/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1154/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1155/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1156/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0392\nEpoch 1157/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0392\nEpoch 1158/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0392\nEpoch 1159/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0392\nEpoch 1160/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0392\nEpoch 1161/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0392\nEpoch 1162/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0392\nEpoch 1163/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1164/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1165/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1166/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1167/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1168/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1169/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1170/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1171/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1172/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1173/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1174/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1175/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1176/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1177/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1178/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1179/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1180/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1181/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1182/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1183/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1184/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1185/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1186/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1187/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1188/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1189/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1190/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1191/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1192/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1193/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1194/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1195/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1196/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1197/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1198/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1199/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1200/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1201/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1202/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1203/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1204/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1205/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1206/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1207/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1208/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1209/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1210/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1211/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1212/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1213/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1214/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1215/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1216/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1217/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1218/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1219/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1220/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1221/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1222/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1223/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1224/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1225/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1226/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1227/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1228/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1229/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1230/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1231/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1232/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1233/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1234/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1235/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1236/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1237/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1238/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1239/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1240/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1241/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1242/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1243/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1244/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1245/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1246/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1247/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1248/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1249/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1250/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1251/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1252/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1253/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1254/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1255/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1256/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1257/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1258/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1259/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1260/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1261/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1262/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1263/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1264/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1265/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1266/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1267/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1268/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1269/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1270/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1271/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1272/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1273/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1274/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1275/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1276/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1277/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1278/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1279/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1280/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1281/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1282/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1283/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1284/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1285/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1286/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1287/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1288/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1289/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1290/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1291/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1292/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1293/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1294/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1295/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1296/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1297/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1298/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1299/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1300/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1301/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1302/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1303/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1304/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1305/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1306/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1307/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1308/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1309/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1310/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1311/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1312/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1313/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1314/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1315/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1316/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1317/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1318/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1319/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1320/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1321/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1322/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1323/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1324/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1325/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1326/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1327/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1328/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1329/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1330/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1331/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1332/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1333/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1334/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1335/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1336/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1337/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1338/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1339/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1340/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1341/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1342/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1343/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1344/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1345/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1346/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1347/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1348/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1349/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1350/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1351/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1352/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1353/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1354/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1355/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1356/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1357/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1358/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1359/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1360/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1361/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1362/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1363/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1364/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1365/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1366/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1367/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1368/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1369/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1370/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1371/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1372/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1373/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1374/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1375/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1376/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1377/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1378/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1379/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1380/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1381/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1382/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1383/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1384/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1385/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1386/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1387/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1388/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1389/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1390/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1391/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1392/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1393/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1394/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1395/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1396/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1397/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1398/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1399/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1400/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1401/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1402/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1403/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1404/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1405/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1406/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1407/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1408/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1409/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1410/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1411/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1412/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1413/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1414/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1415/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1416/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1417/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1418/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1419/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1420/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1421/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1422/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1423/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1424/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1425/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1426/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1427/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1428/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1429/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1430/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1431/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1432/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1433/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1434/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1435/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1436/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1437/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1438/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1439/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1440/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1441/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1442/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1443/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1444/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1445/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1446/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1447/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1448/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1449/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1450/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1451/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1452/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1453/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1454/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1455/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1456/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1457/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1458/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1459/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1460/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1461/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1462/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1463/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1464/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1465/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1466/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1467/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1468/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1469/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1470/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1471/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1472/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1473/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1474/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1475/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1476/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1477/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1478/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1479/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1480/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1481/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1482/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1483/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1484/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1485/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1486/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1487/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1488/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1489/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1490/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1491/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1492/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1493/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1494/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1495/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1496/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1497/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1498/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1499/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1500/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1501/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1502/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1503/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1504/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1505/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1506/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1507/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1508/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1509/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1510/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1511/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1512/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1513/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1514/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1515/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1516/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1517/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1518/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1519/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1520/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1521/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1522/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1523/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1524/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1525/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1526/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1527/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1528/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1529/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1530/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1531/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1532/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1533/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1534/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1535/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1536/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1537/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1538/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1539/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1540/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1541/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1542/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1543/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1544/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1545/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1546/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1547/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1548/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1549/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1550/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1551/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1552/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1553/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1554/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1555/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1556/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1557/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1558/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1559/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1560/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1561/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1562/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1563/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1564/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1565/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1566/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1567/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1568/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1569/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1570/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1571/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1572/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1573/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1574/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1575/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1576/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1577/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1578/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1579/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1580/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1581/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1582/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1583/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1584/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1585/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1586/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1587/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1588/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1589/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1590/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1591/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1592/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1593/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1594/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1595/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1596/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1597/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1598/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1599/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1600/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1601/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1602/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1603/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1604/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1605/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1606/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1607/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1608/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1609/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1610/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1611/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1612/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1613/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1614/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1615/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1616/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1617/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1618/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1619/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1620/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1621/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1622/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1623/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1624/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1625/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1626/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1627/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1628/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1629/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1630/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1631/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1632/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1633/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1634/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1635/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1636/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1637/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1638/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1639/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1640/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1641/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1642/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1643/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1644/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1645/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1646/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1647/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1648/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1649/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1650/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1651/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1652/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1653/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1654/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1655/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1656/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1657/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1658/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1659/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1660/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1661/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1662/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1663/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1664/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1665/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1666/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1667/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1668/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1669/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1670/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1671/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1672/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1673/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1674/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1675/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1676/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1677/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1678/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1679/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1680/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1681/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1682/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1683/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1684/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1685/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1686/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1687/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1688/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1689/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1690/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1691/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1692/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1693/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1694/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1695/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1696/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1697/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1698/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1699/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1700/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1701/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1702/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1703/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1704/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1705/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1706/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1707/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1708/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1709/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1710/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1711/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1712/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1713/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1714/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1715/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1716/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1717/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1718/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1719/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1720/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1721/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1722/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1723/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1724/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1725/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1726/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1727/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1728/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1729/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1730/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1731/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1732/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1733/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1734/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1735/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1736/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1737/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1738/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1739/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1740/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1741/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1742/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1743/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1744/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1745/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1746/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1747/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1748/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1749/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1750/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1751/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1752/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1753/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1754/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1755/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1756/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1757/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1758/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1759/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1760/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1761/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1762/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1763/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1764/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1765/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1766/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1767/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1768/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1769/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1770/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1771/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1772/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1773/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1774/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1775/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1776/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1777/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1778/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1779/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1780/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1781/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1782/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1783/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1784/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1785/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1786/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1787/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1788/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1789/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1790/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1791/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1792/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1793/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1794/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1795/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1796/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1797/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1798/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1799/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1800/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1801/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1802/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1803/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1804/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1805/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1806/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1807/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1808/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1809/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1810/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1811/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1812/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1813/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1814/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1815/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1816/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1817/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1818/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1819/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1820/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1821/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1822/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1823/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1824/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1825/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1826/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1827/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1828/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1829/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1830/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1831/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1832/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1833/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1834/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1835/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1836/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1837/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1838/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1839/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1840/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1841/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1842/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1843/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1844/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1845/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1846/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1847/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1848/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1849/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1850/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1851/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1852/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1853/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1854/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1855/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1856/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1857/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1858/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1859/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1860/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1861/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1862/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1863/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1864/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1865/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1866/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1867/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1868/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1869/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1870/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1871/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1872/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1873/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1874/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1875/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1876/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1877/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1878/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1879/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1880/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1881/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1882/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1883/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1884/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1885/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1886/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1887/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1888/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1889/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1890/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1891/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1892/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1893/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1894/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1895/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1896/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1897/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1898/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1899/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1900/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1901/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1902/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1903/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1904/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1905/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1906/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1907/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1908/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1909/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1910/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1911/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1912/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1913/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1914/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1915/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1916/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1917/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1918/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1919/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1920/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1921/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1922/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1923/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1924/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1925/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1926/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1927/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1928/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1929/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1930/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1931/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1932/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1933/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1934/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1935/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1936/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1937/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1938/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1939/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1940/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1941/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1942/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1943/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1944/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1945/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1946/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1947/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1948/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1949/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1950/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1951/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1952/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1953/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1954/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1955/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1956/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1957/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1958/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1959/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1960/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1961/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1962/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1963/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1964/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1965/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1966/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1967/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1968/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1969/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1970/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1971/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1972/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1973/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1974/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1975/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1976/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1977/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1978/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1979/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1980/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1981/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1982/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1983/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1984/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1985/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1986/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1987/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1988/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1989/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1990/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1991/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1992/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1993/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1994/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1995/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1996/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1997/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1998/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1999/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 2000/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\n\n\n&lt;keras.callbacks.History at 0x7f67c81a26b0&gt;\n\n\n\n#\n#%tensorboard --logdir logs --host 0.0.0.0\n\n\n\n텐서보드: 사용자지정그림 에폭별로 시각화 (2)\n- 중간층의 출력결과를 시각화하고 싶다.\n4. Piecewise-linear regression (15점)\n아래의 모형을 고려하자.\nmodel: \\(y_i=\\begin{cases} x_i +0.3\\epsilon_i & x\\leq 0 \\\\ 3.5x_i +0.3\\epsilon_i & x&gt;0 \\end{cases}\\)\n아래는 위의 모형에서 생성한 샘플이다.\n\n## data\nnp.random.seed(43052)\nN=100\nx= np.linspace(-1,1,N).reshape(N,1)\ny= np.array(list(map(lambda x: x*1+np.random.normal()*0.3 if x&lt;0 else x*3.5+np.random.normal()*0.3,x))).reshape(N,1)\n\n(풀이)\n\ntf.random.set_seed(43055)\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(2))\nnet.add(tf.keras.layers.Activation(tf.nn.relu))\nnet.add(tf.keras.layers.Dense(1))\nnet.compile(optimizer='sgd',loss='mse')\nnet.fit(x,y,epochs=1,batch_size=100)\n\n1/1 [==============================] - 0s 79ms/step - loss: 2.1414\n\n\n&lt;keras.callbacks.History at 0x7f68849e9420&gt;\n\n\n\nl1,a1,l2 =net.layers\n\n\nfig, (ax1,ax2,ax3) = plt.subplots(1,3)\nfig.set_figwidth(9)\nax1.plot(x,y,'.',alpha=0.2); ax1.plot(x,l1(x),'--');\nax2.plot(x,y,'.',alpha=0.2); ax2.plot(x,a1(l1(x)),'--');\nax3.plot(x,y,'.',alpha=0.2); ax3.plot(x,l2(a1(l1(x))),'--r');\n\n\n\n\n\n이런 그림이 100에폭마다 그려졌으면 좋겠다.\n\n- 새로운 클래스를 만들자.\n\nclass PlotMidlayer(tf.keras.callbacks.Callback):\n    def on_epoch_begin(self,epoch,logs): # 입력은 무조건 self, epoch, logs를 써야합니다 --&gt; 이 함수안에 에폭이 끝날때마다 할 동작을 정의한다.\n        if epoch % 100 ==0:\n            fig, (ax1,ax2,ax3) = plt.subplots(1,3)\n            fig.set_figwidth(9)\n            ax1.plot(x,y,'.',alpha=0.2); ax1.plot(x,l1(x),'--');\n            ax2.plot(x,y,'.',alpha=0.2); ax2.plot(x,a1(l1(x)),'--');\n            ax3.plot(x,y,'.',alpha=0.2); ax3.plot(x,l2(a1(l1(x))),'--r');\n            with tf.summary.create_file_writer(logdir).as_default():\n                tf.summary.image(\"적합결과시각화\"+str(epoch), plot_to_image(fig), step=0)\n\n\n!rm -rf logs\ncb1= tf.keras.callbacks.TensorBoard(update_freq='epoch',histogram_freq=100)\ncb2= PlotMidlayer()\nnet.fit(x,y,epochs=1000, batch_size=100,verbose=0 ,callbacks=[cb1,cb2])\n\n&lt;keras.callbacks.History at 0x7f6ef421c280&gt;\n\n\n\n#\n#%tensorboard --logdir logs --host 0.0.0.0"
  },
  {
    "objectID": "posts/2022_05_02_(9주차)_5월2일(2).html",
    "href": "posts/2022_05_02_(9주차)_5월2일(2).html",
    "title": "[STBDA] 09wk(2): 확률적 경사하강법",
    "section": "",
    "text": "해당 강의노트는 전북대학교 최규빈교수님 STBDA2022 자료임"
  },
  {
    "objectID": "posts/2022_05_02_(9주차)_5월2일(2).html#import",
    "href": "posts/2022_05_02_(9주차)_5월2일(2).html#import",
    "title": "[STBDA] 09wk(2): 확률적 경사하강법",
    "section": "import",
    "text": "import\n\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow.experimental.numpy as tnp\n\n\ntf.config.experimental.list_physical_devices()\n\n[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n\n\n\nimport graphviz\ndef gv(s): return graphviz.Source('digraph G{ rankdir=\"LR\"'+ s + ';}')"
  },
  {
    "objectID": "posts/2022_05_02_(9주차)_5월2일(2).html#중간고사-관련-잡담",
    "href": "posts/2022_05_02_(9주차)_5월2일(2).html#중간고사-관련-잡담",
    "title": "[STBDA] 09wk(2): 확률적 경사하강법",
    "section": "중간고사 관련 잡담",
    "text": "중간고사 관련 잡담\n\n중간고사 3번문제\n- 특이한모형: 오버핏이 일어날 수 없는 모형이다.\n\n유의미한 coef: 상수항(bias), \\(\\cos(t)\\)의 계수, \\(\\cos(2t)\\)의 계수, \\(\\cos(5t)\\)의 계수.\n유의미하지 않은 coef: \\(\\cos(3t)\\)의 계수, \\(\\cos(4t)\\)의 계수\n유의미하지 않은 계수는 \\(n%\\)이 커질수록 0으로 추정된다 = \\(\\cos(3t)\\)와 \\(\\cos(5t)\\)는 사용자가 임의로 제외하지 않아도 결국 모형에서 알아서 제거된다 = overfit이 일어나지 않는다. 모형이 알아서 유의미한 변수만 뽑아서 fit하는 느낌\n\n- 3번문제는 overfit이 일어나지 않는다. 이러한 신기한 일이 일어나는 이유는 모든 설명변수가 직교하기 때문임.\n\n이런 모형의 장점: overfit이 일어날 위험이 없으므로 train/test로 나누어 학습할 이유가 없다. (샘플만 버리는 꼴, test에 빼 둔 observation까지 모아서 학습해 \\(\\beta\\)를 좀 더 정확히 추론하는게 차라리 더 이득)\n이러한 모형에서 할일: 추정된 계수들이 0인지 아닌지만 test하면 된다. (이것을 유의성검정이라고 한다)\n\n 변수가 많아도 오버피팅이 일어나지 않는 경우가 있다. orthogonal한것이 있으면!\n- 직교기저의 예시\n\n빨강과 파랑을 255,255만큼 섞으면 보라색이 된다.\n빨강과 파랑과 노랑을 각각 255,255,255만큼 섞으면 검은색이 된다.\n임의의 어떠한 색도 빨강,파랑,노랑의 조합으로 표현가능하다. 즉 $= *_1 + *_2\n*_3$ 이다.\n(빨,파,노)는 색을 표현하는 basis이다. (적절한 \\(\\beta_1,\\beta_2,\\beta_3\\)을 구하기만 하면 임의의 색도 표현가능)\n(빨,보,노)역시 색을 표현하는 basis라 볼 수 있다. (파란색이 필요할때 보라색-빨간색을 하면되니까)\n(빨,보,검)역시 색을 표현하는 basis라 볼 수 있다. (파란색이 필요하면 보라색-빨간색을 하면되고, 노란색이 필요하면 검정색-보라색을 하면 되니까)\n(빨,파,노)는 직교기저이다.\n\n- 3번에서 알아둘 것: (1) 직교기저의 개념 (추후 재설명) (2) 임의의 색을 표현하려면 3개의 basis가 필요함\n\n\n중간고사 1-(3)번 문제\n- 그림을 그려보자.\n\n_x= tf.constant(np.arange(1,10001)/10000)\n_y= tnp.random.randn(10000) + (0.5 + 2*_x)\nplt.plot(_x,_y,'.',alpha=0.1)\n\n\n\n\n- 저것 꼭 10000개 다 모아서 loss계산해야할까?\n\nplt.plot(_x,_y,'.',alpha=0.1)\nplt.plot(_x[::10],_y[::10],'.')\n\n\n\n\n- 대충 이정도만 모아서 해도 비슷하지 않을까? \\(\\to\\) 해보자!"
  },
  {
    "objectID": "posts/2022_05_02_(9주차)_5월2일(2).html#경사하강법과-확률적경사하강법",
    "href": "posts/2022_05_02_(9주차)_5월2일(2).html#경사하강법과-확률적경사하강법",
    "title": "[STBDA] 09wk(2): 확률적 경사하강법",
    "section": "경사하강법과 확률적경사하강법",
    "text": "경사하강법과 확률적경사하강법\n\nver1: 모든 샘플을 사용하여 slope계산\n- 단순회귀분석에서 샘플 10개 관측: \\((x_1,y_1),\\dots,(x_{10},y_{10})\\).\n(epoch1) \\(loss=\\sum_{i=1}^{10}(y_i-\\beta_0-\\beta_1x_i)^2 \\quad \\to \\quad slope \\quad \\to \\quad update\\)\n(epoch2) \\(loss=\\sum_{i=1}^{10}(y_i-\\beta_0-\\beta_1x_i)^2 \\quad \\to \\quad slope \\quad \\to \\quad update\\)\n…\n\n\nver2: 하나의 샘플만 사용하여 slope계산\n(epoch1) - \\(loss=(y_1-\\beta_0-\\beta_1x_1)^2 \\quad \\to \\quad slope \\quad \\to \\quad update\\) - \\(loss=(y_2-\\beta_0-\\beta_1x_2)^2 \\quad \\to \\quad slope \\quad \\to \\quad update\\) - … - \\(loss=(y_{10}-\\beta_0-\\beta_1x_{10})^2 \\quad \\to \\quad slope \\quad \\to \\quad update\\)\n(epoch2) - \\(loss=(y_1-\\beta_0-\\beta_1x_1)^2 \\quad \\to \\quad slope \\quad \\to \\quad update\\) - \\(loss=(y_2-\\beta_0-\\beta_1x_2)^2 \\quad \\to \\quad slope \\quad \\to \\quad update\\) - … - \\(loss=(y_{10}-\\beta_0-\\beta_1x_{10})^2 \\quad \\to \\quad slope \\quad \\to \\quad update\\)\n…\n\n\nver3: \\(m(\\leq n)\\)개의 샘플만 사용하여 slope계산\n\\(m=3\\)이라고 하자.\n(epoch1) - \\(loss=\\sum_{i=1}^{3}(y_i-\\beta_0-\\beta_1x_i)^2 \\quad \\to \\quad slope \\quad \\to \\quad update\\) - \\(loss=\\sum_{i=4}^{6}(y_i-\\beta_0-\\beta_1x_i)^2 \\quad \\to \\quad slope \\quad \\to \\quad update\\) - \\(loss=\\sum_{i=7}^{9}(y_i-\\beta_0-\\beta_1x_i)^2 \\quad \\to \\quad slope \\quad \\to \\quad update\\) - \\(loss=(y_{10}-\\beta_0-\\beta_1x_{10})^2 \\quad \\to \\quad slope \\quad \\to \\quad update\\)\n(epoch2) - \\(loss=\\sum_{i=1}^{3}(y_i-\\beta_0-\\beta_1x_i)^2 \\quad \\to \\quad slope \\quad \\to \\quad update\\) - \\(loss=\\sum_{i=4}^{6}(y_i-\\beta_0-\\beta_1x_i)^2 \\quad \\to \\quad slope \\quad \\to \\quad update\\) - \\(loss=\\sum_{i=7}^{9}(y_i-\\beta_0-\\beta_1x_i)^2 \\quad \\to \\quad slope \\quad \\to \\quad update\\) - \\(loss=(y_{10}-\\beta_0-\\beta_1x_{10})^2 \\quad \\to \\quad slope \\quad \\to \\quad update\\)\n…\n\n\n용어의 정리\n\n옛날 (좀 더 엄밀)\n- ver1: gradient descent, batch gradient descent\n- ver2: stochastic gradient descent\n:확률적 경사하강법\n- ver3: mini-batch gradient descent, mini-batch stochastic gradient descent\n\n\n요즘\n- ver1: gradient descent\n- ver2: stochastic gradient descent with batch size = 1\n- ver3: stochastic gradient descent - https://www.deeplearningbook.org/contents/optimization.html, 알고리즘 8-1 참고.\nnote: 이렇게 많이 쓰는 이유? ver1,2는 사실상 없는 방법이므로\n\n\n\nver1,2,3 이외에 좀 더 지저분한 것들이 있다.\n- ver2,3에서 샘플을 셔플할 수도 있다.\n- ver3에서 일부 샘플이 학습에 참여 안하는 버전도 있다.\n- 개인적 생각: 크게3개정도만 알면 괜찮고 나머지는 그렇게 유의미하지 않아보인다.\n\n\nDiscussion\n- 핵심개념\n\n메모리사용량: ver1 &gt; ver3 &gt; ver2\n계산속도: ver1 &gt; ver3 &gt; ver2\nlocal-min에 갇힘: ver1 &gt; ver3 &gt; ver2\n\n- 본질: GPU 메모리가 한정되어 있어서 ver1을 쓰지는 못한다. GPU 메모리를 가장 적게쓰는것은 ver2인데 이것은 너무 불안정하다.\n- 틀리진 않지만 어색한 블로그 정리 내용들\n\n경사하강법은 종종 국소최소점에 갇히는 문제가 있다. 이를 해결하기 위해서 등장한 방법이 확률적 경사하강법이다. –&gt; 영 틀린말은 아니지만 그걸 의도하고 만든건 아님\n경사하강법은 계산시간이 오래걸린다. 계산을 빠르게 하기 위해서 등장한 방법이 확률적 경사하강법이다. –&gt; 1회 업데이트는 빠르게 계산함. 하지만 그것이 최적의 \\(\\beta\\)를 빠르게 얻을 수 있다는 의미는 아님"
  },
  {
    "objectID": "posts/2022_05_02_(9주차)_5월2일(2).html#fashion_mnist-모듈",
    "href": "posts/2022_05_02_(9주차)_5월2일(2).html#fashion_mnist-모듈",
    "title": "[STBDA] 09wk(2): 확률적 경사하강법",
    "section": "fashion_mnist 모듈",
    "text": "fashion_mnist 모듈\n\ntf.keras.datasets.fashion_mnist.load_data()\n- tf.keras.datasets.fashion_mnist.load_data 의 리턴값 조사\n\ntype(tf.keras.datasets.fashion_mnist)\n\nmodule\n\n\n\ntf.keras.datasets.fashion_mnist.load_data??\n\n\nSignature: tf.keras.datasets.fashion_mnist.load_data()\nSource:   \n@keras_export(\"keras.datasets.fashion_mnist.load_data\")\ndef load_data():\n    \"\"\"Loads the Fashion-MNIST dataset.\n    This is a dataset of 60,000 28x28 grayscale images of 10 fashion categories,\n    along with a test set of 10,000 images. This dataset can be used as\n    a drop-in replacement for MNIST.\n    The classes are:\n    | Label | Description |\n    |:-----:|-------------|\n    |   0   | T-shirt/top |\n    |   1   | Trouser     |\n    |   2   | Pullover    |\n    |   3   | Dress       |\n    |   4   | Coat        |\n    |   5   | Sandal      |\n    |   6   | Shirt       |\n    |   7   | Sneaker     |\n    |   8   | Bag         |\n    |   9   | Ankle boot  |\n    Returns:\n      Tuple of NumPy arrays: `(x_train, y_train), (x_test, y_test)`.\n    **x_train**: uint8 NumPy array of grayscale image data with shapes\n      `(60000, 28, 28)`, containing the training data.\n    **y_train**: uint8 NumPy array of labels (integers in range 0-9)\n      with shape `(60000,)` for the training data.\n    **x_test**: uint8 NumPy array of grayscale image data with shapes\n      (10000, 28, 28), containing the test data.\n    **y_test**: uint8 NumPy array of labels (integers in range 0-9)\n      with shape `(10000,)` for the test data.\n    Example:\n    ```python\n    (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n    assert x_train.shape == (60000, 28, 28)\n    assert x_test.shape == (10000, 28, 28)\n    assert y_train.shape == (60000,)\n    assert y_test.shape == (10000,)\n    ```\n    License:\n      The copyright for Fashion-MNIST is held by Zalando SE.\n      Fashion-MNIST is licensed under the [MIT license](\n      https://github.com/zalandoresearch/fashion-mnist/blob/master/LICENSE).\n    \"\"\"\n    dirname = os.path.join(\"datasets\", \"fashion-mnist\")\n    base = \"https://storage.googleapis.com/tensorflow/tf-keras-datasets/\"\n    files = [\n        \"train-labels-idx1-ubyte.gz\",\n        \"train-images-idx3-ubyte.gz\",\n        \"t10k-labels-idx1-ubyte.gz\",\n        \"t10k-images-idx3-ubyte.gz\",\n    ]\n    paths = []\n    for fname in files:\n        paths.append(get_file(fname, origin=base + fname, cache_subdir=dirname))\n    with gzip.open(paths[0], \"rb\") as lbpath:\n        y_train = np.frombuffer(lbpath.read(), np.uint8, offset=8)\n    with gzip.open(paths[1], \"rb\") as imgpath:\n        x_train = np.frombuffer(imgpath.read(), np.uint8, offset=16).reshape(\n            len(y_train), 28, 28\n        )\n    with gzip.open(paths[2], \"rb\") as lbpath:\n        y_test = np.frombuffer(lbpath.read(), np.uint8, offset=8)\n    with gzip.open(paths[3], \"rb\") as imgpath:\n        x_test = np.frombuffer(imgpath.read(), np.uint8, offset=16).reshape(\n            len(y_test), 28, 28\n        )\n    return (x_train, y_train), (x_test, y_test)\nFile:      ~/anaconda3/envs/py38/lib/python3.8/site-packages/keras/datasets/fashion_mnist.py\nType:      function\n\n\n\n\n\n데이터생성 및 탐색\n- tf.keras.datasets.fashion_mnist.load_data()를 이용한 데이터 생성\n\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n\n- 차원확인\n\nx_train.shape, y_train.shape, x_test.shape,y_test.shape\n\n((60000, 28, 28), (60000,), (10000, 28, 28), (10000,))\n\n\n\n60000은 obs숫자인듯\n(28,28)은 28픽셀,28픽셀을 의미하는듯\ntrain/test는 6:1로 나눈것 같음\n\n- 첫번째 obs\n\nplt.imshow(x_train[0])\n\n&lt;matplotlib.image.AxesImage at 0x7fa03407afd0&gt;\n\n\n\n\n\n\ny_train[0]\n\n9\n\n\n\n첫번쨰 obs에 대응하는 라벨\n\n- 첫번째 obs와 동일한 라벨을 가지는 그림을 찾아보자.\n\nnp.where(y_train==9)\n\n(array([    0,    11,    15, ..., 59932, 59970, 59978]),)\n\n\n\ny_train[11]\n\n9\n\n\n\nplt.imshow(x_train[11])\n\n&lt;matplotlib.image.AxesImage at 0x7fa0342098e0&gt;\n\n\n\n\n\n\n\n데이터구조\n- \\({\\bf X}\\): (n,28,28)\n- \\({\\bf y}\\): (n,) , \\(y=0,1,2,3,\\dots,9\\)"
  },
  {
    "objectID": "posts/2022_05_02_(9주차)_5월2일(2).html#예제1",
    "href": "posts/2022_05_02_(9주차)_5월2일(2).html#예제1",
    "title": "[STBDA] 09wk(2): 확률적 경사하강법",
    "section": "예제1",
    "text": "예제1\n\n데이터 정리\n- y=0,1에 대응하는 이미지만 정리하자. (우리가 배운건 로지스틱이니까)\n\ny= y_train[(y_train==0) | (y_train==1)].reshape(-1,1)\nX= x_train[(y_train==0) | (y_train==1)].reshape(-1,784)\nyy= y_test[(y_test==0) | (y_test==1)].reshape(-1,1)\nXX= x_test[(y_test==0) | (y_test==1)].reshape(-1,784)\n\n y_rain==0 | y_rain==1 할때 가로 () 통해서 묶어줘야 컴퓨터가 잘 계산함. 안그러면 오류남.\n\n28*28\n\n784\n\n\n\nX.shape, y.shape, XX.shape, yy.shape\n\n((12000, 784), (12000, 1), (2000, 784), (2000, 1))\n\n\n\n\n풀이1: 은닉층을 포함한 신경망 // epochs=100\n\n#collapse\ngv('''\nsplines=line\nsubgraph cluster_1{\n    style=filled;\n    color=lightgrey;\n    \"x1\"\n    \"x2\"\n    \"..\"\n    \"x784\"\n    label = \"Layer 0\"\n}\nsubgraph cluster_2{\n    style=filled;\n    color=lightgrey;\n    \"x1\" -&gt; \"node1\"\n    \"x2\" -&gt; \"node1\"\n    \"..\" -&gt; \"node1\"\n\n    \"x784\" -&gt; \"node1\"\n    \"x1\" -&gt; \"node2\"\n    \"x2\" -&gt; \"node2\"\n    \"..\" -&gt; \"node2\"\n    \"x784\" -&gt; \"node2\"\n\n    \"x1\" -&gt; \"...\"\n    \"x2\" -&gt; \"...\"\n    \"..\" -&gt; \"...\"\n    \"x784\" -&gt; \"...\"\n\n    \"x1\" -&gt; \"node30\"\n    \"x2\" -&gt; \"node30\"\n    \"..\" -&gt; \"node30\"\n    \"x784\" -&gt; \"node30\"\n\n\n    label = \"Layer 1: relu\"\n}\nsubgraph cluster_3{\n    style=filled;\n    color=lightgrey;\n    \"node1\" -&gt; \"y\"\n    \"node2\" -&gt; \"y\"\n    \"...\" -&gt; \"y\"\n    \"node30\" -&gt; \"y\"\n    label = \"Layer 2: sigmoid\"\n}\n''')\n\n\n\n\n\ntf.random.set_seed(43052)\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(30,activation='relu'))\nnet.add(tf.keras.layers.Dense(1,activation='sigmoid'))\nnet.compile(optimizer='sgd',loss=tf.losses.binary_crossentropy)\nnet.fit(X,y,epochs=100,batch_size=12000)\n\nEpoch 1/100\n1/1 [==============================] - 0s 160ms/step - loss: 92.5370\nEpoch 2/100\n1/1 [==============================] - 0s 10ms/step - loss: 17046.6074\nEpoch 3/100\n1/1 [==============================] - 0s 10ms/step - loss: 1.0171\nEpoch 4/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.7000\nEpoch 5/100\n1/1 [==============================] - 0s 11ms/step - loss: 0.6993\nEpoch 6/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.6987\nEpoch 7/100\n1/1 [==============================] - 0s 11ms/step - loss: 0.6982\nEpoch 8/100\n1/1 [==============================] - 0s 11ms/step - loss: 0.6977\nEpoch 9/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.6973\nEpoch 10/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.6970\nEpoch 11/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.6966\nEpoch 12/100\n1/1 [==============================] - 0s 11ms/step - loss: 0.6963\nEpoch 13/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.6961\nEpoch 14/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.6958\nEpoch 15/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.6956\nEpoch 16/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.6955\nEpoch 17/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.6953\nEpoch 18/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.6952\nEpoch 19/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.6950\nEpoch 20/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.6949\nEpoch 21/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.6948\nEpoch 22/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.6947\nEpoch 23/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.6946\nEpoch 24/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.6945\nEpoch 25/100\n1/1 [==============================] - 0s 11ms/step - loss: 0.6944\nEpoch 26/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.6943\nEpoch 27/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.6942\nEpoch 28/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.6941\nEpoch 29/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.6941\nEpoch 30/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.6940\nEpoch 31/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.6939\nEpoch 32/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.6939\nEpoch 33/100\n1/1 [==============================] - 0s 11ms/step - loss: 0.6938\nEpoch 34/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.6938\nEpoch 35/100\n1/1 [==============================] - 0s 11ms/step - loss: 0.6937\nEpoch 36/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.6937\nEpoch 37/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.6937\nEpoch 38/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.6936\nEpoch 39/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.6936\nEpoch 40/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.6936\nEpoch 41/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.6935\nEpoch 42/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.6935\nEpoch 43/100\n1/1 [==============================] - 0s 11ms/step - loss: 0.6935\nEpoch 44/100\n1/1 [==============================] - 0s 11ms/step - loss: 0.6935\nEpoch 45/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.6934\nEpoch 46/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.6934\nEpoch 47/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.6934\nEpoch 48/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.6934\nEpoch 49/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.6933\nEpoch 50/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.6933\nEpoch 51/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.6933\nEpoch 52/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.6933\nEpoch 53/100\n1/1 [==============================] - 0s 11ms/step - loss: 0.6933\nEpoch 54/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.6933\nEpoch 55/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.6933\nEpoch 56/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.6932\nEpoch 57/100\n1/1 [==============================] - 0s 11ms/step - loss: 0.6932\nEpoch 58/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.6932\nEpoch 59/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.6932\nEpoch 60/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.6932\nEpoch 61/100\n1/1 [==============================] - 0s 11ms/step - loss: 0.6932\nEpoch 62/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.6932\nEpoch 63/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.6932\nEpoch 64/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.6932\nEpoch 65/100\n1/1 [==============================] - 0s 11ms/step - loss: 0.6932\nEpoch 66/100\n1/1 [==============================] - 0s 11ms/step - loss: 0.6932\nEpoch 67/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.6932\nEpoch 68/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.6932\nEpoch 69/100\n1/1 [==============================] - 0s 11ms/step - loss: 0.6932\nEpoch 70/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.6931\nEpoch 71/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.6931\nEpoch 72/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.6931\nEpoch 73/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.6931\nEpoch 74/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.6931\nEpoch 75/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.6931\nEpoch 76/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.6931\nEpoch 77/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.6931\nEpoch 78/100\n1/1 [==============================] - 0s 11ms/step - loss: 0.6931\nEpoch 79/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.6931\nEpoch 80/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.6931\nEpoch 81/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.6931\nEpoch 82/100\n1/1 [==============================] - 0s 11ms/step - loss: 0.6931\nEpoch 83/100\n1/1 [==============================] - 0s 11ms/step - loss: 0.6931\nEpoch 84/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.6931\nEpoch 85/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6931\nEpoch 86/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6931\nEpoch 87/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6931\nEpoch 88/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.6931\nEpoch 89/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6931\nEpoch 90/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6931\nEpoch 91/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6931\nEpoch 92/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6931\nEpoch 93/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6931\nEpoch 94/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6931\nEpoch 95/100\n1/1 [==============================] - 0s 12ms/step - loss: 0.6931\nEpoch 96/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6931\nEpoch 97/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.6930\nEpoch 98/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.6930\nEpoch 99/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6930\nEpoch 100/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.6930\n\n\n&lt;keras.callbacks.History at 0x7fa0ccd71a00&gt;\n\n\n\nnp.mean((net(X)&gt;0.5) == y)\n\n0.5024166666666666\n\n\n\nnp.mean((net(XX)&gt;0.5) == yy)\n\n0.5055\n\n\n 옵티마이저 sgd로 하니까 local-min에 갇힘\n\n\n풀이2: 옵티마이저 개선\n\ntf.random.set_seed(43052)\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(30,activation='relu'))\nnet.add(tf.keras.layers.Dense(1,activation='sigmoid'))\nnet.compile(optimizer='adam',loss=tf.losses.binary_crossentropy)\nnet.fit(X,y,epochs=100,batch_size=12000)\n\nEpoch 1/100\n1/1 [==============================] - 0s 306ms/step - loss: 65.5856\nEpoch 2/100\n1/1 [==============================] - 0s 11ms/step - loss: 19.6951\nEpoch 3/100\n1/1 [==============================] - 0s 11ms/step - loss: 20.4201\nEpoch 4/100\n1/1 [==============================] - 0s 11ms/step - loss: 10.9633\nEpoch 5/100\n1/1 [==============================] - 0s 10ms/step - loss: 6.0247\nEpoch 6/100\n1/1 [==============================] - 0s 11ms/step - loss: 4.8173\nEpoch 7/100\n1/1 [==============================] - 0s 11ms/step - loss: 4.5056\nEpoch 8/100\n1/1 [==============================] - 0s 11ms/step - loss: 4.5642\nEpoch 9/100\n1/1 [==============================] - 0s 11ms/step - loss: 4.8446\nEpoch 10/100\n1/1 [==============================] - 0s 10ms/step - loss: 5.1005\nEpoch 11/100\n1/1 [==============================] - 0s 11ms/step - loss: 5.1322\nEpoch 12/100\n1/1 [==============================] - 0s 11ms/step - loss: 4.8958\nEpoch 13/100\n1/1 [==============================] - 0s 11ms/step - loss: 4.4664\nEpoch 14/100\n1/1 [==============================] - 0s 11ms/step - loss: 3.9710\nEpoch 15/100\n1/1 [==============================] - 0s 11ms/step - loss: 3.5173\nEpoch 16/100\n1/1 [==============================] - 0s 10ms/step - loss: 3.1330\nEpoch 17/100\n1/1 [==============================] - 0s 11ms/step - loss: 2.8294\nEpoch 18/100\n1/1 [==============================] - 0s 11ms/step - loss: 2.5868\nEpoch 19/100\n1/1 [==============================] - 0s 11ms/step - loss: 2.3845\nEpoch 20/100\n1/1 [==============================] - 0s 10ms/step - loss: 2.1961\nEpoch 21/100\n1/1 [==============================] - 0s 10ms/step - loss: 2.0136\nEpoch 22/100\n1/1 [==============================] - 0s 11ms/step - loss: 1.8419\nEpoch 23/100\n1/1 [==============================] - 0s 10ms/step - loss: 1.6893\nEpoch 24/100\n1/1 [==============================] - 0s 11ms/step - loss: 1.5621\nEpoch 25/100\n1/1 [==============================] - 0s 10ms/step - loss: 1.4647\nEpoch 26/100\n1/1 [==============================] - 0s 11ms/step - loss: 1.3956\nEpoch 27/100\n1/1 [==============================] - 0s 11ms/step - loss: 1.3458\nEpoch 28/100\n1/1 [==============================] - 0s 10ms/step - loss: 1.3024\nEpoch 29/100\n1/1 [==============================] - 0s 11ms/step - loss: 1.2631\nEpoch 30/100\n1/1 [==============================] - 0s 11ms/step - loss: 1.2277\nEpoch 31/100\n1/1 [==============================] - 0s 10ms/step - loss: 1.1928\nEpoch 32/100\n1/1 [==============================] - 0s 11ms/step - loss: 1.1540\nEpoch 33/100\n1/1 [==============================] - 0s 11ms/step - loss: 1.1148\nEpoch 34/100\n1/1 [==============================] - 0s 11ms/step - loss: 1.0759\nEpoch 35/100\n1/1 [==============================] - 0s 10ms/step - loss: 1.0349\nEpoch 36/100\n1/1 [==============================] - 0s 11ms/step - loss: 0.9931\nEpoch 37/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.9538\nEpoch 38/100\n1/1 [==============================] - 0s 11ms/step - loss: 0.9176\nEpoch 39/100\n1/1 [==============================] - 0s 11ms/step - loss: 0.8832\nEpoch 40/100\n1/1 [==============================] - 0s 11ms/step - loss: 0.8508\nEpoch 41/100\n1/1 [==============================] - 0s 11ms/step - loss: 0.8201\nEpoch 42/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.7908\nEpoch 43/100\n1/1 [==============================] - 0s 11ms/step - loss: 0.7629\nEpoch 44/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.7365\nEpoch 45/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.7131\nEpoch 46/100\n1/1 [==============================] - 0s 11ms/step - loss: 0.6917\nEpoch 47/100\n1/1 [==============================] - 0s 11ms/step - loss: 0.6717\nEpoch 48/100\n1/1 [==============================] - 0s 11ms/step - loss: 0.6526\nEpoch 49/100\n1/1 [==============================] - 0s 11ms/step - loss: 0.6343\nEpoch 50/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.6159\nEpoch 51/100\n1/1 [==============================] - 0s 11ms/step - loss: 0.5970\nEpoch 52/100\n1/1 [==============================] - 0s 11ms/step - loss: 0.5782\nEpoch 53/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.5602\nEpoch 54/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.5444\nEpoch 55/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.5302\nEpoch 56/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.5173\nEpoch 57/100\n1/1 [==============================] - 0s 11ms/step - loss: 0.5045\nEpoch 58/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.4924\nEpoch 59/100\n1/1 [==============================] - 0s 11ms/step - loss: 0.4807\nEpoch 60/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.4697\nEpoch 61/100\n1/1 [==============================] - 0s 11ms/step - loss: 0.4593\nEpoch 62/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.4494\nEpoch 63/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.4403\nEpoch 64/100\n1/1 [==============================] - 0s 11ms/step - loss: 0.4322\nEpoch 65/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.4244\nEpoch 66/100\n1/1 [==============================] - 0s 11ms/step - loss: 0.4168\nEpoch 67/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.4094\nEpoch 68/100\n1/1 [==============================] - 0s 11ms/step - loss: 0.4019\nEpoch 69/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.3944\nEpoch 70/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.3873\nEpoch 71/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.3810\nEpoch 72/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.3748\nEpoch 73/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.3692\nEpoch 74/100\n1/1 [==============================] - 0s 11ms/step - loss: 0.3635\nEpoch 75/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.3575\nEpoch 76/100\n1/1 [==============================] - 0s 13ms/step - loss: 0.3517\nEpoch 77/100\n1/1 [==============================] - 0s 11ms/step - loss: 0.3460\nEpoch 78/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.3400\nEpoch 79/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.3341\nEpoch 80/100\n1/1 [==============================] - 0s 11ms/step - loss: 0.3284\nEpoch 81/100\n1/1 [==============================] - 0s 11ms/step - loss: 0.3227\nEpoch 82/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.3171\nEpoch 83/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.3120\nEpoch 84/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.3068\nEpoch 85/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.3018\nEpoch 86/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.2970\nEpoch 87/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.2922\nEpoch 88/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.2876\nEpoch 89/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.2831\nEpoch 90/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.2786\nEpoch 91/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.2743\nEpoch 92/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.2702\nEpoch 93/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.2662\nEpoch 94/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.2621\nEpoch 95/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.2581\nEpoch 96/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.2542\nEpoch 97/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.2502\nEpoch 98/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.2464\nEpoch 99/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.2426\nEpoch 100/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.2391\n\n\n&lt;keras.callbacks.History at 0x7fa1234fc580&gt;\n\n\n\nnp.mean((net(X)&gt;0.5) == y)\n\n0.9866666666666667\n\n\n\nnp.mean((net(XX)&gt;0.5) == yy)\n\n0.98\n\n\n\n\n풀이3: 컴파일시 metrics=[‘accuracy’] 추가\n\ntf.random.set_seed(43052)\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(30,activation='relu'))\nnet.add(tf.keras.layers.Dense(1,activation='sigmoid'))\nnet.compile(optimizer='adam',loss=tf.losses.binary_crossentropy,metrics=['accuracy'])\nnet.fit(X,y,epochs=100,batch_size=12000)\n\nEpoch 1/100\n1/1 [==============================] - 0s 242ms/step - loss: 28.7517 - accuracy: 0.5519\nEpoch 2/100\n1/1 [==============================] - 0s 11ms/step - loss: 25.1717 - accuracy: 0.6363\nEpoch 3/100\n1/1 [==============================] - 0s 11ms/step - loss: 21.2647 - accuracy: 0.6923\nEpoch 4/100\n1/1 [==============================] - 0s 11ms/step - loss: 9.8526 - accuracy: 0.8635\nEpoch 5/100\n1/1 [==============================] - 0s 11ms/step - loss: 5.4689 - accuracy: 0.9360\nEpoch 6/100\n1/1 [==============================] - 0s 11ms/step - loss: 4.2996 - accuracy: 0.9312\nEpoch 7/100\n1/1 [==============================] - 0s 12ms/step - loss: 5.0362 - accuracy: 0.9087\nEpoch 8/100\n1/1 [==============================] - 0s 12ms/step - loss: 6.4399 - accuracy: 0.8825\nEpoch 9/100\n1/1 [==============================] - 0s 11ms/step - loss: 6.9378 - accuracy: 0.8757\nEpoch 10/100\n1/1 [==============================] - 0s 14ms/step - loss: 6.0575 - accuracy: 0.8892\nEpoch 11/100\n1/1 [==============================] - 0s 10ms/step - loss: 4.6225 - accuracy: 0.9107\nEpoch 12/100\n1/1 [==============================] - 0s 11ms/step - loss: 3.4159 - accuracy: 0.9296\nEpoch 13/100\n1/1 [==============================] - 0s 12ms/step - loss: 2.6765 - accuracy: 0.9417\nEpoch 14/100\n1/1 [==============================] - 0s 12ms/step - loss: 2.3759 - accuracy: 0.9516\nEpoch 15/100\n1/1 [==============================] - 0s 12ms/step - loss: 2.3544 - accuracy: 0.9558\nEpoch 16/100\n1/1 [==============================] - 0s 14ms/step - loss: 2.3973 - accuracy: 0.9572\nEpoch 17/100\n1/1 [==============================] - 0s 11ms/step - loss: 2.4026 - accuracy: 0.9578\nEpoch 18/100\n1/1 [==============================] - 0s 11ms/step - loss: 2.3559 - accuracy: 0.9589\nEpoch 19/100\n1/1 [==============================] - 0s 11ms/step - loss: 2.2520 - accuracy: 0.9599\nEpoch 20/100\n1/1 [==============================] - 0s 12ms/step - loss: 2.1145 - accuracy: 0.9616\nEpoch 21/100\n1/1 [==============================] - 0s 11ms/step - loss: 1.9729 - accuracy: 0.9613\nEpoch 22/100\n1/1 [==============================] - 0s 11ms/step - loss: 1.8410 - accuracy: 0.9632\nEpoch 23/100\n1/1 [==============================] - 0s 11ms/step - loss: 1.7298 - accuracy: 0.9646\nEpoch 24/100\n1/1 [==============================] - 0s 11ms/step - loss: 1.6384 - accuracy: 0.9661\nEpoch 25/100\n1/1 [==============================] - 0s 11ms/step - loss: 1.5576 - accuracy: 0.9658\nEpoch 26/100\n1/1 [==============================] - 0s 11ms/step - loss: 1.4781 - accuracy: 0.9672\nEpoch 27/100\n1/1 [==============================] - 0s 11ms/step - loss: 1.4088 - accuracy: 0.9688\nEpoch 28/100\n1/1 [==============================] - 0s 11ms/step - loss: 1.3492 - accuracy: 0.9697\nEpoch 29/100\n1/1 [==============================] - 0s 11ms/step - loss: 1.2919 - accuracy: 0.9703\nEpoch 30/100\n1/1 [==============================] - 0s 11ms/step - loss: 1.2393 - accuracy: 0.9712\nEpoch 31/100\n1/1 [==============================] - 0s 11ms/step - loss: 1.2040 - accuracy: 0.9722\nEpoch 32/100\n1/1 [==============================] - 0s 12ms/step - loss: 1.1795 - accuracy: 0.9725\nEpoch 33/100\n1/1 [==============================] - 0s 11ms/step - loss: 1.1721 - accuracy: 0.9730\nEpoch 34/100\n1/1 [==============================] - 0s 11ms/step - loss: 1.1705 - accuracy: 0.9735\nEpoch 35/100\n1/1 [==============================] - 0s 11ms/step - loss: 1.1678 - accuracy: 0.9737\nEpoch 36/100\n1/1 [==============================] - 0s 11ms/step - loss: 1.1590 - accuracy: 0.9744\nEpoch 37/100\n1/1 [==============================] - 0s 11ms/step - loss: 1.1432 - accuracy: 0.9747\nEpoch 38/100\n1/1 [==============================] - 0s 11ms/step - loss: 1.1207 - accuracy: 0.9748\nEpoch 39/100\n1/1 [==============================] - 0s 12ms/step - loss: 1.0926 - accuracy: 0.9751\nEpoch 40/100\n1/1 [==============================] - 0s 12ms/step - loss: 1.0612 - accuracy: 0.9751\nEpoch 41/100\n1/1 [==============================] - 0s 11ms/step - loss: 1.0301 - accuracy: 0.9752\nEpoch 42/100\n1/1 [==============================] - 0s 12ms/step - loss: 1.0011 - accuracy: 0.9756\nEpoch 43/100\n1/1 [==============================] - 0s 12ms/step - loss: 0.9761 - accuracy: 0.9757\nEpoch 44/100\n1/1 [==============================] - 0s 11ms/step - loss: 0.9521 - accuracy: 0.9756\nEpoch 45/100\n1/1 [==============================] - 0s 11ms/step - loss: 0.9306 - accuracy: 0.9759\nEpoch 46/100\n1/1 [==============================] - 0s 14ms/step - loss: 0.9121 - accuracy: 0.9764\nEpoch 47/100\n1/1 [==============================] - 0s 11ms/step - loss: 0.8953 - accuracy: 0.9764\nEpoch 48/100\n1/1 [==============================] - 0s 11ms/step - loss: 0.8787 - accuracy: 0.9768\nEpoch 49/100\n1/1 [==============================] - 0s 12ms/step - loss: 0.8623 - accuracy: 0.9773\nEpoch 50/100\n1/1 [==============================] - 0s 12ms/step - loss: 0.8471 - accuracy: 0.9779\nEpoch 51/100\n1/1 [==============================] - 0s 13ms/step - loss: 0.8341 - accuracy: 0.9780\nEpoch 52/100\n1/1 [==============================] - 0s 12ms/step - loss: 0.8243 - accuracy: 0.9783\nEpoch 53/100\n1/1 [==============================] - 0s 12ms/step - loss: 0.8158 - accuracy: 0.9783\nEpoch 54/100\n1/1 [==============================] - 0s 11ms/step - loss: 0.8080 - accuracy: 0.9784\nEpoch 55/100\n1/1 [==============================] - 0s 12ms/step - loss: 0.8007 - accuracy: 0.9786\nEpoch 56/100\n1/1 [==============================] - 0s 12ms/step - loss: 0.7927 - accuracy: 0.9786\nEpoch 57/100\n1/1 [==============================] - 0s 12ms/step - loss: 0.7835 - accuracy: 0.9785\nEpoch 58/100\n1/1 [==============================] - 0s 12ms/step - loss: 0.7740 - accuracy: 0.9787\nEpoch 59/100\n1/1 [==============================] - 0s 12ms/step - loss: 0.7644 - accuracy: 0.9790\nEpoch 60/100\n1/1 [==============================] - 0s 12ms/step - loss: 0.7538 - accuracy: 0.9789\nEpoch 61/100\n1/1 [==============================] - 0s 12ms/step - loss: 0.7426 - accuracy: 0.9790\nEpoch 62/100\n1/1 [==============================] - 0s 12ms/step - loss: 0.7315 - accuracy: 0.9792\nEpoch 63/100\n1/1 [==============================] - 0s 12ms/step - loss: 0.7220 - accuracy: 0.9793\nEpoch 64/100\n1/1 [==============================] - 0s 12ms/step - loss: 0.7144 - accuracy: 0.9797\nEpoch 65/100\n1/1 [==============================] - 0s 12ms/step - loss: 0.7080 - accuracy: 0.9793\nEpoch 66/100\n1/1 [==============================] - 0s 12ms/step - loss: 0.7017 - accuracy: 0.9795\nEpoch 67/100\n1/1 [==============================] - 0s 12ms/step - loss: 0.6953 - accuracy: 0.9794\nEpoch 68/100\n1/1 [==============================] - 0s 13ms/step - loss: 0.6886 - accuracy: 0.9797\nEpoch 69/100\n1/1 [==============================] - 0s 12ms/step - loss: 0.6818 - accuracy: 0.9796\nEpoch 70/100\n1/1 [==============================] - 0s 12ms/step - loss: 0.6746 - accuracy: 0.9799\nEpoch 71/100\n1/1 [==============================] - 0s 12ms/step - loss: 0.6672 - accuracy: 0.9803\nEpoch 72/100\n1/1 [==============================] - 0s 12ms/step - loss: 0.6599 - accuracy: 0.9808\nEpoch 73/100\n1/1 [==============================] - 0s 12ms/step - loss: 0.6528 - accuracy: 0.9810\nEpoch 74/100\n1/1 [==============================] - 0s 12ms/step - loss: 0.6458 - accuracy: 0.9812\nEpoch 75/100\n1/1 [==============================] - 0s 12ms/step - loss: 0.6391 - accuracy: 0.9813\nEpoch 76/100\n1/1 [==============================] - 0s 12ms/step - loss: 0.6325 - accuracy: 0.9814\nEpoch 77/100\n1/1 [==============================] - 0s 12ms/step - loss: 0.6259 - accuracy: 0.9815\nEpoch 78/100\n1/1 [==============================] - 0s 12ms/step - loss: 0.6195 - accuracy: 0.9817\nEpoch 79/100\n1/1 [==============================] - 0s 12ms/step - loss: 0.6130 - accuracy: 0.9817\nEpoch 80/100\n1/1 [==============================] - 0s 12ms/step - loss: 0.6068 - accuracy: 0.9818\nEpoch 81/100\n1/1 [==============================] - 0s 12ms/step - loss: 0.6006 - accuracy: 0.9820\nEpoch 82/100\n1/1 [==============================] - 0s 11ms/step - loss: 0.5945 - accuracy: 0.9820\nEpoch 83/100\n1/1 [==============================] - 0s 12ms/step - loss: 0.5883 - accuracy: 0.9822\nEpoch 84/100\n1/1 [==============================] - 0s 12ms/step - loss: 0.5822 - accuracy: 0.9822\nEpoch 85/100\n1/1 [==============================] - 0s 11ms/step - loss: 0.5762 - accuracy: 0.9824\nEpoch 86/100\n1/1 [==============================] - 0s 11ms/step - loss: 0.5701 - accuracy: 0.9826\nEpoch 87/100\n1/1 [==============================] - 0s 11ms/step - loss: 0.5638 - accuracy: 0.9829\nEpoch 88/100\n1/1 [==============================] - 0s 11ms/step - loss: 0.5576 - accuracy: 0.9829\nEpoch 89/100\n1/1 [==============================] - 0s 11ms/step - loss: 0.5515 - accuracy: 0.9833\nEpoch 90/100\n1/1 [==============================] - 0s 12ms/step - loss: 0.5455 - accuracy: 0.9836\nEpoch 91/100\n1/1 [==============================] - 0s 12ms/step - loss: 0.5396 - accuracy: 0.9833\nEpoch 92/100\n1/1 [==============================] - 0s 11ms/step - loss: 0.5340 - accuracy: 0.9829\nEpoch 93/100\n1/1 [==============================] - 0s 11ms/step - loss: 0.5285 - accuracy: 0.9830\nEpoch 94/100\n1/1 [==============================] - 0s 11ms/step - loss: 0.5229 - accuracy: 0.9834\nEpoch 95/100\n1/1 [==============================] - 0s 11ms/step - loss: 0.5174 - accuracy: 0.9834\nEpoch 96/100\n1/1 [==============================] - 0s 12ms/step - loss: 0.5119 - accuracy: 0.9836\nEpoch 97/100\n1/1 [==============================] - 0s 12ms/step - loss: 0.5064 - accuracy: 0.9837\nEpoch 98/100\n1/1 [==============================] - 0s 12ms/step - loss: 0.5008 - accuracy: 0.9840\nEpoch 99/100\n1/1 [==============================] - 0s 12ms/step - loss: 0.4953 - accuracy: 0.9841\nEpoch 100/100\n1/1 [==============================] - 0s 11ms/step - loss: 0.4900 - accuracy: 0.9846\n\n\n&lt;keras.callbacks.History at 0x7fa0cccc07f0&gt;\n\n\n\nnet.evaluate(X,y)\n\n375/375 [==============================] - 0s 402us/step - loss: 0.4845 - accuracy: 0.9849\n\n\n[0.48451268672943115, 0.9849166870117188]\n\n\n\nnet.evaluate(XX,yy)\n\n63/63 [==============================] - 0s 420us/step - loss: 0.6619 - accuracy: 0.9755\n\n\n[0.6618630886077881, 0.9754999876022339]\n\n\n\n\n풀이4: 확률적경사하강법 이용 // epochs=10\n\ntf.random.set_seed(43052)\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(30,activation='relu'))\nnet.add(tf.keras.layers.Dense(1,activation='sigmoid'))\nnet.compile(optimizer='adam',loss=tf.losses.binary_crossentropy,metrics=['accuracy'])\nnet.fit(X,y,epochs=10,batch_size=120)\n\nEpoch 1/10\n100/100 [==============================] - 0s 807us/step - loss: 1.8125 - accuracy: 0.9566\nEpoch 2/10\n100/100 [==============================] - 0s 633us/step - loss: 0.5900 - accuracy: 0.9791\nEpoch 3/10\n100/100 [==============================] - 0s 634us/step - loss: 0.4965 - accuracy: 0.9822\nEpoch 4/10\n100/100 [==============================] - 0s 605us/step - loss: 0.3176 - accuracy: 0.9849\nEpoch 5/10\n100/100 [==============================] - 0s 629us/step - loss: 0.1967 - accuracy: 0.9896\nEpoch 6/10\n100/100 [==============================] - 0s 631us/step - loss: 0.2400 - accuracy: 0.9872\nEpoch 7/10\n100/100 [==============================] - 0s 606us/step - loss: 0.1644 - accuracy: 0.9902\nEpoch 8/10\n100/100 [==============================] - 0s 602us/step - loss: 0.1564 - accuracy: 0.9903\nEpoch 9/10\n100/100 [==============================] - 0s 615us/step - loss: 0.1128 - accuracy: 0.9931\nEpoch 10/10\n100/100 [==============================] - 0s 627us/step - loss: 0.1091 - accuracy: 0.9928\n\n\n&lt;keras.callbacks.History at 0x7fa0ccb212b0&gt;\n\n\n\nnet.evaluate(X,y)\n\n375/375 [==============================] - 0s 379us/step - loss: 0.0361 - accuracy: 0.9956\n\n\n[0.036105770617723465, 0.9955833554267883]\n\n\n\nnet.evaluate(XX,yy)\n\n63/63 [==============================] - 0s 442us/step - loss: 0.2614 - accuracy: 0.9870\n\n\n[0.2613808810710907, 0.9869999885559082]"
  },
  {
    "objectID": "posts/2022_04_11_(6주차)_4월11일.html",
    "href": "posts/2022_04_11_(6주차)_4월11일.html",
    "title": "[STBDA] 06wk: keras",
    "section": "",
    "text": "해당 강의노트는 전북대학교 최규빈교수님 STBDA2022 자료임\n\n\nimports\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport tensorflow.experimental.numpy as tnp\n\n2023-06-23 14:44:48.422443: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n\n\n\ntnp.experimental_enable_numpy_behavior()\n\n\nimport graphviz\ndef gv(s): return graphviz.Source('digraph G{ rankdir=\"LR\"'+s + '; }')\n\n\n\n\\(x \\to \\hat{y}\\) 가 되는 과정을 그림으로 그리기\n- 단순회귀분석의 예시 - \\(\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i, \\quad i=1,2,\\dots,n\\)\n(표현1)\n\n#collapse\ngv('''\n    \"1\" -&gt; \"β̂₀ + xₙ*β̂₁,    bias=False\"[label=\"* β̂₀\"]\n    \"xₙ\" -&gt; \"β̂₀ + xₙ*β̂₁,    bias=False\"[label=\"* β̂₁\"]\n    \"β̂₀ + xₙ*β̂₁,    bias=False\" -&gt; \"ŷₙ\"[label=\"identity\"]\n\n    \".\" -&gt; \"....................................\"[label=\"* β̂₀\"]\n    \"..\" -&gt; \"....................................\"[label=\"* β̂₁\"]\n    \"....................................\" -&gt; \"...\"[label=\" \"]\n\n    \"1 \" -&gt; \"β̂₀ + x₂*β̂₁,    bias=False\"[label=\"* β̂₀\"]\n    \"x₂\" -&gt; \"β̂₀ + x₂*β̂₁,    bias=False\"[label=\"* β̂₁\"]\n    \"β̂₀ + x₂*β̂₁,    bias=False\" -&gt; \"ŷ₂\"[label=\"identity\"]\n\n    \"1  \" -&gt; \"β̂₀ + x₁*β̂₁,    bias=False\"[label=\"* β̂₀\"]\n    \"x₁\" -&gt; \"β̂₀ + x₁*β̂₁,    bias=False\"[label=\"* β̂₁\"]\n    \"β̂₀ + x₁*β̂₁,    bias=False\" -&gt; \"ŷ₁\"[label=\"identity\"]\n''')\n\n\n\n\n- 표현1의 소감? - 교수님이 고생해서 만든것 같음 - 그런데 그냥 다 똑같은 그림의 반복이라 사실 고생한 의미가 없음.\n(표현2)\n- 그냥 아래와 같이 그리고 “모든 \\(i=1,2,3,\\dots,n\\)에 대하여 \\(\\hat{y}_i\\)을 아래의 그림과 같이 그린다”고 하면 될것 같다.\n\n#collapse\ngv('''\n    \"1\" -&gt; \"β̂₀ + xᵢ*β̂₁,    bias=False\"[label=\"* β̂₀\"]\n    \"xᵢ\" -&gt; \"β̂₀ + xᵢ*β̂₁,    bias=False\"[label=\"* β̂₁\"]\n    \"β̂₀ + xᵢ*β̂₁,    bias=False\" -&gt; \"ŷᵢ\"[label=\"identity\"]\n\n''')\n\n\n\n\n(표현3)\n- 그런데 “모든 \\(i=1,2,3,\\dots,n\\)에 대하여 \\(\\hat{y}_i\\)을 아래의 그림과 같이 그린다” 라는 언급자체도 반복할 필요가 없을 것 같다. (어차피 당연히 그럴테니까) 그래서 단순히 아래와 같이 그려도 무방할듯 하다.\n\ngv('''\n    \"1\" -&gt; \"β̂₀ + x*β̂₁,    bias=False\"[label=\"* β̂₀\"]\n    \"x\" -&gt; \"β̂₀ + x*β̂₁,    bias=False\"[label=\"* β̂₁\"]\n    \"β̂₀ + x*β̂₁,    bias=False\" -&gt; \"ŷ\"[label=\"identity\"]\n\n''')\n\n\n\n\n(표현4)\n- 위의 모델은 아래와 같이 쓸 수 있다. (\\(\\beta_0\\)를 바이어스로 표현)\n\n#collapse\ngv('''\n\"x\" -&gt; \"x*β̂₁,    bias=True\"[label=\"*β̂₁\"] ;\n\"x*β̂₁,    bias=True\" -&gt; \"ŷ\"[label=\"indentity\"] ''')\n\n\n\n\n\n실제로는 이 표현을 많이 사용함\n\n(표현5)\n- 벡터버전으로 표현하면 아래와 같다. 이 경우에는 \\({\\bf X}=[1,x]\\)에 포함된 1이 bias의 역할을 해주므로 bias = False 임.\n\n#collapse\ngv('''\n\"X\" -&gt; \"X@β̂,    bias=False\"[label=\"@β̂\"] ;\n\"X@β̂,    bias=False\" -&gt; \"ŷ\"[label=\"indentity\"] ''')\n\n\n\n\n\n저는 이걸 좋아해요\n\n(표현6)\n- 딥러닝에서는 \\(\\hat{\\boldsymbol{\\beta}}\\) 대신에 \\(\\hat{{\\bf W}}\\)을 라고 표현한다.\n\n#collapse\ngv('''\n\"X\" -&gt; \"X@Ŵ,    bias=False\"[label=\"@Ŵ\"] ;\n\"X@Ŵ,    bias=False\" -&gt; \"ŷ\"[label=\"identity\"] ''')\n\n\n\n\n- 실제로는 표현4 혹은 표현5를 외우면 된다.\n\n\nLayer의 개념\n- (표현4) 혹은 (표현5)의 그림은 레이어로 설명할 수 있다.\n- 레이어는 항상 아래와 같은 규칙을 가진다. - 첫 동그라미는 레이어의 입력이다. - 첫번째 화살표는 선형변환을 의미한다. - 두번째 동그라미는 선형변환의 결과이다. (이때 bias가 false인지 true인지에 따라서 실제 수식이 조금 다름) - 두번째 화살표는 두번째 동그라미에 어떠한 함수 \\(f\\)를 취하는 과정을 의미한다. (우리의 그림에서는 \\(f(x)=x\\)) - 세번째 동그라미는 레이어의 최종출력이다.\n- 엄청 복잡한데, 결국 레이어를 만들때 위의 그림들을 의미하도록 하려면 아래의 4개의 요소만 필요하다. 1. 레이어의 입력차원 2. 선형변환의 결과로 얻어지는 차원 3. 선형변환에서 바이어스를 쓸지? 안쓸지? 4. 함수 \\(f\\)\n- 주목: 1,2가 결정되면 자동으로 \\(\\hat{{\\bf W}}\\)의 차원이 결정된다.\n(예시) - 레이어의 입력차원=2, 선형변환의 결과로 얻어지는 차원=1: \\(\\hat{\\bf W}\\)는 (2,1) 매트릭스 - 레이어의 입력차원=20, 선형변환의 결과로 얻어지는 차원=5: \\(\\hat{\\bf W}\\)는 (20,5) 매트릭스 - 레이어의 입력차원=2, 선형변환의 결과로 얻어지는 차원=50: \\(\\hat{\\bf W}\\)는 (2,50) 매트릭스\n- 주목2: 이중에서 절대 생략불가능 것은 “2. 선형변환의 결과로 얻어지는 차원” 이다. - 레이어의 입력차원: 실제 레이어에 데이터가 들어올 때 데이터의 입력차원을 컴퓨터 스스로 체크하여 \\(\\hat{\\bf W}\\)의 차원을 결정할 수 있음. - 바이어스를 쓸지? 안쓸지? 기본적으로 쓴다고 가정한다. - 함수 \\(f\\): 기본적으로 항등함수를 가정하면 된다.\n\n\nKeras를 이용한 풀이\n- 기본뼈대: net생성 \\(\\to\\) add(layer) \\(\\to\\) compile(opt,loss) \\(\\to\\) fit(data,epochs)\n- 데이터정리\n\\[{\\bf y}\\approx 2.5 +4*x\\]\n\ntnp.random.seed(43052)\nN= 200\nx= tnp.linspace(0,1,N)\nepsilon= tnp.random.randn(N)*0.5\ny= 2.5+4*x +epsilon\n\n\nX=tf.stack([tf.ones(N,dtype='float64'),x],axis=1)\n\n\n풀이1: 스칼라버전\n(0단계) 데이터정리\n\ny=y.reshape(N,1)\nx=x.reshape(N,1)\nx.shape,y.shape\n\n(TensorShape([200, 1]), TensorShape([200, 1]))\n\n\n(1단계) net 생성\n\nnet = tf.keras.Sequential()\n\n(2단계) net.add(layer)\n\nlayer = tf.keras.layers.Dense(1)\n# 입력차원? 데이터를 넣어보고 결정, 바이어스=디폴드값을 쓰겠음 (use_bias=true), 함수도 디폴트값을 쓰겠음 (f(x)=x)\nnet.add(layer)\n\n(3단계) net.compile(opt,loss_fn)\n\nnet.compile(tf.keras.optimizers.SGD(0.1), tf.keras.losses.MSE)\n\n(4단계) net.fit(x,y,epochs)\n\nnet.fit(x,y,epochs=1000,verbose=0,batch_size=N) # batch_size=N 일 경우에 경사하강법이 적용, batch_size!=N 이면 확률적 경사하강법 적용\n\n&lt;keras.callbacks.History at 0x7f0750a5f550&gt;\n\n\n(결과확인)\n\nnet.weights\n\n[&lt;tf.Variable 'dense/kernel:0' shape=(1, 1) dtype=float32, numpy=array([[3.9330256]], dtype=float32)&gt;,\n &lt;tf.Variable 'dense/bias:0' shape=(1,) dtype=float32, numpy=array([2.583672], dtype=float32)&gt;]\n\n\n\n\n풀이2: 벡터버전\n(0단계) 데이터정리\n\nX.shape,y.shape\n\n(TensorShape([200, 2]), TensorShape([200, 1]))\n\n\n(1단계) net 생성\n\nnet = tf.keras.Sequential()\n\n(2단계) net.add(layer)\n\nlayer = tf.keras.layers.Dense(1,use_bias=False)\nnet.add(layer)\n\n(3단계) net.compile(opt,loss_fn)\n\nnet.compile(tf.keras.optimizers.SGD(0.1), tf.keras.losses.MSE)\n\n(4단계) net.fit(x,y,epochs)\n\nnet.fit(X,y,epochs=1000,verbose=0,batch_size=N) # batch_size=N 일 경우에 경사하강법이 적용, batch_size!=N 이면 확률적 경사하강법 적용\n\n&lt;keras.callbacks.History at 0x7f07507fb070&gt;\n\n\n(결과확인)\n\nnet.weights\n\n[&lt;tf.Variable 'dense_1/kernel:0' shape=(2, 1) dtype=float32, numpy=\n array([[2.5836723],\n        [3.9330251]], dtype=float32)&gt;]\n\n\n\n\n잠시문법정리\n- 잠깐 Dense layer를 만드는 코드를 정리해보자.\n\n아래는 모두 같은 코드이다.\n\n\ntf.keras.layers.Dense(1)\ntf.keras.layers.Dense(units=1)\ntf.keras.layers.Dense(units=1,activation=‘linear’) // identity 가 더 맞는것 같은데..\ntf.keras.layers.Dense(units=1,activation=‘linear’,use_bias=True)\n\n\n아래의 코드1,2는 (1)의 코드들과 살짝 다른코드이다. (코드1과 코드2는 같은코드임)\n\n\ntf.keras.layers.Dense(1,input_dim=2) # 코드1\ntf.keras.layers.Dense(1,input_shape=(2,)) # 코드2\n\n\n아래는 사용불가능한 코드이다.\n\n\ntf.keras.layers.Dense(1,input_dim=(2,)) # 코드1\ntf.keras.layers.Dense(1,input_shape=2) # 코드2\n\n- 왜 input_dim이 필요한가?\n\nnet1 = tf.keras.Sequential()\nnet1.add(tf.keras.layers.Dense(1,use_bias=False))\n\n\nnet2 = tf.keras.Sequential()\nnet2.add(tf.keras.layers.Dense(1,use_bias=False,input_dim=2))\n\n\nnet1.weights\n\nValueError: Weights for model sequential_2 have not yet been created. Weights are created when the Model is first called on inputs or `build()` is called with an `input_shape`.\n\n\n\nnet2.weights\n\n[&lt;tf.Variable 'dense_3/kernel:0' shape=(2, 1) dtype=float32, numpy=\n array([[0.8638755],\n        [1.1012684]], dtype=float32)&gt;]\n\n\n\nnet1.summary()\n\nValueError: This model has not yet been built. Build the model first by calling `build()` or by calling the model on a batch of data.\n\n\n\nnet2.summary()\n\nModel: \"sequential_3\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense_3 (Dense)             (None, 1)                 2         \n                                                                 \n=================================================================\nTotal params: 2\nTrainable params: 2\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\n\n풀이3: 스칼라버전, 임의의 초기값을 설정\n(0단계) 데이터정리\n\ny=y.reshape(N,1)\nx=x.reshape(N,1)\nx.shape,y.shape\n\n(TensorShape([200, 1]), TensorShape([200, 1]))\n\n\n(1단계) net생성\n\nnet = tf.keras.Sequential()\n\n(2단계) net.add(layer)\n\nlayer = tf.keras.layers.Dense(1,input_dim=1)\n\n\nnet.add(layer)\n\n\n초기값을 설정\n\nnet.weights\n\n[&lt;tf.Variable 'dense_4/kernel:0' shape=(1, 1) dtype=float32, numpy=array([[0.31969225]], dtype=float32)&gt;,\n &lt;tf.Variable 'dense_4/bias:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)&gt;]\n\n\n\ninput_dim=1로 설정해서 weights값이 찍힘\n\n\nnet.get_weights()\n\n[array([[0.31969225]], dtype=float32), array([0.], dtype=float32)]\n\n\n\nweight, bias순으로 출력\n\n\nnet.set_weights?\n\n\nSignature: net.set_weights(weights)\nDocstring:\nSets the weights of the layer, from NumPy arrays.\nThe weights of a layer represent the state of the layer. This function\nsets the weight values from numpy arrays. The weight values should be\npassed in the order they are created by the layer. Note that the layer's\nweights must be instantiated before calling this function, by calling\nthe layer.\nFor example, a `Dense` layer returns a list of two values: the kernel\nmatrix and the bias vector. These can be used to set the weights of\nanother `Dense` layer:\n&gt;&gt;&gt; layer_a = tf.keras.layers.Dense(1,\n...   kernel_initializer=tf.constant_initializer(1.))\n&gt;&gt;&gt; a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]]))\n&gt;&gt;&gt; layer_a.get_weights()\n[array([[1.],\n       [1.],\n       [1.]], dtype=float32), array([0.], dtype=float32)]\n&gt;&gt;&gt; layer_b = tf.keras.layers.Dense(1,\n...   kernel_initializer=tf.constant_initializer(2.))\n&gt;&gt;&gt; b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]]))\n&gt;&gt;&gt; layer_b.get_weights()\n[array([[2.],\n       [2.],\n       [2.]], dtype=float32), array([0.], dtype=float32)]\n&gt;&gt;&gt; layer_b.set_weights(layer_a.get_weights())\n&gt;&gt;&gt; layer_b.get_weights()\n[array([[1.],\n       [1.],\n       [1.]], dtype=float32), array([0.], dtype=float32)]\nArgs:\n  weights: a list of NumPy arrays. The number\n    of arrays and their shape must match\n    number of the dimensions of the weights\n    of the layer (i.e. it should match the\n    output of `get_weights`).\nRaises:\n  ValueError: If the provided weights list does not match the\n    layer's specifications.\nFile:      ~/anaconda3/envs/py38/lib/python3.8/site-packages/keras/engine/base_layer.py\nType:      method\n\n\n\n\nlayer_b.set_weights(layer_a.get_weights()) 와 같은방식으로 쓴다는 것이군?\n\n- 한번따라해보자.\n\n_w = net.get_weights()\n_w\n\n[array([[0.31969225]], dtype=float32), array([0.], dtype=float32)]\n\n\n\ntype(_w)\n\nlist\n\n\n\n길이가 2인 리스트이고, 각 원소는 numpy array 임\n\n\nnet.set_weights(\n    [np.array([[10.0]],dtype=np.float32), # weight, β1_hat\n     np.array([-5.0],dtype=np.float32)] # bias, β0_hat\n)\n\n\nnet.weights\n\n[&lt;tf.Variable 'dense_4/kernel:0' shape=(1, 1) dtype=float32, numpy=array([[10.]], dtype=float32)&gt;,\n &lt;tf.Variable 'dense_4/bias:0' shape=(1,) dtype=float32, numpy=array([-5.], dtype=float32)&gt;]\n\n\n\n(3단계) net.compile()\n\nnet.compile(tf.keras.optimizers.SGD(0.1),tf.losses.MSE)\n\n(4단계) net.fit()\n\nnet.fit(x,y,epochs=1000,verbose=0,batch_size=N)\n\n&lt;keras.callbacks.History at 0x7f06014dd460&gt;\n\n\n결과확인\n\nnet.weights\n\n[&lt;tf.Variable 'dense_4/kernel:0' shape=(1, 1) dtype=float32, numpy=array([[3.933048]], dtype=float32)&gt;,\n &lt;tf.Variable 'dense_4/bias:0' shape=(1,) dtype=float32, numpy=array([2.58366], dtype=float32)&gt;]\n\n\n\n\n풀이4: 벡터버전, 임의의 초기값을 설정\n(0단계) 데이터정리\n\nX.shape, y.shape\n\n(TensorShape([200, 2]), TensorShape([200, 1]))\n\n\n(1단계) net생성\n\nnet = tf.keras.Sequential()\n\n(2단계) net.add(layer)\n\nlayer = tf.keras.layers.Dense(1,use_bias=False,input_dim=2)\n\n\n위으 X.shape이 [200,2]니까 input_dim=2 로 해준다.\n\n\nnet.add(layer)\n\n\n초기값을 설정하자\n\nnet.set_weights([np.array([[ -5.0],[10.0]], dtype=np.float32)])\n\n\nnet.get_weights()\n\n[array([[-5.],\n        [10.]], dtype=float32)]\n\n\n\n(3단계) net.compile()\n\nnet.compile(tf.keras.optimizers.SGD(0.1), tf.losses.MSE)\n\n(4단계) net.fit()\n\nnet.fit(X,y,epochs=1000,verbose=0,batch_size=N)\n\n&lt;keras.callbacks.History at 0x7f05ebe84f10&gt;\n\n\n\nnet.weights\n\n[&lt;tf.Variable 'dense_5/kernel:0' shape=(2, 1) dtype=float32, numpy=\n array([[2.58366 ],\n        [3.933048]], dtype=float32)&gt;]\n\n\n- 사실 실전에서는 초기값을 설정할 필요가 별로 없음.\n\n\n풀이5: 벡터버전 사용자정의 손실함수\n(0단계) 데이터정리\n\nX.shape, y.shape\n\n(TensorShape([200, 2]), TensorShape([200, 1]))\n\n\n(1단계) net생성\n\nnet = tf.keras.Sequential()\n\n(2단계) net.add(layer)\n\nlayer = tf.keras.layers.Dense(1,use_bias=False)\n\n\nnet.add(layer)\n\n(3단계) net.compile()\n\nloss_fn = lambda y,yhat: (y-yhat).T @ (y-yhat) / N\n\n\nnet.compile(tf.keras.optimizers.SGD(0.1), loss_fn)\n\n(4단계) net.fit()\n\nnet.fit(X,y,epochs=1000,verbose=0,batch_size=N)\n\nWARNING:tensorflow:From /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\nInstructions for updating:\nLambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n\n\n&lt;keras.callbacks.History at 0x7f05ebe799d0&gt;\n\n\n\nnet.weights\n\n[&lt;tf.Variable 'dense_6/kernel:0' shape=(2, 1) dtype=float32, numpy=\n array([[2.5836723],\n        [3.9330251]], dtype=float32)&gt;]\n\n\n\n\n풀이6: 벡터버전, net.compile의 옵션으로 손실함수 지정\n(0단계) 데이터정리\n\nX.shape, y.shape\n\n(TensorShape([200, 2]), TensorShape([200, 1]))\n\n\n(1단계) net생성\n\nnet = tf.keras.Sequential()\n\n(2단계) net.add(layer)\n\nnet.add(tf.keras.layers.Dense(1,use_bias=False))\n\n(3단계) net.compile()\n\nnet.compile(tf.keras.optimizers.SGD(0.1), loss='mse')\n\n(4단계) net.fit()\n\nnet.fit(X,y,epochs=1000,verbose=0,batch_size=N)\n\n&lt;keras.callbacks.History at 0x7f05ebf1e4c0&gt;\n\n\n\nnet.weights\n\n[&lt;tf.Variable 'dense_7/kernel:0' shape=(2, 1) dtype=float32, numpy=\n array([[2.5836723],\n        [3.9330251]], dtype=float32)&gt;]\n\n\n\n\n풀이7: 벡터버전, net.compile의 옵션으로 손실함수 지정 + 옵티마이저 지정\n(0단계) 데이터정리\n\nX.shape, y.shape\n\n(TensorShape([200, 2]), TensorShape([200, 1]))\n\n\n(1단계) net생성\n\nnet = tf.keras.Sequential()\n\n(2단계) net.add(layer)\n\nnet.add(tf.keras.layers.Dense(1,use_bias=False))\n\n(3단계) net.compile()\n\nnet.compile(optimizer='sgd', loss='mse')\n#net.optimizer.lr = tf.Variable(0.1,dtype=tf.float32)\n#net.optimizer.lr = 0.1\n\n\nnet.optimizer.lr\n\n&lt;tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01&gt;\n\n\n\n그 전풀이에서는 \\(\\alpha=0.1\\)로 설정되어 있으나, 위 compile의 옵션 optimizer=’sgd’에서는 0.01이 기본으로 설정되어 있어\n\n(4단계) net.fit()\n\nnet.fit(X,y,epochs=5000,verbose=0,batch_size=N)\n\n&lt;keras.callbacks.History at 0x7f05ebf94880&gt;\n\n\n\n학습률 건드리지 말고 그냥 epcohs을 늘려주자.\n\n\nnet.weights\n\n[&lt;tf.Variable 'dense_8/kernel:0' shape=(2, 1) dtype=float32, numpy=\n array([[2.5841727],\n        [3.9320903]], dtype=float32)&gt;]\n\n\n\n\n\n여러가지 회귀모형의 적합과 학습과정의 모니터링\n\n예제1\nmodel: \\(y_i \\approx \\beta_0 +\\beta_1 x_i\\)\n\nnp.random.seed(43052)\nN= 100\nx= np.random.randn(N)\nepsilon = np.random.randn(N)*0.5\ny= 2.5+4*x +epsilon\n\n\nX= np.stack([np.ones(N),x],axis=1)\ny= y.reshape(N,1)\n\n\nplt.plot(x,y,'o') # 관측한 자료\n\n\n\n\n\nbeta_hat = np.array([-3,-2]).reshape(2,1)\n\n\n위의 -3,-2는 그냥 이렇지 않을까? 하고 넣어본 임의의 숫자.\n\n\nyhat = X@beta_hat\n\n\nplt.plot(x,y,'o')\nplt.plot(x,yhat.reshape(-1),'-')\n\n\n\n\n더 좋은 적합선을 얻기위해서!\n\nslope = (2*X.T@X@beta_hat - 2*X.T@y)/ N\nbeta_hat2 = beta_hat - 0.1*slope\nyhat2 = X@beta_hat2\n\n\nplt.plot(x,y,'o')\nplt.plot(x,yhat.reshape(-1),'-')\nplt.plot(x,yhat2.reshape(-1),'-')\n\n\n\n\n초록색이 좀 더 나아보인다.\n\nbeta_hat = np.array([-3,-2]).reshape(2,1)\nbeta_hats = beta_hat # beta_hats = beta_hat.copy() 가 더 안전한 코드입니다.\nfor i in range(1,30):\n    yhat = X@beta_hat\n    slope = (2*X.T@X@beta_hat - 2*X.T@y) / N\n    beta_hat = beta_hat - 0.1*slope # 0.1은 적당, 0.3은 쪼금빠르지만 그래도 적당, 0.9는 너무 나간것같음, 1.0 은 수렴안함, 1.2\n    beta_hats = np.concatenate([beta_hats,beta_hat],axis=1)\n\n\nbeta_hats\n\narray([[-3.        , -1.98776175, -1.16054651, -0.48448286,  0.06808892,\n         0.51975837,  0.88897604,  1.19081334,  1.43758253,  1.63934274,\n         1.80431301,  1.93920945,  2.04952049,  2.13973166,  2.2135091 ,\n         2.27384947,  2.32320238,  2.36357035,  2.39659058,  2.42360161,\n         2.44569792,  2.46377445,  2.47856304,  2.49066214,  2.50056123,\n         2.5086606 ,  2.51528766,  2.52071021,  2.52514732,  2.52877816],\n       [-2.        , -0.929175  , -0.05089843,  0.66940348,  1.26010705,\n         1.74449975,  2.14169103,  2.46736052,  2.73437247,  2.95328049,\n         3.13274182,  3.27985761,  3.40045219,  3.4993023 ,  3.58032529,\n         3.64673351,  3.70116104,  3.74576767,  3.78232418,  3.81228235,\n         3.83683236,  3.85694989,  3.87343472,  3.88694243,  3.89801039,\n         3.90707901,  3.91450928,  3.92059703,  3.92558472,  3.92967104]])\n\n\n\nb0hats = beta_hats[0].tolist()\nb1hats = beta_hats[1].tolist()\n\n\nnp.linalg.inv(X.T@X) @ X.T @ y\n\narray([[2.5451404 ],\n       [3.94818596]])\n\n\n\nβ0, β1에 대한 최솟값\n\n\nfrom matplotlib import animation\nplt.rcParams[\"animation.html\"] = \"jshtml\"\n\n\nfig = plt.figure(); fig.set_figheight(5); fig.set_figwidth(12)\n\n&lt;Figure size 1200x500 with 0 Axes&gt;\n\n\n\nax1= fig.add_subplot(1,2,1)\nax2= fig.add_subplot(1,2,2,projection='3d')\n# ax1: 왼쪽그림\nax1.plot(x,y,'o')\nline, = ax1.plot(x,b0hats[0] + b1hats[0]*x)\n# ax2: 오른쪽그림\nβ0,β1 = np.meshgrid(np.arange(-6,11,0.25),np.arange(-6,11,0.25),indexing='ij')\nβ0=β0.reshape(-1)\nβ1=β1.reshape(-1)\nloss_fn = lambda b0,b1: np.sum((y-b0-b1*x)**2)\nloss = list(map(loss_fn, β0,β1))\nax2.scatter(β0,β1,loss,alpha=0.02)\nax2.scatter(2.5451404,3.94818596,loss_fn(2.5451404,3.94818596),s=200,marker='*')\n\ndef animate(i):\n    line.set_ydata(b0hats[i] + b1hats[i]*x)\n    ax2.scatter(b0hats[i],b1hats[i],loss_fn(b0hats[i],b1hats[i]),color=\"grey\")\n\nani = animation.FuncAnimation(fig,animate,frames=30)\nani\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\n예제2\nmodel: \\(y_i \\approx \\beta_0 +\\beta_1 e^{-x_i}\\)\n\nnp.random.seed(43052)\nN= 100\nx= np.linspace(-1,1,N)\nepsilon = np.random.randn(N)*0.5\ny= 2.5+4*np.exp(-x) +epsilon\n\n\nplt.plot(x,y,'o')\n\n\n\n\n\nX= np.stack([np.ones(N),np.exp(-x)],axis=1)\ny= y.reshape(N,1)\n\n\nbeta_hat = np.array([-3,-2]).reshape(2,1)\nbeta_hats = beta_hat.copy() # shallow copy, deep copy &lt;--- 여름 방학 특강\nfor i in range(1,30):\n    yhat = X@beta_hat\n    slope = (2*X.T@X@beta_hat - 2*X.T@y) /N\n    beta_hat = beta_hat - 0.05*slope\n    beta_hats = np.concatenate([beta_hats,beta_hat],axis=1)\n\n\nbeta_hats\n\narray([[-3.        , -1.74671631, -0.82428979, -0.14453919,  0.35720029,\n         0.72834869,  1.0036803 ,  1.20869624,  1.36209751,  1.47759851,\n         1.56525696,  1.63244908,  1.68458472,  1.72563174,  1.75850062,\n         1.78532638,  1.80767543,  1.82669717,  1.84323521,  1.85790889,\n         1.8711731 ,  1.88336212,  1.89472176,  1.90543297,  1.91562909,\n         1.92540859,  1.93484428,  1.94399023,  1.9528867 ,  1.96156382],\n       [-2.        , -0.25663415,  1.01939241,  1.95275596,  2.63488171,\n         3.13281171,  3.49570765,  3.75961951,  3.95098231,  4.08918044,\n         4.18842797,  4.2591476 ,  4.30898175,  4.34353413,  4.36691339,\n         4.38213187,  4.39139801,  4.39633075,  4.39811673,  4.3976256 ,\n         4.3954946 ,  4.3921905 ,  4.38805511,  4.3833386 ,  4.37822393,\n         4.37284482,  4.36729887,  4.36165718,  4.35597148,  4.35027923]])\n\n\n\nb0hats= beta_hats[0].tolist()\nb1hats= beta_hats[1].tolist()\n\n\nnp.linalg.inv(X.T@X)@X.T@y\n\narray([[2.46307644],\n       [3.99681332]])\n\n\n\nfig = plt.figure(); fig.set_figheight(5); fig.set_figwidth(12)\n\n&lt;Figure size 1200x500 with 0 Axes&gt;\n\n\n\nax1= fig.add_subplot(1,2,1)\nax2= fig.add_subplot(1,2,2,projection='3d')\n# ax1: 왼쪽그림\nax1.plot(x,y,'o')\nline, = ax1.plot(x,b0hats[0] + b1hats[0]*np.exp(-x))\n# ax2: 오른쪽그림\nβ0,β1 = np.meshgrid(np.arange(-6,11,0.25),np.arange(-6,11,0.25),indexing='ij')\nβ0=β0.reshape(-1)\nβ1=β1.reshape(-1)\nloss_fn = lambda b0,b1: np.sum((y-b0-b1*np.exp(-x))**2)\nloss = list(map(loss_fn, β0,β1))\nax2.scatter(β0,β1,loss,alpha=0.02)\nax2.scatter(2.46307644,3.99681332,loss_fn(2.46307644,3.99681332),s=200,marker='*')\n\ndef animate(i):\n    line.set_ydata(b0hats[i] + b1hats[i]*np.exp(-x))\n    ax2.scatter(b0hats[i],b1hats[i],loss_fn(b0hats[i],b1hats[i]),color=\"grey\")\n\nani = animation.FuncAnimation(fig,animate,frames=30)\nani\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\n예제3\nmodel: \\(y_i \\approx \\beta_0 +\\beta_1 e^{-x_i} + \\beta_2 \\cos(5x_i)\\)\n\nnp.random.seed(43052)\nN= 100\nx= np.linspace(-1,1,N)\nepsilon = np.random.randn(N)*0.5\ny= 2.5+4*np.exp(-x) + 5*np.cos(5*x) + epsilon\n\n\nplt.plot(x,y,'o')\n\n\n\n\n\nX=np.stack([np.ones(N),np.exp(-x),np.cos(5*x)],axis=1)\ny=y.reshape(N,1)\n\n\nbeta_hat = np.array([-3,-2,-1]).reshape(3,1)\nbeta_hats = beta_hat.copy()\nfor i in range(1,30):\n    yhat = X@beta_hat\n    slope = (2*X.T@X@beta_hat -2*X.T@y) /N\n    beta_hat = beta_hat - 0.1 * slope\n    beta_hats= np.concatenate([beta_hats,beta_hat],axis=1)\n\n\nbeta_hats\n\narray([[-3.        , -0.71767532,  0.36255782,  0.89072137,  1.16423101,\n         1.31925078,  1.41819551,  1.48974454,  1.54713983,  1.59655416,\n         1.64091846,  1.68167278,  1.71956758,  1.75503084,  1.78833646,\n         1.81968188,  1.84922398,  1.877096  ,  1.90341567,  1.92828934,\n         1.95181415,  1.97407943,  1.99516755,  2.01515463,  2.0341111 ,\n         2.05210214,  2.06918818,  2.08542523,  2.10086524,  2.11555643],\n       [-2.        ,  1.16947474,  2.64116513,  3.33411605,  3.66880042,\n         3.83768856,  3.92897389,  3.98315095,  4.01888831,  4.04486085,\n         4.06516144,  4.08177665,  4.09571971,  4.10754954,  4.1176088 ,\n         4.12613352,  4.13330391,  4.13926816,  4.14415391,  4.14807403,\n         4.15112966,  4.1534121 ,  4.15500404,  4.15598045,  4.15640936,\n         4.15635249,  4.15586584,  4.15500014,  4.15380139,  4.1523112 ],\n       [-1.        , -0.95492718, -0.66119313, -0.27681968,  0.12788212,\n         0.52254445,  0.89491388,  1.24088224,  1.55993978,  1.85310654,\n         2.12199631,  2.36839745,  2.59408948,  2.8007666 ,  2.99000967,\n         3.16327964,  3.32192026,  3.46716468,  3.60014318,  3.72189116,\n         3.83335689,  3.93540864,  4.02884144,  4.11438316,  4.19270026,\n         4.26440288,  4.33004965,  4.39015202,  4.44517824,  4.49555703]])\n\n\n\nb0hats,b1hats,b2hats = beta_hats\n\n\nnp.linalg.inv(X.T@X) @ X.T @ y\n\narray([[2.46597526],\n       [4.00095138],\n       [5.04161877]])\n\n\n\nfig = plt.figure(); fig.set_figheight(5); fig.set_figwidth(12)\n\n&lt;Figure size 1200x500 with 0 Axes&gt;\n\n\n\nax1= fig.add_subplot(1,2,1)\nax2= fig.add_subplot(1,2,2,projection='3d')\n# ax1: 왼쪽그림\nax1.plot(x,y,'o')\nline, = ax1.plot(x,b0hats[0] + b1hats[0]*np.exp(-x) + b2hats[0]*np.cos(5*x))\n# ax2: 오른쪽그림\n# β0,β1 = np.meshgrid(np.arange(-6,11,0.25),np.arange(-6,11,0.25),indexing='ij')\n# β0=β0.reshape(-1)\n# β1=β1.reshape(-1)\n# loss_fn = lambda b0,b1: np.sum((y-b0-b1*np.exp(-x))**2)\n# loss = list(map(loss_fn, β0,β1))\n# ax2.scatter(β0,β1,loss,alpha=0.02)\n# ax2.scatter(2.46307644,3.99681332,loss_fn(2.46307644,3.99681332),s=200,marker='*')\n\ndef animate(i):\n    line.set_ydata(b0hats[i] + b1hats[i]*np.exp(-x) + b2hats[i]*np.cos(5*x))\n    # ax2.scatter(b0hats[i],b1hats[i],loss_fn(b0hats[i],b1hats[i]),color=\"grey\")\n\nani = animation.FuncAnimation(fig,animate,frames=30)\nani\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\n예제3: 케라스로 해보자!\nmodel: \\(y_i \\approx \\beta_0 +\\beta_1 e^{-x_i} + \\beta_2 \\cos(5x_i)\\)\n\nnp.random.seed(43052)\nN= 100\nx= np.linspace(-1,1,N)\nepsilon = np.random.randn(N)*0.5\ny= 2.5+4*np.exp(-x) + 5*np.cos(5*x) + epsilon\n\n\nX=np.stack([np.ones(N),np.exp(-x),np.cos(5*x)],axis=1)\ny=y.reshape(N,1)\n\n\nnet = tf.keras.Sequential() # 1: 네트워크 생성\nnet.add(tf.keras.layers.Dense(1,use_bias=False)) # 2: add layer\nnet.compile(tf.optimizers.SGD(0.1), loss='mse') # 3: compile\nnet.fit(X,y,epochs=30, batch_size=N) # 4: fit\n\nEpoch 1/30\n1/1 [==============================] - 0s 268ms/step - loss: 28.0844\nEpoch 2/30\n1/1 [==============================] - 0s 1ms/step - loss: 13.7046\nEpoch 3/30\n1/1 [==============================] - 0s 1ms/step - loss: 9.4655\nEpoch 4/30\n1/1 [==============================] - 0s 1ms/step - loss: 7.5460\nEpoch 5/30\n1/1 [==============================] - 0s 1ms/step - loss: 6.2784\nEpoch 6/30\n1/1 [==============================] - 0s 1ms/step - loss: 5.2871\nEpoch 7/30\n1/1 [==============================] - 0s 1ms/step - loss: 4.4707\nEpoch 8/30\n1/1 [==============================] - 0s 1ms/step - loss: 3.7893\nEpoch 9/30\n1/1 [==============================] - 0s 1ms/step - loss: 3.2186\nEpoch 10/30\n1/1 [==============================] - 0s 1ms/step - loss: 2.7401\nEpoch 11/30\n1/1 [==============================] - 0s 1ms/step - loss: 2.3389\nEpoch 12/30\n1/1 [==============================] - 0s 1ms/step - loss: 2.0023\nEpoch 13/30\n1/1 [==============================] - 0s 1ms/step - loss: 1.7200\nEpoch 14/30\n1/1 [==============================] - 0s 1ms/step - loss: 1.4833\nEpoch 15/30\n1/1 [==============================] - 0s 1ms/step - loss: 1.2846\nEpoch 16/30\n1/1 [==============================] - 0s 1ms/step - loss: 1.1180\nEpoch 17/30\n1/1 [==============================] - 0s 1ms/step - loss: 0.9782\nEpoch 18/30\n1/1 [==============================] - 0s 1ms/step - loss: 0.8609\nEpoch 19/30\n1/1 [==============================] - 0s 1ms/step - loss: 0.7624\nEpoch 20/30\n1/1 [==============================] - 0s 1ms/step - loss: 0.6797\nEpoch 21/30\n1/1 [==============================] - 0s 1ms/step - loss: 0.6104\nEpoch 22/30\n1/1 [==============================] - 0s 1ms/step - loss: 0.5521\nEpoch 23/30\n1/1 [==============================] - 0s 1ms/step - loss: 0.5032\nEpoch 24/30\n1/1 [==============================] - 0s 1ms/step - loss: 0.4621\nEpoch 25/30\n1/1 [==============================] - 0s 1ms/step - loss: 0.4276\nEpoch 26/30\n1/1 [==============================] - 0s 1ms/step - loss: 0.3985\nEpoch 27/30\n1/1 [==============================] - 0s 1ms/step - loss: 0.3741\nEpoch 28/30\n1/1 [==============================] - 0s 1ms/step - loss: 0.3536\nEpoch 29/30\n1/1 [==============================] - 0s 1ms/step - loss: 0.3364\nEpoch 30/30\n1/1 [==============================] - 0s 1ms/step - loss: 0.3219\n\n\n&lt;keras.callbacks.History at 0x7f05e0df71c0&gt;\n\n\n\nnet.weights\n\n[&lt;tf.Variable 'dense_9/kernel:0' shape=(3, 1) dtype=float32, numpy=\n array([[2.6067586],\n        [3.8351023],\n        [4.6664157]], dtype=float32)&gt;]\n\n\n\nplt.plot(x,y,'o')\nplt.plot(x,(X@net.weights).reshape(-1),'--')\n\n\n\n\n\n\n\n숙제\n\n예제2: 케라스를 이용하여 아래를 만족하는 적절한 \\(\\beta_0\\)와 \\(\\beta_1\\)을 구하라. 적합결과를 시각화하라. (애니메이션 시각화 X)\nmodel: \\(y_i \\approx \\beta_0 +\\beta_1 e^{-x_i}\\)\n\nnp.random.seed(43052)\nN= 100\nx= np.linspace(-1,1,N)\nepsilon = np.random.randn(N)*0.5\ny= 2.5+4*np.exp(-x) +epsilon\n\n\nX=np.stack([np.ones(N),np.exp(-x)],axis=1)\ny=y.reshape(N,1)\n\n\nnet = tf.keras.Sequential() # 1: 네트워크 생성\nnet.add(tf.keras.layers.Dense(1,use_bias=False)) # 2: add layer\nnet.compile(tf.optimizers.SGD(0.1), loss='mse') # 3: compile\nnet.fit(X,y,epochs=30, verbose=0, batch_size=N) # 4: fit\n\n&lt;keras.callbacks.History at 0x7f06011c7c40&gt;\n\n\n\nnet.weights\n\n[&lt;tf.Variable 'dense_10/kernel:0' shape=(2, 1) dtype=float32, numpy=\n array([[2.126046 ],\n        [4.2351737]], dtype=float32)&gt;]\n\n\n\nplt.plot(x,y,'o')\nplt.plot(x,(X@net.weights).reshape(-1),'--')"
  },
  {
    "objectID": "posts/2022-05-23-(12주차) 5월23일.html",
    "href": "posts/2022-05-23-(12주차) 5월23일.html",
    "title": "[STBDA] 12wk: CONV,MAXPOOL,CNN",
    "section": "",
    "text": "해당 강의노트는 전북대학교 최규빈교수님 STBDA2022 자료임"
  },
  {
    "objectID": "posts/2022-05-23-(12주차) 5월23일.html#imports",
    "href": "posts/2022-05-23-(12주차) 5월23일.html#imports",
    "title": "[STBDA] 12wk: CONV,MAXPOOL,CNN",
    "section": "imports",
    "text": "imports\n\nimport tensorflow as tf\nimport tensorflow.experimental.numpy as tnp\n\n\ntnp.experimental_enable_numpy_behavior()\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np"
  },
  {
    "objectID": "posts/2022-05-23-(12주차) 5월23일.html#cnn",
    "href": "posts/2022-05-23-(12주차) 5월23일.html#cnn",
    "title": "[STBDA] 12wk: CONV,MAXPOOL,CNN",
    "section": "CNN",
    "text": "CNN\n\nCONV의 역할\n- 데이터생성 (그냥 흑백대비 데이터)\n\n_X1 = tnp.ones([50,25])*10 \n_X1\n\n&lt;tf.Tensor: shape=(50, 25), dtype=float64, numpy=\narray([[10., 10., 10., ..., 10., 10., 10.],\n       [10., 10., 10., ..., 10., 10., 10.],\n       [10., 10., 10., ..., 10., 10., 10.],\n       ...,\n       [10., 10., 10., ..., 10., 10., 10.],\n       [10., 10., 10., ..., 10., 10., 10.],\n       [10., 10., 10., ..., 10., 10., 10.]])&gt;\n\n\n\n_X2 = tnp.zeros([50,25])*10 \n_X2\n\n&lt;tf.Tensor: shape=(50, 25), dtype=float64, numpy=\narray([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]])&gt;\n\n\n\ntf.concat([_X1,_X2],axis=1)\n\n&lt;tf.Tensor: shape=(50, 50), dtype=float64, numpy=\narray([[10., 10., 10., ...,  0.,  0.,  0.],\n       [10., 10., 10., ...,  0.,  0.,  0.],\n       [10., 10., 10., ...,  0.,  0.,  0.],\n       ...,\n       [10., 10., 10., ...,  0.,  0.,  0.],\n       [10., 10., 10., ...,  0.,  0.,  0.],\n       [10., 10., 10., ...,  0.,  0.,  0.]])&gt;\n\n\n\nplt.imshow(tf.concat([_X1,_X2],axis=1),cmap='gray')\n\n&lt;matplotlib.image.AxesImage at 0x7f6e504c5520&gt;\n\n\n\n\n\n\n_noise = tnp.random.randn(50*50).reshape(50,50)\n_noise\n\n&lt;tf.Tensor: shape=(50, 50), dtype=float64, numpy=\narray([[ 0.17076016,  0.24556194,  0.67583537, ...,  0.66758916,\n         0.11793288, -0.40684891],\n       [ 1.24943253,  0.80192435, -0.10629932, ...,  0.19412642,\n         1.01308712, -0.51910469],\n       [-0.54708995,  1.32241983,  1.71576768, ..., -0.70771522,\n        -1.41728966,  1.29099964],\n       ...,\n       [ 0.41993666,  0.01969409, -1.59803669, ..., -0.22710783,\n         1.10394856, -0.75150101],\n       [ 0.18437325, -0.18661286, -0.1701197 , ...,  0.15435531,\n         0.92806262, -1.19241476],\n       [ 0.20934349,  1.24031218,  1.52275298, ...,  0.41015174,\n         0.76922351,  1.74561576]])&gt;\n\n\n\nXXX = tf.concat([_X1,_X2],axis=1) + _noise\n\n\nXXX=XXX.reshape(1,50,50,1)\n\n\nplt.imshow(XXX.reshape(50,50),cmap='gray')\n\n&lt;matplotlib.image.AxesImage at 0x7f6f0017f970&gt;\n\n\n\n\n\n- conv layer 생성\n\nconv = tf.keras.layers.Conv2D(2,(2,2)) \n\n\nconv.weights # 처음에는 가중치가 없음 \n\n[]\n\n\n\nconv(XXX) # 가중치를 만들기 위해서 XXX를 conv에 한번 통과시킴\nconv.weights # 이제 가중치가 생김\n\n[&lt;tf.Variable 'conv2d/kernel:0' shape=(2, 2, 1, 2) dtype=float32, numpy=\n array([[[[ 0.12864834, -0.3870984 ]],\n \n         [[-0.4960475 , -0.09744334]]],\n \n \n        [[[ 0.38146013, -0.5358522 ]],\n \n         [[ 0.5728392 , -0.45205393]]]], dtype=float32)&gt;,\n &lt;tf.Variable 'conv2d/bias:0' shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)&gt;]\n\n\n- 가중치의 값을 확인해보자.\n\nconv.weights[0] # kernel에 해당하는것 \n\n&lt;tf.Variable 'conv2d/kernel:0' shape=(2, 2, 1, 2) dtype=float32, numpy=\narray([[[[ 0.12864834, -0.3870984 ]],\n\n        [[-0.4960475 , -0.09744334]]],\n\n\n       [[[ 0.38146013, -0.5358522 ]],\n\n        [[ 0.5728392 , -0.45205393]]]], dtype=float32)&gt;\n\n\n\nconv.weights[1] # bias에 해당하는것 \n\n&lt;tf.Variable 'conv2d/bias:0' shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)&gt;\n\n\n- 필터값을 원하는 것으로 변경해보자.\n\nw0 = [[0.25,0.25],[0.25,0.25]] # 잡티(noise)를 제거하는 효과를 준다. \nw1 = [[-1.0,1.0],[-1.0,1.0]] # 경계를 찾기 좋아보이는 필터이다. (엣지검출)\n\n\nw=np.concatenate([np.array(w0).reshape(2,2,1,1),np.array(w1).reshape(2,2,1,1)],axis=-1)\nw   # conv.weights[0] 의 디멘젼 (2,2,1,1)과 같이 만들기 위해서\n\narray([[[[ 0.25, -1.  ]],\n\n        [[ 0.25,  1.  ]]],\n\n\n       [[[ 0.25, -1.  ]],\n\n        [[ 0.25,  1.  ]]]])\n\n\n\nb= np.array([0.0,0.0])\nb\n\narray([0., 0.])\n\n\n\nconv.set_weights([w,b])\nconv.get_weights()\n\n[array([[[[ 0.25, -1.  ]],\n \n         [[ 0.25,  1.  ]]],\n \n \n        [[[ 0.25, -1.  ]],\n \n         [[ 0.25,  1.  ]]]], dtype=float32),\n array([0., 0.], dtype=float32)]\n\n\n\n첫번째는 평균을 구하는 필터\n두번째는 엣지를 검출하는 필터\n\n- 필터를 넣은 결과를 확인\n\nXXX0=conv(XXX)[...,0] # 채널0\nXXX0\n\n&lt;tf.Tensor: shape=(1, 49, 49), dtype=float32, numpy=\narray([[[ 1.06169195e+01,  1.04042549e+01,  9.53252792e+00, ...,\n          1.73615709e-01,  4.98183906e-01,  5.12666255e-02],\n        [ 1.07066717e+01,  1.09334526e+01,  9.69403076e+00, ...,\n         -4.03259695e-03, -2.29447812e-01,  9.19231325e-02],\n        [ 9.95601559e+00,  1.03079481e+01,  1.01195173e+01, ...,\n         -1.59087405e-01, -1.04700661e+00, -8.29812646e-01],\n        ...,\n        [ 1.03822594e+01,  9.66355324e+00,  9.83089066e+00, ...,\n         -3.26146245e-01,  5.32525420e-01,  4.50823545e-01],\n        [ 1.01093473e+01,  9.51623154e+00,  9.73521042e+00, ...,\n         -2.99041808e-01,  4.89814699e-01,  2.20238566e-02],\n        [ 1.03618546e+01,  1.06015835e+01,  1.05513611e+01, ...,\n          3.70336890e-01,  5.65448284e-01,  5.62621772e-01]]],\n      dtype=float32)&gt;\n\n\n\nXXX1=conv(XXX)[...,1] # 채널1\nXXX1\n\n&lt;tf.Tensor: shape=(1, 49, 49), dtype=float32, numpy=\narray([[[-0.37270737, -0.47794914, -3.0089607 , ...,  1.0289683 ,\n          0.2693045 , -2.0569737 ],\n        [ 1.4220009 , -0.5148754 , -4.442814  , ..., -1.0110471 ,\n          0.10938632,  1.1760974 ],\n        [ 1.1624336 ,  0.24529648, -0.9990196 , ..., -0.8970997 ,\n         -2.6545773 ,  3.523353  ],\n        ...,\n        [-1.4986343 , -1.3761921 ,  2.0455437 , ...,  2.929906  ,\n          0.5047808 , -0.8315882 ],\n        [-0.7712288 , -1.6012363 ,  2.4771557 , ...,  1.0506623 ,\n          2.1047637 , -3.975927  ],\n        [ 0.6599827 ,  0.29893494, -0.49982262, ..., -0.3523335 ,\n          1.1327791 , -1.1440852 ]]], dtype=float32)&gt;\n\n\n- 각 채널을 시각화\n\nfig, ((ax1,ax2),(ax3,ax4)) = plt.subplots(2,2)\n\n\n\n\n\nax1.imshow(XXX.reshape(50,50),cmap='gray')\n\n&lt;matplotlib.image.AxesImage at 0x7f6de401ae50&gt;\n\n\n\nax3.imshow(XXX0.reshape(49,49),cmap='gray')\n\n&lt;matplotlib.image.AxesImage at 0x7f6de41ac100&gt;\n\n\n\nax4.imshow(XXX1.reshape(49,49),cmap='gray')\n\n&lt;matplotlib.image.AxesImage at 0x7f6e04214ee0&gt;\n\n\n\nfig\n\n\n\n\n\n2사분면: 원래이미지\n3사분면: 원래이미지 -&gt; 평균을 의미하는 conv적용\n4사분면: 원래이미지 -&gt; 엣지를 검출하는 conv적용\n\n- conv(XXX)의 각 채널에 한번더 conv를 통과시켜보자\n\nconv(XXX0.reshape(1,49,49,1))[...,0] ### XXX0 -&gt; 평균필터 &lt;=&gt; XXX -&gt; 평균필터 -&gt; 평균필터 \nconv(XXX0.reshape(1,49,49,1))[...,1] ### XXX0 -&gt; 엣지필터 &lt;=&gt; XXX -&gt; 평균필터 -&gt; 엣지필터 \nconv(XXX1.reshape(1,49,49,1))[...,0] ### XXX1 -&gt; 평균필터 &lt;=&gt; XXX -&gt; 엣지필터 -&gt; 평균필터 \nconv(XXX1.reshape(1,49,49,1))[...,1] ### XXX1 -&gt; 엣지필터 &lt;=&gt; XXX -&gt; 엣지필터 -&gt; 엣지필터 \n\n&lt;tf.Tensor: shape=(1, 48, 48), dtype=float32, numpy=\narray([[[-2.042118  , -6.45895   , 13.813332  , ...,  3.446345  ,\n          0.36076963, -1.2595671 ],\n        [-2.8540134 , -5.1722546 , 16.216526  , ...,  0.44306922,\n         -0.63704395,  7.2446413 ],\n        [-1.1664009 ,  0.9689274 ,  6.7377033 , ...,  3.0709298 ,\n         -7.5607853 , 10.2151375 ],\n        ...,\n        [ 0.08633995,  3.5339375 , -3.587349  , ...,  6.584819  ,\n         -4.3739424 ,  1.4463077 ],\n        [-0.7075653 ,  7.500128  , -7.38381   , ...,  9.872438  ,\n         -1.3710237 , -7.41706   ],\n        [-1.1910553 ,  3.2796345 , -6.0162563 , ...,  2.9223158 ,\n          2.5392141 , -8.357555  ]]], dtype=float32)&gt;\n\n\n\nfig,ax =plt.subplots(3,4)\n\n\n\n\n\nax[0][0].imshow(XXX.reshape(50,50),cmap='gray') # 원래이미지\n\n&lt;matplotlib.image.AxesImage at 0x7f6de401a430&gt;\n\n\n\nax[1][0].imshow(XXX0.reshape(49,49),cmap='gray') # 원래이미지 -&gt; 평균필터 \nax[1][2].imshow(XXX1.reshape(49,49),cmap='gray') # 원래이미지 -&gt; 엣지필터\n\n&lt;matplotlib.image.AxesImage at 0x7f6dc4e59f10&gt;\n\n\n\nax[2][0].imshow(conv(XXX0.reshape(1,49,49,1))[...,0].reshape(48,48),cmap='gray') # 원래이미지 -&gt; 평균필터 \nax[2][1].imshow(conv(XXX0.reshape(1,49,49,1))[...,1].reshape(48,48),cmap='gray') # 원래이미지 -&gt; 엣지필터\nax[2][2].imshow(conv(XXX1.reshape(1,49,49,1))[...,0].reshape(48,48),cmap='gray') # 원래이미지 -&gt; 평균필터 \nax[2][3].imshow(conv(XXX1.reshape(1,49,49,1))[...,1].reshape(48,48),cmap='gray') # 원래이미지 -&gt; 엣지필터\n\n&lt;matplotlib.image.AxesImage at 0x7f6dc4ce0e20&gt;\n\n\n\nfig.set_figheight(8)\nfig.set_figwidth(16)\nfig.tight_layout()\nfig\n\n\n\n\n- 요약 - conv의 weight에 따라서 엣지를 검출하는 필터가 만들어지기도 하고 스무딩의 역할을 하는 필터가 만들어지기도 한다. 그리고 우리는 의미를 알 수 없지만 어떠한 역할을 하는 필터가 만들어질 것이다. - 이것들을 조합하다보면 우연히 이미지를 분류하기에 유리한 특징을 뽑아내는 weight가 맞춰질 수도 있겠다. - 채널수를 많이 만들고 다양한 웨이트조합을 실험하다보면 보다 복잡한 이미지의 특징을 추출할 수도 있을 것이다? - 컨볼루션 레이어의 역할 = 이미지의 특징을 추출하는 역할\n- 참고: 스트라이드, 패딩 - 스트라이드: 윈도우가 1칸씩 이동하는 것이 아니라 2~3칸씩 이동함 - 패딩: 이미지의 가장자리에 정당한 값을 넣어서 (예를들어 0) 컨볼루션을 수행. 따라서 컨볼루션 연산 이후에도 이미지의 크기가 줄어들지 않도록 방지한다.\n\n\nMAXPOOL\n- 기본적역할: 이미지의 크기를 줄이는 것\n\n이미지의의 크기를 줄여야하는 이유? 어차피 최종적으로 10차원으로 줄어야하므로\n이미지의 크기를 줄이면서도 동시에 아주 크리티컬한 특징은 손실없이 유지하고 싶다~\n\n- 점점 작은 이미지가 되면서 중요한 특징들은 살아남지만 그렇지 않으면 죽는다. (캐리커쳐 느낌)\n- 평균이 아니라 max를 쓴 이유는? 그냥 평균보다 나을것이라고 생각했음..\n\n그런데 사실은 꼭 그렇지만은 않아서 최근에는 꼭 맥스풀링을 고집하진 않는 추세 (평균풀링도 많이씀)\n\n\n\nCNN 아키텍처의 표현방법\n- 아래와 같이 아키텍처의 다이어그램형태로 표현하고 굳이 노드별로 이미지를 그리진 않음\n\n- 물론 아래와 같이 그리는 경우도 있음\n\n\n\nDiscusstion about CNN\n- 격자형태로 배열된 자료를 처리하는데 특화된 신경망이다.\n\n시계열 (1차원격자), 이미지 (2차원격자)\n\n- 실제응용에서 엄청난 성공을 거두었다.\n- 이름의 유래는 컨볼루션이라는 수학적 연산을 사용했기 때문\n\n컨볼루션은 조금 특별한 선형변환이다.\n\n- 신경과학의 원리가 심층학습에 영향을 미친 사례이다.\n\n\nCNN의 모티브\n- 희소성 + 매개변수의 공유\n\n다소 철학적인 모티브임\n희소성: 이미지를 분석하여 특징을 뽑아낼때 부분부분의 특징만 뽑으면 된다는 의미\n매개변수의 공유: 한 채널에는 하나의 역할을 하는 커널을 설계하면 된다는 의미 (스무딩이든 엣징이든). 즉 어떤지역은 스무딩, 어떤지역은 엣징을 할 필요가 없이 한채널에서는 엣징만, 다른채널에서는 스무딩만 수행한뒤 여러채널을 조합해서 이해하면 된다.\n\n- 매개변수 공유효과로 인해서 파라메터가 확 줄어든다.\n(예시) (1,6,6,1) -&gt; (1,5,5,2)\n\nMLP방식이면 (36,50) 의 차원을 가진 매트릭스가 필요함 =&gt; 1800개의 매개변수 필요\nCNN은 8개의 매개변수 필요\n\n\n\nCNN 신경망의 기본구조\n- 기본유닛\n\nconv - activation - pooling\nconv - conv - activation - pooling"
  },
  {
    "objectID": "posts/2022-05-23-(12주차) 5월23일.html#모형의-성능을-올리기-위한-노력들",
    "href": "posts/2022-05-23-(12주차) 5월23일.html#모형의-성능을-올리기-위한-노력들",
    "title": "[STBDA] 12wk: CONV,MAXPOOL,CNN",
    "section": "모형의 성능을 올리기 위한 노력들",
    "text": "모형의 성능을 올리기 위한 노력들\n\ndropout\n- 아래의 예제를 복습하자.\n\nnp.random.seed(43052)\nx = np.linspace(0,1,100).reshape(100,1)\ny = np.random.normal(loc=0,scale=0.01,size=(100,1))\nplt.plot(x,y)\n\n\n\n\n\ntf.random.set_seed(43052)\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(2048,activation='relu'))\nnet.add(tf.keras.layers.Dense(1))\nnet.compile(loss='mse',optimizer='adam')\nnet.fit(x,y,epochs=5000,verbose=0,batch_size=100)\n\n&lt;keras.callbacks.History at 0x7f6dc4bd6760&gt;\n\n\n\nplt.plot(x,y)\nplt.plot(x,net(x),'--')\n\n\n\n\n- train/test로 나누어서 생각해보자.\n\ntf.random.set_seed(43052)\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(2048,activation='relu'))\nnet.add(tf.keras.layers.Dense(1))\nnet.compile(loss='mse',optimizer='adam')\nnet.fit(x[:80],y[:80],epochs=5000,verbose=0,batch_size=80)\n\n&lt;keras.callbacks.History at 0x7f6dc48eb0a0&gt;\n\n\n\nplt.plot(x,y)\nplt.plot(x[:80],net(x[:80]),'--')\n\n\n\n\n\nplt.plot(x,y)\nplt.plot(x[:80],net(x[:80]),'--')\nplt.plot(x[80:],net(x[80:]),'--')\n\n\n\n\n\ntrain에서 추세를 따라가는게 좋은게 아니다 \\(\\to\\) 그냥 직선으로 핏하는거 이외에는 다 오버핏이다.\n\n- 매 에폭마다 적당히 80%의 노드들을 빼고 학습하자 \\(\\to\\) 너무 잘 학습되는 문제는 생기지 않을 것이다 (과적합이 방지될것이다?)\n\ntf.random.set_seed(43052)\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(2048,activation='relu'))\nnet.add(tf.keras.layers.Dropout(0.8))\nnet.add(tf.keras.layers.Dense(1))\nnet.compile(loss='mse',optimizer='adam')\nnet.fit(x[:80],y[:80],epochs=5000,verbose=0,batch_size=80)\n\n&lt;keras.callbacks.History at 0x7f6dc40380a0&gt;\n\n\n\nplt.plot(x,y)\nplt.plot(x[:80],net(x[:80]),'--')\nplt.plot(x[80:],net(x[80:]),'--')\n\n\n\n\n- 드랍아웃에 대한 summary\n\n직관: 특정노드를 랜덤으로 off시키면 학습이 방해되어 오히려 과적합이 방지되는 효과가 있다 (그렇지만 진짜 중요한 특징이라면 랜덤으로 off 되더라도 어느정도는 학습될 듯)\nnote: 드랍아웃을 쓰면 오버핏이 줄어드는건 맞지만 완전히 없어지는건 아니다.\nnote: 오버핏을 줄이는 유일한 방법이 드랍아웃만 있는것도 아니며, 드랍아웃이 오버핏을 줄이는 가장 효과적인 방법도 아니다 (최근에는 dropout보다 batch nomalization을 사용하는 추세임)\n\n\n\ntrain / val / test\n- data\n\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n\n\nX= x_train.reshape(-1,28,28,1)/255 ## 입력이 0~255 -&gt; 0~1로 표준화 시키는 효과 + float으로 자료형이 바뀜 \ny = tf.keras.utils.to_categorical(y_train)\nXX = x_test.reshape(-1,28,28,1)/255\nyy = tf.keras.utils.to_categorical(y_test)\n\n\nX.dtype\n\ndtype('float64')\n\n\n표준화하면 파라미터를 학습할 때 용이함\n\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Flatten())\nnet.add(tf.keras.layers.Dense(50,activation='relu'))\nnet.add(tf.keras.layers.Dense(10,activation='softmax'))\nnet.compile(optimizer='adam',loss=tf.losses.categorical_crossentropy,metrics='accuracy')\n\n\nvalidation_split val을 0.2로\n\n\n#collapse_output\ncb1 = tf.keras.callbacks.TensorBoard()\nnet.fit(X,y,epochs=200,batch_size=200,validation_split=0.2,callbacks=cb1,verbose=1) \n\nEpoch 1/200\n240/240 [==============================] - 1s 1ms/step - loss: 0.6794 - accuracy: 0.7736 - val_loss: 0.4910 - val_accuracy: 0.8310\nEpoch 2/200\n240/240 [==============================] - 0s 815us/step - loss: 0.4608 - accuracy: 0.8415 - val_loss: 0.4685 - val_accuracy: 0.8362\nEpoch 3/200\n240/240 [==============================] - 0s 799us/step - loss: 0.4217 - accuracy: 0.8520 - val_loss: 0.4349 - val_accuracy: 0.8491\nEpoch 4/200\n240/240 [==============================] - 0s 802us/step - loss: 0.4005 - accuracy: 0.8596 - val_loss: 0.4052 - val_accuracy: 0.8568\nEpoch 5/200\n240/240 [==============================] - 0s 817us/step - loss: 0.3790 - accuracy: 0.8669 - val_loss: 0.3915 - val_accuracy: 0.8632\nEpoch 6/200\n240/240 [==============================] - 0s 818us/step - loss: 0.3656 - accuracy: 0.8710 - val_loss: 0.3906 - val_accuracy: 0.8624\nEpoch 7/200\n240/240 [==============================] - 0s 802us/step - loss: 0.3542 - accuracy: 0.8743 - val_loss: 0.3905 - val_accuracy: 0.8640\nEpoch 8/200\n240/240 [==============================] - 0s 788us/step - loss: 0.3438 - accuracy: 0.8769 - val_loss: 0.3653 - val_accuracy: 0.8712\nEpoch 9/200\n240/240 [==============================] - 0s 815us/step - loss: 0.3366 - accuracy: 0.8814 - val_loss: 0.3601 - val_accuracy: 0.8741\nEpoch 10/200\n240/240 [==============================] - 0s 794us/step - loss: 0.3272 - accuracy: 0.8825 - val_loss: 0.3570 - val_accuracy: 0.8757\nEpoch 11/200\n240/240 [==============================] - 0s 812us/step - loss: 0.3188 - accuracy: 0.8861 - val_loss: 0.3588 - val_accuracy: 0.8737\nEpoch 12/200\n240/240 [==============================] - 0s 809us/step - loss: 0.3135 - accuracy: 0.8861 - val_loss: 0.3546 - val_accuracy: 0.8743\nEpoch 13/200\n240/240 [==============================] - 0s 813us/step - loss: 0.3067 - accuracy: 0.8894 - val_loss: 0.3595 - val_accuracy: 0.8757\nEpoch 14/200\n240/240 [==============================] - 0s 812us/step - loss: 0.3006 - accuracy: 0.8922 - val_loss: 0.3552 - val_accuracy: 0.8748\nEpoch 15/200\n240/240 [==============================] - 0s 804us/step - loss: 0.2985 - accuracy: 0.8924 - val_loss: 0.3571 - val_accuracy: 0.8744\nEpoch 16/200\n240/240 [==============================] - 0s 804us/step - loss: 0.2950 - accuracy: 0.8947 - val_loss: 0.3960 - val_accuracy: 0.8563\nEpoch 17/200\n240/240 [==============================] - 0s 801us/step - loss: 0.2890 - accuracy: 0.8958 - val_loss: 0.3461 - val_accuracy: 0.8783\nEpoch 18/200\n240/240 [==============================] - 0s 800us/step - loss: 0.2836 - accuracy: 0.8965 - val_loss: 0.3525 - val_accuracy: 0.8748\nEpoch 19/200\n240/240 [==============================] - 0s 821us/step - loss: 0.2802 - accuracy: 0.8976 - val_loss: 0.3502 - val_accuracy: 0.8783\nEpoch 20/200\n240/240 [==============================] - 0s 801us/step - loss: 0.2756 - accuracy: 0.8998 - val_loss: 0.3466 - val_accuracy: 0.8777\nEpoch 21/200\n240/240 [==============================] - 0s 817us/step - loss: 0.2735 - accuracy: 0.9005 - val_loss: 0.3424 - val_accuracy: 0.8790\nEpoch 22/200\n240/240 [==============================] - 0s 797us/step - loss: 0.2675 - accuracy: 0.9029 - val_loss: 0.3376 - val_accuracy: 0.8827\nEpoch 23/200\n240/240 [==============================] - 0s 806us/step - loss: 0.2637 - accuracy: 0.9031 - val_loss: 0.3444 - val_accuracy: 0.8793\nEpoch 24/200\n240/240 [==============================] - 0s 821us/step - loss: 0.2626 - accuracy: 0.9052 - val_loss: 0.3458 - val_accuracy: 0.8779\nEpoch 25/200\n240/240 [==============================] - 0s 795us/step - loss: 0.2556 - accuracy: 0.9074 - val_loss: 0.3401 - val_accuracy: 0.8816\nEpoch 26/200\n240/240 [==============================] - 0s 817us/step - loss: 0.2531 - accuracy: 0.9087 - val_loss: 0.3404 - val_accuracy: 0.8852\nEpoch 27/200\n240/240 [==============================] - 0s 813us/step - loss: 0.2517 - accuracy: 0.9086 - val_loss: 0.3423 - val_accuracy: 0.8832\nEpoch 28/200\n240/240 [==============================] - 0s 795us/step - loss: 0.2450 - accuracy: 0.9126 - val_loss: 0.3451 - val_accuracy: 0.8825\nEpoch 29/200\n240/240 [==============================] - 0s 794us/step - loss: 0.2464 - accuracy: 0.9101 - val_loss: 0.3477 - val_accuracy: 0.8788\nEpoch 30/200\n240/240 [==============================] - 0s 789us/step - loss: 0.2440 - accuracy: 0.9109 - val_loss: 0.3417 - val_accuracy: 0.8814\nEpoch 31/200\n240/240 [==============================] - 0s 792us/step - loss: 0.2394 - accuracy: 0.9136 - val_loss: 0.3442 - val_accuracy: 0.8796\nEpoch 32/200\n240/240 [==============================] - 0s 799us/step - loss: 0.2379 - accuracy: 0.9141 - val_loss: 0.3402 - val_accuracy: 0.8817\nEpoch 33/200\n240/240 [==============================] - 0s 796us/step - loss: 0.2360 - accuracy: 0.9148 - val_loss: 0.3585 - val_accuracy: 0.8792\nEpoch 34/200\n240/240 [==============================] - 0s 807us/step - loss: 0.2318 - accuracy: 0.9157 - val_loss: 0.3452 - val_accuracy: 0.8807\nEpoch 35/200\n240/240 [==============================] - 0s 790us/step - loss: 0.2292 - accuracy: 0.9178 - val_loss: 0.3570 - val_accuracy: 0.8763\nEpoch 36/200\n240/240 [==============================] - 0s 790us/step - loss: 0.2288 - accuracy: 0.9173 - val_loss: 0.3478 - val_accuracy: 0.8824\nEpoch 37/200\n240/240 [==============================] - 0s 795us/step - loss: 0.2252 - accuracy: 0.9193 - val_loss: 0.3505 - val_accuracy: 0.8836\nEpoch 38/200\n240/240 [==============================] - 0s 797us/step - loss: 0.2262 - accuracy: 0.9178 - val_loss: 0.3436 - val_accuracy: 0.8828\nEpoch 39/200\n240/240 [==============================] - 0s 782us/step - loss: 0.2213 - accuracy: 0.9201 - val_loss: 0.3394 - val_accuracy: 0.8846\nEpoch 40/200\n240/240 [==============================] - 0s 806us/step - loss: 0.2178 - accuracy: 0.9223 - val_loss: 0.3427 - val_accuracy: 0.8831\nEpoch 41/200\n240/240 [==============================] - 0s 789us/step - loss: 0.2145 - accuracy: 0.9232 - val_loss: 0.3548 - val_accuracy: 0.8798\nEpoch 42/200\n240/240 [==============================] - 0s 805us/step - loss: 0.2178 - accuracy: 0.9213 - val_loss: 0.3520 - val_accuracy: 0.8834\nEpoch 43/200\n240/240 [==============================] - 0s 796us/step - loss: 0.2137 - accuracy: 0.9223 - val_loss: 0.3565 - val_accuracy: 0.8803\nEpoch 44/200\n240/240 [==============================] - 0s 806us/step - loss: 0.2129 - accuracy: 0.9220 - val_loss: 0.3566 - val_accuracy: 0.8822\nEpoch 45/200\n240/240 [==============================] - 0s 791us/step - loss: 0.2065 - accuracy: 0.9254 - val_loss: 0.3483 - val_accuracy: 0.8830\nEpoch 46/200\n240/240 [==============================] - 0s 801us/step - loss: 0.2043 - accuracy: 0.9261 - val_loss: 0.3816 - val_accuracy: 0.8763\nEpoch 47/200\n240/240 [==============================] - 0s 804us/step - loss: 0.2078 - accuracy: 0.9247 - val_loss: 0.3576 - val_accuracy: 0.8793\nEpoch 48/200\n240/240 [==============================] - 0s 810us/step - loss: 0.1998 - accuracy: 0.9290 - val_loss: 0.3610 - val_accuracy: 0.8831\nEpoch 49/200\n240/240 [==============================] - 0s 816us/step - loss: 0.2015 - accuracy: 0.9267 - val_loss: 0.3578 - val_accuracy: 0.8813\nEpoch 50/200\n240/240 [==============================] - 0s 819us/step - loss: 0.1985 - accuracy: 0.9288 - val_loss: 0.3674 - val_accuracy: 0.8770\nEpoch 51/200\n240/240 [==============================] - 0s 798us/step - loss: 0.2012 - accuracy: 0.9274 - val_loss: 0.3534 - val_accuracy: 0.8827\nEpoch 52/200\n240/240 [==============================] - 0s 809us/step - loss: 0.1937 - accuracy: 0.9302 - val_loss: 0.3805 - val_accuracy: 0.8752\nEpoch 53/200\n240/240 [==============================] - 0s 793us/step - loss: 0.1926 - accuracy: 0.9311 - val_loss: 0.3702 - val_accuracy: 0.8786\nEpoch 54/200\n240/240 [==============================] - 0s 792us/step - loss: 0.1950 - accuracy: 0.9293 - val_loss: 0.3778 - val_accuracy: 0.8766\nEpoch 55/200\n240/240 [==============================] - 0s 790us/step - loss: 0.1914 - accuracy: 0.9316 - val_loss: 0.3700 - val_accuracy: 0.8774\nEpoch 56/200\n240/240 [==============================] - 0s 801us/step - loss: 0.1881 - accuracy: 0.9329 - val_loss: 0.3684 - val_accuracy: 0.8822\nEpoch 57/200\n240/240 [==============================] - 0s 815us/step - loss: 0.1883 - accuracy: 0.9324 - val_loss: 0.3698 - val_accuracy: 0.8809\nEpoch 58/200\n240/240 [==============================] - 0s 792us/step - loss: 0.1848 - accuracy: 0.9335 - val_loss: 0.3661 - val_accuracy: 0.8831\nEpoch 59/200\n240/240 [==============================] - 0s 809us/step - loss: 0.1860 - accuracy: 0.9332 - val_loss: 0.3744 - val_accuracy: 0.8783\nEpoch 60/200\n240/240 [==============================] - 0s 808us/step - loss: 0.1816 - accuracy: 0.9345 - val_loss: 0.3830 - val_accuracy: 0.8762\nEpoch 61/200\n240/240 [==============================] - 0s 798us/step - loss: 0.1801 - accuracy: 0.9351 - val_loss: 0.3805 - val_accuracy: 0.8779\nEpoch 62/200\n240/240 [==============================] - 0s 806us/step - loss: 0.1788 - accuracy: 0.9356 - val_loss: 0.3660 - val_accuracy: 0.8817\nEpoch 63/200\n240/240 [==============================] - 0s 806us/step - loss: 0.1803 - accuracy: 0.9348 - val_loss: 0.3693 - val_accuracy: 0.8814\nEpoch 64/200\n240/240 [==============================] - 0s 799us/step - loss: 0.1771 - accuracy: 0.9364 - val_loss: 0.3842 - val_accuracy: 0.8767\nEpoch 65/200\n240/240 [==============================] - 0s 795us/step - loss: 0.1738 - accuracy: 0.9385 - val_loss: 0.3977 - val_accuracy: 0.8768\nEpoch 66/200\n240/240 [==============================] - 0s 809us/step - loss: 0.1744 - accuracy: 0.9380 - val_loss: 0.3807 - val_accuracy: 0.8810\nEpoch 67/200\n240/240 [==============================] - 0s 808us/step - loss: 0.1727 - accuracy: 0.9379 - val_loss: 0.3881 - val_accuracy: 0.8790\nEpoch 68/200\n240/240 [==============================] - 0s 798us/step - loss: 0.1718 - accuracy: 0.9389 - val_loss: 0.3811 - val_accuracy: 0.8823\nEpoch 69/200\n240/240 [==============================] - 0s 796us/step - loss: 0.1711 - accuracy: 0.9389 - val_loss: 0.3894 - val_accuracy: 0.8768\nEpoch 70/200\n240/240 [==============================] - 0s 791us/step - loss: 0.1680 - accuracy: 0.9392 - val_loss: 0.3799 - val_accuracy: 0.8821\nEpoch 71/200\n240/240 [==============================] - 0s 791us/step - loss: 0.1683 - accuracy: 0.9392 - val_loss: 0.3839 - val_accuracy: 0.8804\nEpoch 72/200\n240/240 [==============================] - 0s 790us/step - loss: 0.1659 - accuracy: 0.9406 - val_loss: 0.3881 - val_accuracy: 0.8784\nEpoch 73/200\n240/240 [==============================] - 0s 802us/step - loss: 0.1628 - accuracy: 0.9424 - val_loss: 0.3856 - val_accuracy: 0.8808\nEpoch 74/200\n240/240 [==============================] - 0s 806us/step - loss: 0.1633 - accuracy: 0.9411 - val_loss: 0.3874 - val_accuracy: 0.8800\nEpoch 75/200\n240/240 [==============================] - 0s 804us/step - loss: 0.1621 - accuracy: 0.9426 - val_loss: 0.3862 - val_accuracy: 0.8814\nEpoch 76/200\n240/240 [==============================] - 0s 812us/step - loss: 0.1609 - accuracy: 0.9419 - val_loss: 0.4129 - val_accuracy: 0.8765\nEpoch 77/200\n240/240 [==============================] - 0s 792us/step - loss: 0.1586 - accuracy: 0.9434 - val_loss: 0.3877 - val_accuracy: 0.8788\nEpoch 78/200\n240/240 [==============================] - 0s 812us/step - loss: 0.1606 - accuracy: 0.9420 - val_loss: 0.4083 - val_accuracy: 0.8786\nEpoch 79/200\n240/240 [==============================] - 0s 806us/step - loss: 0.1581 - accuracy: 0.9442 - val_loss: 0.3989 - val_accuracy: 0.8801\nEpoch 80/200\n240/240 [==============================] - 0s 802us/step - loss: 0.1548 - accuracy: 0.9443 - val_loss: 0.4007 - val_accuracy: 0.8812\nEpoch 81/200\n240/240 [==============================] - 0s 787us/step - loss: 0.1536 - accuracy: 0.9462 - val_loss: 0.4037 - val_accuracy: 0.8779\nEpoch 82/200\n240/240 [==============================] - 0s 809us/step - loss: 0.1531 - accuracy: 0.9451 - val_loss: 0.4085 - val_accuracy: 0.8776\nEpoch 83/200\n240/240 [==============================] - 0s 795us/step - loss: 0.1521 - accuracy: 0.9455 - val_loss: 0.4313 - val_accuracy: 0.8749\nEpoch 84/200\n240/240 [==============================] - 0s 808us/step - loss: 0.1488 - accuracy: 0.9465 - val_loss: 0.4161 - val_accuracy: 0.8751\nEpoch 85/200\n240/240 [==============================] - 0s 803us/step - loss: 0.1491 - accuracy: 0.9465 - val_loss: 0.4109 - val_accuracy: 0.8764\nEpoch 86/200\n240/240 [==============================] - 0s 816us/step - loss: 0.1526 - accuracy: 0.9446 - val_loss: 0.4114 - val_accuracy: 0.8789\nEpoch 87/200\n240/240 [==============================] - 0s 794us/step - loss: 0.1466 - accuracy: 0.9483 - val_loss: 0.4237 - val_accuracy: 0.8760\nEpoch 88/200\n240/240 [==============================] - 0s 786us/step - loss: 0.1464 - accuracy: 0.9477 - val_loss: 0.4202 - val_accuracy: 0.8776\nEpoch 89/200\n240/240 [==============================] - 0s 788us/step - loss: 0.1486 - accuracy: 0.9467 - val_loss: 0.4262 - val_accuracy: 0.8784\nEpoch 90/200\n240/240 [==============================] - 0s 793us/step - loss: 0.1495 - accuracy: 0.9461 - val_loss: 0.4205 - val_accuracy: 0.8778\nEpoch 91/200\n240/240 [==============================] - 0s 790us/step - loss: 0.1407 - accuracy: 0.9504 - val_loss: 0.4134 - val_accuracy: 0.8804\nEpoch 92/200\n240/240 [==============================] - 0s 805us/step - loss: 0.1441 - accuracy: 0.9483 - val_loss: 0.4205 - val_accuracy: 0.8812\nEpoch 93/200\n240/240 [==============================] - 0s 805us/step - loss: 0.1440 - accuracy: 0.9496 - val_loss: 0.4224 - val_accuracy: 0.8789\nEpoch 94/200\n240/240 [==============================] - 0s 820us/step - loss: 0.1426 - accuracy: 0.9492 - val_loss: 0.4261 - val_accuracy: 0.8792\nEpoch 95/200\n240/240 [==============================] - 0s 807us/step - loss: 0.1418 - accuracy: 0.9501 - val_loss: 0.4332 - val_accuracy: 0.8779\nEpoch 96/200\n240/240 [==============================] - 0s 812us/step - loss: 0.1381 - accuracy: 0.9503 - val_loss: 0.4334 - val_accuracy: 0.8767\nEpoch 97/200\n240/240 [==============================] - 0s 811us/step - loss: 0.1378 - accuracy: 0.9511 - val_loss: 0.4355 - val_accuracy: 0.8788\nEpoch 98/200\n240/240 [==============================] - 0s 805us/step - loss: 0.1347 - accuracy: 0.9531 - val_loss: 0.4274 - val_accuracy: 0.8752\nEpoch 99/200\n240/240 [==============================] - 0s 826us/step - loss: 0.1358 - accuracy: 0.9521 - val_loss: 0.4296 - val_accuracy: 0.8775\nEpoch 100/200\n240/240 [==============================] - 0s 807us/step - loss: 0.1369 - accuracy: 0.9515 - val_loss: 0.4329 - val_accuracy: 0.8777\nEpoch 101/200\n240/240 [==============================] - 0s 802us/step - loss: 0.1319 - accuracy: 0.9531 - val_loss: 0.4352 - val_accuracy: 0.8772\nEpoch 102/200\n240/240 [==============================] - 0s 788us/step - loss: 0.1363 - accuracy: 0.9519 - val_loss: 0.4417 - val_accuracy: 0.8784\nEpoch 103/200\n240/240 [==============================] - 0s 793us/step - loss: 0.1334 - accuracy: 0.9528 - val_loss: 0.4410 - val_accuracy: 0.8780\nEpoch 104/200\n240/240 [==============================] - 0s 809us/step - loss: 0.1330 - accuracy: 0.9526 - val_loss: 0.4424 - val_accuracy: 0.8771\nEpoch 105/200\n240/240 [==============================] - 0s 807us/step - loss: 0.1339 - accuracy: 0.9524 - val_loss: 0.4622 - val_accuracy: 0.8724\nEpoch 106/200\n240/240 [==============================] - 0s 814us/step - loss: 0.1313 - accuracy: 0.9535 - val_loss: 0.4406 - val_accuracy: 0.8803\nEpoch 107/200\n240/240 [==============================] - 0s 803us/step - loss: 0.1299 - accuracy: 0.9541 - val_loss: 0.4748 - val_accuracy: 0.8723\nEpoch 108/200\n240/240 [==============================] - 0s 812us/step - loss: 0.1278 - accuracy: 0.9551 - val_loss: 0.4593 - val_accuracy: 0.8742\nEpoch 109/200\n240/240 [==============================] - 0s 826us/step - loss: 0.1288 - accuracy: 0.9551 - val_loss: 0.4564 - val_accuracy: 0.8758\nEpoch 110/200\n240/240 [==============================] - 0s 799us/step - loss: 0.1268 - accuracy: 0.9551 - val_loss: 0.4547 - val_accuracy: 0.8758\nEpoch 111/200\n240/240 [==============================] - 0s 810us/step - loss: 0.1293 - accuracy: 0.9544 - val_loss: 0.4526 - val_accuracy: 0.8778\nEpoch 112/200\n240/240 [==============================] - 0s 810us/step - loss: 0.1280 - accuracy: 0.9550 - val_loss: 0.4563 - val_accuracy: 0.8777\nEpoch 113/200\n240/240 [==============================] - 0s 798us/step - loss: 0.1253 - accuracy: 0.9555 - val_loss: 0.4629 - val_accuracy: 0.8762\nEpoch 114/200\n240/240 [==============================] - 0s 804us/step - loss: 0.1229 - accuracy: 0.9569 - val_loss: 0.4688 - val_accuracy: 0.8764\nEpoch 115/200\n240/240 [==============================] - 0s 794us/step - loss: 0.1215 - accuracy: 0.9577 - val_loss: 0.4664 - val_accuracy: 0.8737\nEpoch 116/200\n240/240 [==============================] - 0s 804us/step - loss: 0.1198 - accuracy: 0.9579 - val_loss: 0.4675 - val_accuracy: 0.8751\nEpoch 117/200\n240/240 [==============================] - 0s 801us/step - loss: 0.1209 - accuracy: 0.9581 - val_loss: 0.4704 - val_accuracy: 0.8758\nEpoch 118/200\n240/240 [==============================] - 0s 788us/step - loss: 0.1207 - accuracy: 0.9574 - val_loss: 0.4779 - val_accuracy: 0.8752\nEpoch 119/200\n240/240 [==============================] - 0s 795us/step - loss: 0.1222 - accuracy: 0.9572 - val_loss: 0.5012 - val_accuracy: 0.8697\nEpoch 120/200\n240/240 [==============================] - 0s 806us/step - loss: 0.1197 - accuracy: 0.9581 - val_loss: 0.4750 - val_accuracy: 0.8757\nEpoch 121/200\n240/240 [==============================] - 0s 806us/step - loss: 0.1185 - accuracy: 0.9585 - val_loss: 0.4926 - val_accuracy: 0.8737\nEpoch 122/200\n240/240 [==============================] - 0s 819us/step - loss: 0.1161 - accuracy: 0.9594 - val_loss: 0.4891 - val_accuracy: 0.8717\nEpoch 123/200\n240/240 [==============================] - 0s 804us/step - loss: 0.1175 - accuracy: 0.9584 - val_loss: 0.4745 - val_accuracy: 0.8765\nEpoch 124/200\n240/240 [==============================] - 0s 812us/step - loss: 0.1187 - accuracy: 0.9590 - val_loss: 0.4990 - val_accuracy: 0.8714\nEpoch 125/200\n240/240 [==============================] - 0s 819us/step - loss: 0.1140 - accuracy: 0.9605 - val_loss: 0.4990 - val_accuracy: 0.8748\nEpoch 126/200\n240/240 [==============================] - 0s 822us/step - loss: 0.1160 - accuracy: 0.9590 - val_loss: 0.5139 - val_accuracy: 0.8716\nEpoch 127/200\n240/240 [==============================] - 0s 815us/step - loss: 0.1158 - accuracy: 0.9586 - val_loss: 0.4926 - val_accuracy: 0.8773\nEpoch 128/200\n240/240 [==============================] - 0s 806us/step - loss: 0.1104 - accuracy: 0.9624 - val_loss: 0.4994 - val_accuracy: 0.8731\nEpoch 129/200\n240/240 [==============================] - 0s 799us/step - loss: 0.1134 - accuracy: 0.9603 - val_loss: 0.4956 - val_accuracy: 0.8759\nEpoch 130/200\n240/240 [==============================] - 0s 799us/step - loss: 0.1082 - accuracy: 0.9619 - val_loss: 0.5080 - val_accuracy: 0.8727\nEpoch 131/200\n240/240 [==============================] - 0s 806us/step - loss: 0.1110 - accuracy: 0.9605 - val_loss: 0.5051 - val_accuracy: 0.8722\nEpoch 132/200\n240/240 [==============================] - 0s 783us/step - loss: 0.1135 - accuracy: 0.9596 - val_loss: 0.5060 - val_accuracy: 0.8714\nEpoch 133/200\n240/240 [==============================] - 0s 794us/step - loss: 0.1125 - accuracy: 0.9607 - val_loss: 0.5162 - val_accuracy: 0.8706\nEpoch 134/200\n240/240 [==============================] - 0s 817us/step - loss: 0.1064 - accuracy: 0.9627 - val_loss: 0.5018 - val_accuracy: 0.8744\nEpoch 135/200\n240/240 [==============================] - 0s 822us/step - loss: 0.1084 - accuracy: 0.9614 - val_loss: 0.5090 - val_accuracy: 0.8728\nEpoch 136/200\n240/240 [==============================] - 0s 808us/step - loss: 0.1054 - accuracy: 0.9637 - val_loss: 0.5060 - val_accuracy: 0.8763\nEpoch 137/200\n240/240 [==============================] - 0s 812us/step - loss: 0.1083 - accuracy: 0.9618 - val_loss: 0.5197 - val_accuracy: 0.8752\nEpoch 138/200\n240/240 [==============================] - 0s 824us/step - loss: 0.1082 - accuracy: 0.9625 - val_loss: 0.5108 - val_accuracy: 0.8738\nEpoch 139/200\n240/240 [==============================] - 0s 815us/step - loss: 0.1064 - accuracy: 0.9623 - val_loss: 0.5169 - val_accuracy: 0.8754\nEpoch 140/200\n240/240 [==============================] - 0s 821us/step - loss: 0.1053 - accuracy: 0.9627 - val_loss: 0.5307 - val_accuracy: 0.8748\nEpoch 141/200\n240/240 [==============================] - 0s 820us/step - loss: 0.1062 - accuracy: 0.9625 - val_loss: 0.5226 - val_accuracy: 0.8748\nEpoch 142/200\n240/240 [==============================] - 0s 808us/step - loss: 0.1091 - accuracy: 0.9611 - val_loss: 0.5308 - val_accuracy: 0.8731\nEpoch 143/200\n240/240 [==============================] - 0s 816us/step - loss: 0.1058 - accuracy: 0.9641 - val_loss: 0.5271 - val_accuracy: 0.8725\nEpoch 144/200\n240/240 [==============================] - 0s 806us/step - loss: 0.1026 - accuracy: 0.9644 - val_loss: 0.5439 - val_accuracy: 0.8740\nEpoch 145/200\n240/240 [==============================] - 0s 815us/step - loss: 0.1002 - accuracy: 0.9661 - val_loss: 0.5382 - val_accuracy: 0.8735\nEpoch 146/200\n240/240 [==============================] - 0s 814us/step - loss: 0.1039 - accuracy: 0.9636 - val_loss: 0.5334 - val_accuracy: 0.8730\nEpoch 147/200\n240/240 [==============================] - 0s 813us/step - loss: 0.1007 - accuracy: 0.9657 - val_loss: 0.5416 - val_accuracy: 0.8739\nEpoch 148/200\n240/240 [==============================] - 0s 788us/step - loss: 0.1025 - accuracy: 0.9644 - val_loss: 0.5343 - val_accuracy: 0.8755\nEpoch 149/200\n240/240 [==============================] - 0s 817us/step - loss: 0.1014 - accuracy: 0.9648 - val_loss: 0.5390 - val_accuracy: 0.8736\nEpoch 150/200\n240/240 [==============================] - 0s 804us/step - loss: 0.1016 - accuracy: 0.9644 - val_loss: 0.5470 - val_accuracy: 0.8742\nEpoch 151/200\n240/240 [==============================] - 0s 818us/step - loss: 0.0989 - accuracy: 0.9656 - val_loss: 0.5455 - val_accuracy: 0.8761\nEpoch 152/200\n240/240 [==============================] - 0s 799us/step - loss: 0.1004 - accuracy: 0.9642 - val_loss: 0.5531 - val_accuracy: 0.8740\nEpoch 153/200\n240/240 [==============================] - 0s 808us/step - loss: 0.0975 - accuracy: 0.9654 - val_loss: 0.5628 - val_accuracy: 0.8687\nEpoch 154/200\n240/240 [==============================] - 0s 808us/step - loss: 0.0981 - accuracy: 0.9665 - val_loss: 0.5476 - val_accuracy: 0.8741\nEpoch 155/200\n240/240 [==============================] - 0s 821us/step - loss: 0.0976 - accuracy: 0.9661 - val_loss: 0.5558 - val_accuracy: 0.8747\nEpoch 156/200\n240/240 [==============================] - 0s 806us/step - loss: 0.0978 - accuracy: 0.9661 - val_loss: 0.5666 - val_accuracy: 0.8750\nEpoch 157/200\n240/240 [==============================] - 0s 808us/step - loss: 0.0954 - accuracy: 0.9674 - val_loss: 0.5755 - val_accuracy: 0.8708\nEpoch 158/200\n240/240 [==============================] - 0s 822us/step - loss: 0.0962 - accuracy: 0.9661 - val_loss: 0.5584 - val_accuracy: 0.8737\nEpoch 159/200\n240/240 [==============================] - 0s 788us/step - loss: 0.0970 - accuracy: 0.9656 - val_loss: 0.5821 - val_accuracy: 0.8707\nEpoch 160/200\n240/240 [==============================] - 0s 812us/step - loss: 0.0914 - accuracy: 0.9693 - val_loss: 0.5705 - val_accuracy: 0.8731\nEpoch 161/200\n240/240 [==============================] - 0s 799us/step - loss: 0.0985 - accuracy: 0.9644 - val_loss: 0.5941 - val_accuracy: 0.8702\nEpoch 162/200\n240/240 [==============================] - 0s 812us/step - loss: 0.0925 - accuracy: 0.9676 - val_loss: 0.5597 - val_accuracy: 0.8742\nEpoch 163/200\n240/240 [==============================] - 0s 819us/step - loss: 0.0917 - accuracy: 0.9687 - val_loss: 0.5687 - val_accuracy: 0.8735\nEpoch 164/200\n240/240 [==============================] - 0s 798us/step - loss: 0.0949 - accuracy: 0.9668 - val_loss: 0.5754 - val_accuracy: 0.8726\nEpoch 165/200\n240/240 [==============================] - 0s 813us/step - loss: 0.0897 - accuracy: 0.9697 - val_loss: 0.5657 - val_accuracy: 0.8748\nEpoch 166/200\n240/240 [==============================] - 0s 802us/step - loss: 0.0940 - accuracy: 0.9679 - val_loss: 0.5834 - val_accuracy: 0.8741\nEpoch 167/200\n240/240 [==============================] - 0s 796us/step - loss: 0.0900 - accuracy: 0.9686 - val_loss: 0.5859 - val_accuracy: 0.8726\nEpoch 168/200\n240/240 [==============================] - 0s 813us/step - loss: 0.0936 - accuracy: 0.9674 - val_loss: 0.5875 - val_accuracy: 0.8743\nEpoch 169/200\n240/240 [==============================] - 0s 813us/step - loss: 0.0897 - accuracy: 0.9680 - val_loss: 0.5895 - val_accuracy: 0.8723\nEpoch 170/200\n240/240 [==============================] - 0s 811us/step - loss: 0.0902 - accuracy: 0.9694 - val_loss: 0.5747 - val_accuracy: 0.8746\nEpoch 171/200\n240/240 [==============================] - 0s 805us/step - loss: 0.0883 - accuracy: 0.9695 - val_loss: 0.5907 - val_accuracy: 0.8733\nEpoch 172/200\n240/240 [==============================] - 0s 806us/step - loss: 0.0894 - accuracy: 0.9689 - val_loss: 0.6095 - val_accuracy: 0.8688\nEpoch 173/200\n240/240 [==============================] - 0s 820us/step - loss: 0.0867 - accuracy: 0.9701 - val_loss: 0.5941 - val_accuracy: 0.8715\nEpoch 174/200\n240/240 [==============================] - 0s 820us/step - loss: 0.0857 - accuracy: 0.9709 - val_loss: 0.6018 - val_accuracy: 0.8723\nEpoch 175/200\n240/240 [==============================] - 0s 806us/step - loss: 0.0879 - accuracy: 0.9698 - val_loss: 0.6147 - val_accuracy: 0.8708\nEpoch 176/200\n240/240 [==============================] - 0s 814us/step - loss: 0.0879 - accuracy: 0.9694 - val_loss: 0.6056 - val_accuracy: 0.8727\nEpoch 177/200\n240/240 [==============================] - 0s 827us/step - loss: 0.0880 - accuracy: 0.9689 - val_loss: 0.5992 - val_accuracy: 0.8748\nEpoch 178/200\n240/240 [==============================] - 0s 811us/step - loss: 0.0860 - accuracy: 0.9699 - val_loss: 0.6020 - val_accuracy: 0.8721\nEpoch 179/200\n240/240 [==============================] - 0s 796us/step - loss: 0.0855 - accuracy: 0.9708 - val_loss: 0.6206 - val_accuracy: 0.8705\nEpoch 180/200\n240/240 [==============================] - 0s 797us/step - loss: 0.0869 - accuracy: 0.9697 - val_loss: 0.6087 - val_accuracy: 0.8749\nEpoch 181/200\n240/240 [==============================] - 0s 810us/step - loss: 0.0919 - accuracy: 0.9678 - val_loss: 0.6212 - val_accuracy: 0.8709\nEpoch 182/200\n240/240 [==============================] - 0s 801us/step - loss: 0.0852 - accuracy: 0.9710 - val_loss: 0.6057 - val_accuracy: 0.8740\nEpoch 183/200\n240/240 [==============================] - 0s 810us/step - loss: 0.0830 - accuracy: 0.9720 - val_loss: 0.6209 - val_accuracy: 0.8707\nEpoch 184/200\n240/240 [==============================] - 0s 820us/step - loss: 0.0811 - accuracy: 0.9730 - val_loss: 0.6288 - val_accuracy: 0.8722\nEpoch 185/200\n240/240 [==============================] - 0s 808us/step - loss: 0.0847 - accuracy: 0.9700 - val_loss: 0.6498 - val_accuracy: 0.8694\nEpoch 186/200\n240/240 [==============================] - 0s 813us/step - loss: 0.0851 - accuracy: 0.9702 - val_loss: 0.6333 - val_accuracy: 0.8733\nEpoch 187/200\n240/240 [==============================] - 0s 810us/step - loss: 0.0827 - accuracy: 0.9716 - val_loss: 0.6300 - val_accuracy: 0.8734\nEpoch 188/200\n240/240 [==============================] - 0s 796us/step - loss: 0.0841 - accuracy: 0.9713 - val_loss: 0.6223 - val_accuracy: 0.8733\nEpoch 189/200\n240/240 [==============================] - 0s 830us/step - loss: 0.0866 - accuracy: 0.9702 - val_loss: 0.6512 - val_accuracy: 0.8692\nEpoch 190/200\n240/240 [==============================] - 0s 787us/step - loss: 0.0820 - accuracy: 0.9718 - val_loss: 0.6240 - val_accuracy: 0.8725\nEpoch 191/200\n240/240 [==============================] - 0s 807us/step - loss: 0.0777 - accuracy: 0.9738 - val_loss: 0.6435 - val_accuracy: 0.8706\nEpoch 192/200\n240/240 [==============================] - 0s 821us/step - loss: 0.0776 - accuracy: 0.9746 - val_loss: 0.6375 - val_accuracy: 0.8732\nEpoch 193/200\n240/240 [==============================] - 0s 806us/step - loss: 0.0776 - accuracy: 0.9735 - val_loss: 0.6398 - val_accuracy: 0.8725\nEpoch 194/200\n240/240 [==============================] - 0s 794us/step - loss: 0.0829 - accuracy: 0.9714 - val_loss: 0.6403 - val_accuracy: 0.8706\nEpoch 195/200\n240/240 [==============================] - 0s 817us/step - loss: 0.0774 - accuracy: 0.9732 - val_loss: 0.6408 - val_accuracy: 0.8734\nEpoch 196/200\n240/240 [==============================] - 0s 824us/step - loss: 0.0789 - accuracy: 0.9729 - val_loss: 0.6458 - val_accuracy: 0.8701\nEpoch 197/200\n240/240 [==============================] - 0s 814us/step - loss: 0.0781 - accuracy: 0.9732 - val_loss: 0.6451 - val_accuracy: 0.8712\nEpoch 198/200\n240/240 [==============================] - 0s 817us/step - loss: 0.0817 - accuracy: 0.9715 - val_loss: 0.6450 - val_accuracy: 0.8699\nEpoch 199/200\n240/240 [==============================] - 0s 821us/step - loss: 0.0783 - accuracy: 0.9727 - val_loss: 0.6496 - val_accuracy: 0.8720\nEpoch 200/200\n240/240 [==============================] - 0s 815us/step - loss: 0.0789 - accuracy: 0.9730 - val_loss: 0.6477 - val_accuracy: 0.8722\n\n\n&lt;keras.callbacks.History at 0x7f6da1afa280&gt;\n\n\n- 텐서보드 여는 방법1\n\n%load_ext tensorboard\n# 주피터노트북 (혹은 주피터랩)에서 텐서보드를 임베딩하여 넣을 수 있도록 도와주는 매직펑션\n\n\n#\n# !rm -rf logs\n# !kill 313799\n\n\n#\n# %tensorboard --logdir logs --host 0.0.0.0\n# %tensorboard --logdir logs # &lt;-- 실습에서는 이렇게 하면됩니다. \n\n(참고사항) 파이썬 3.10의 경우 아래의 수정이 필요\n?/python3.10/site-packages/tensorboard/_vendor/html5lib/_trie/_base.py 을 열고\nfrom collections import Mapping ### 수정전\nfrom collections.abc import Mapping ### 수정후 \n와 같이 수정한다.\n\n왜냐하면 파이썬 3.10부터 from collections import Mapping 가 동작하지 않고 from collections.abc import Mapping 가 동작하도록 문법이 바뀜\n\n- 텐서보드를 실행하는 방법2\n\n#\n# !tensorboard --logdir logs --host 0.0.0.0\n# !tensorboard --logdir logs # &lt;-- 실습에서는 이렇게 하면됩니다. \n\n\n\n조기종료\n- 텐서보드를 살펴보니 특정에폭 이후에는 오히려 과적합이 진행되는 듯 하다 (학습할수록 손해인듯 하다) \\(\\to\\) 그 특정에폭까지만 학습해보자\n\ntf.random.set_seed(43052)\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Flatten())\nnet.add(tf.keras.layers.Dense(5000,activation='relu')) ## 과적합좀 시키려고 \nnet.add(tf.keras.layers.Dense(5000,activation='relu')) ## 레이어를 2장만듬 + 레이어하나당 노드수도 증가 \nnet.add(tf.keras.layers.Dense(10,activation='softmax'))\nnet.compile(optimizer='adam',loss=tf.losses.categorical_crossentropy,metrics='accuracy')\n\n\n#\n#cb1 = tf.keras.callbacks.TensorBoard()\ncb2 = tf.keras.callbacks.EarlyStopping(patience=1) # val-loss가 1회증가하면 멈추어라 \nnet.fit(X,y,epochs=200,batch_size=200,validation_split=0.2,callbacks=cb2,verbose=1) \n\nEpoch 1/200\n240/240 [==============================] - 1s 4ms/step - loss: 0.5483 - accuracy: 0.8134 - val_loss: 0.4027 - val_accuracy: 0.8546\nEpoch 2/200\n240/240 [==============================] - 1s 3ms/step - loss: 0.3568 - accuracy: 0.8671 - val_loss: 0.3531 - val_accuracy: 0.8712\nEpoch 3/200\n240/240 [==============================] - 1s 3ms/step - loss: 0.3210 - accuracy: 0.8799 - val_loss: 0.3477 - val_accuracy: 0.8733\nEpoch 4/200\n240/240 [==============================] - 1s 3ms/step - loss: 0.2971 - accuracy: 0.8876 - val_loss: 0.3502 - val_accuracy: 0.8776\n\n\n&lt;keras.callbacks.History at 0x7f1b80086650&gt;\n\n\n\n#\n#cb1 = tf.keras.callbacks.TensorBoard()\ncb2 = tf.keras.callbacks.EarlyStopping(patience=1) # val-loss가 1회증가하면 멈추어라 \nnet.fit(X,y,epochs=200,batch_size=200,validation_split=0.2,callbacks=cb2,verbose=1) \n\nEpoch 1/200\n240/240 [==============================] - 1s 3ms/step - loss: 0.2791 - accuracy: 0.8935 - val_loss: 0.3224 - val_accuracy: 0.8820\nEpoch 2/200\n240/240 [==============================] - 1s 3ms/step - loss: 0.2619 - accuracy: 0.8999 - val_loss: 0.3498 - val_accuracy: 0.8779\n\n\n&lt;keras.callbacks.History at 0x7f1b24290a90&gt;\n\n\n\n#\n#cb1 = tf.keras.callbacks.TensorBoard()\ncb2 = tf.keras.callbacks.EarlyStopping(patience=1) # val-loss가 1회증가하면 멈추어라 \nnet.fit(X,y,epochs=200,batch_size=200,validation_split=0.2,callbacks=cb2,verbose=1) \n\nEpoch 1/200\n240/240 [==============================] - 1s 3ms/step - loss: 0.2491 - accuracy: 0.9043 - val_loss: 0.3641 - val_accuracy: 0.8711\nEpoch 2/200\n240/240 [==============================] - 1s 3ms/step - loss: 0.2328 - accuracy: 0.9110 - val_loss: 0.3282 - val_accuracy: 0.8848\nEpoch 3/200\n240/240 [==============================] - 1s 3ms/step - loss: 0.2254 - accuracy: 0.9151 - val_loss: 0.3280 - val_accuracy: 0.8843\nEpoch 4/200\n240/240 [==============================] - 1s 3ms/step - loss: 0.2144 - accuracy: 0.9177 - val_loss: 0.3191 - val_accuracy: 0.8925\nEpoch 5/200\n240/240 [==============================] - 1s 3ms/step - loss: 0.2074 - accuracy: 0.9223 - val_loss: 0.3152 - val_accuracy: 0.8949\nEpoch 6/200\n240/240 [==============================] - 1s 3ms/step - loss: 0.1952 - accuracy: 0.9250 - val_loss: 0.3322 - val_accuracy: 0.8863\n\n\n&lt;keras.callbacks.History at 0x7f1b242c1660&gt;\n\n\n\n#\n#cb1 = tf.keras.callbacks.TensorBoard()\ncb2 = tf.keras.callbacks.EarlyStopping(patience=1) # val-loss가 1회증가하면 멈추어라 \nnet.fit(X,y,epochs=200,batch_size=200,validation_split=0.2,callbacks=cb2,verbose=1) \n\nEpoch 1/200\n240/240 [==============================] - 1s 3ms/step - loss: 0.1908 - accuracy: 0.9257 - val_loss: 0.3513 - val_accuracy: 0.8836\nEpoch 2/200\n240/240 [==============================] - 1s 3ms/step - loss: 0.1799 - accuracy: 0.9304 - val_loss: 0.3376 - val_accuracy: 0.8901\nEpoch 3/200\n240/240 [==============================] - 1s 3ms/step - loss: 0.1712 - accuracy: 0.9346 - val_loss: 0.3568 - val_accuracy: 0.8894\n\n\n&lt;keras.callbacks.History at 0x7f1b24302230&gt;\n\n\n\n#\n#cb1 = tf.keras.callbacks.TensorBoard()\ncb2 = tf.keras.callbacks.EarlyStopping(patience=1) # val-loss가 1회증가하면 멈추어라 \nnet.fit(X,y,epochs=200,batch_size=200,validation_split=0.2,callbacks=cb2,verbose=1) \n\nEpoch 1/200\n240/240 [==============================] - 1s 3ms/step - loss: 0.1591 - accuracy: 0.9367 - val_loss: 0.3995 - val_accuracy: 0.8780\nEpoch 2/200\n240/240 [==============================] - 1s 3ms/step - loss: 0.1552 - accuracy: 0.9398 - val_loss: 0.3469 - val_accuracy: 0.8917\nEpoch 3/200\n240/240 [==============================] - 1s 3ms/step - loss: 0.1481 - accuracy: 0.9423 - val_loss: 0.3726 - val_accuracy: 0.8853\n\n\n&lt;keras.callbacks.History at 0x7f1b24136e00&gt;\n\n\n- 몇 번 좀 참았다가 멈추면 좋겠다.\n\ntf.random.set_seed(43052)\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Flatten())\nnet.add(tf.keras.layers.Dense(5000,activation='relu')) ## 과적합좀 시키려고 \nnet.add(tf.keras.layers.Dense(5000,activation='relu')) ## 레이어를 2장만듬 + 레이어하나당 노드수도 증가 \nnet.add(tf.keras.layers.Dense(10,activation='softmax'))\nnet.compile(optimizer='adam',loss=tf.losses.categorical_crossentropy,metrics='accuracy')\n\n\n#\n#cb1 = tf.keras.callbacks.TensorBoard()\ncb2 = tf.keras.callbacks.EarlyStopping(patience=5) # 좀더 참다가 멈추어라 \nnet.fit(X,y,epochs=200,batch_size=200,validation_split=0.2,callbacks=cb2,verbose=1) \n\nEpoch 1/200\n240/240 [==============================] - 1s 4ms/step - loss: 0.5475 - accuracy: 0.8139 - val_loss: 0.4219 - val_accuracy: 0.8453\nEpoch 2/200\n240/240 [==============================] - 1s 3ms/step - loss: 0.3575 - accuracy: 0.8676 - val_loss: 0.3647 - val_accuracy: 0.8712\nEpoch 3/200\n240/240 [==============================] - 1s 3ms/step - loss: 0.3219 - accuracy: 0.8792 - val_loss: 0.3559 - val_accuracy: 0.8710\nEpoch 4/200\n240/240 [==============================] - 1s 3ms/step - loss: 0.2990 - accuracy: 0.8883 - val_loss: 0.3448 - val_accuracy: 0.8808\nEpoch 5/200\n240/240 [==============================] - 1s 3ms/step - loss: 0.2759 - accuracy: 0.8966 - val_loss: 0.3337 - val_accuracy: 0.8792\nEpoch 6/200\n240/240 [==============================] - 1s 3ms/step - loss: 0.2621 - accuracy: 0.9004 - val_loss: 0.3220 - val_accuracy: 0.8841\nEpoch 7/200\n240/240 [==============================] - 1s 3ms/step - loss: 0.2478 - accuracy: 0.9074 - val_loss: 0.3302 - val_accuracy: 0.8858\nEpoch 8/200\n240/240 [==============================] - 1s 3ms/step - loss: 0.2342 - accuracy: 0.9110 - val_loss: 0.3150 - val_accuracy: 0.8904\nEpoch 9/200\n240/240 [==============================] - 1s 3ms/step - loss: 0.2261 - accuracy: 0.9144 - val_loss: 0.3117 - val_accuracy: 0.8932\nEpoch 10/200\n240/240 [==============================] - 1s 3ms/step - loss: 0.2116 - accuracy: 0.9200 - val_loss: 0.3345 - val_accuracy: 0.8888\nEpoch 11/200\n240/240 [==============================] - 1s 3ms/step - loss: 0.2081 - accuracy: 0.9207 - val_loss: 0.3344 - val_accuracy: 0.8867\nEpoch 12/200\n240/240 [==============================] - 1s 3ms/step - loss: 0.1956 - accuracy: 0.9255 - val_loss: 0.3158 - val_accuracy: 0.8975\nEpoch 13/200\n240/240 [==============================] - 1s 3ms/step - loss: 0.1863 - accuracy: 0.9275 - val_loss: 0.3302 - val_accuracy: 0.8934\nEpoch 14/200\n240/240 [==============================] - 1s 3ms/step - loss: 0.1764 - accuracy: 0.9324 - val_loss: 0.3717 - val_accuracy: 0.8859\n\n\n&lt;keras.callbacks.History at 0x7f1b24301960&gt;\n\n\n- 텐서보드로 그려보자?\n\n#\n# %tensorboard --logdir logs --host 0.0.0.0 \n# 아무것도 안나온다 -&gt; 왜? cb1을 써야 텐서보드가 나옴\n\n- 조기종료와 텐서보드를 같이 쓰려면?\n\ntf.random.set_seed(43052)\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Flatten())\nnet.add(tf.keras.layers.Dense(50,activation='relu')) \nnet.add(tf.keras.layers.Dense(10,activation='softmax'))\nnet.compile(optimizer='adam',loss=tf.losses.categorical_crossentropy,metrics='accuracy')\n\n\n callback에 cb1, cb2,를 리스트로 같이 전달하면 된다.\n\n\ncb1 = tf.keras.callbacks.TensorBoard()\ncb2 = tf.keras.callbacks.EarlyStopping(patience=7) # 좀더 참다가 멈추어라 \nnet.fit(X,y,epochs=200,batch_size=200,validation_split=0.2,callbacks=[cb1,cb2]) \n\nEpoch 1/200\n240/240 [==============================] - 0s 1ms/step - loss: 0.7184 - accuracy: 0.7581 - val_loss: 0.5077 - val_accuracy: 0.8276\nEpoch 2/200\n240/240 [==============================] - 0s 890us/step - loss: 0.4752 - accuracy: 0.8386 - val_loss: 0.4793 - val_accuracy: 0.8342\nEpoch 3/200\n240/240 [==============================] - 0s 899us/step - loss: 0.4304 - accuracy: 0.8517 - val_loss: 0.4386 - val_accuracy: 0.8497\nEpoch 4/200\n240/240 [==============================] - 0s 880us/step - loss: 0.4048 - accuracy: 0.8582 - val_loss: 0.4029 - val_accuracy: 0.8603\nEpoch 5/200\n240/240 [==============================] - 0s 923us/step - loss: 0.3832 - accuracy: 0.8669 - val_loss: 0.3932 - val_accuracy: 0.8619\nEpoch 6/200\n240/240 [==============================] - 0s 934us/step - loss: 0.3697 - accuracy: 0.8705 - val_loss: 0.3842 - val_accuracy: 0.8657\nEpoch 7/200\n240/240 [==============================] - 0s 900us/step - loss: 0.3569 - accuracy: 0.8759 - val_loss: 0.3844 - val_accuracy: 0.8668\nEpoch 8/200\n240/240 [==============================] - 0s 889us/step - loss: 0.3482 - accuracy: 0.8774 - val_loss: 0.3679 - val_accuracy: 0.8708\nEpoch 9/200\n240/240 [==============================] - 0s 912us/step - loss: 0.3387 - accuracy: 0.8799 - val_loss: 0.3602 - val_accuracy: 0.8719\nEpoch 10/200\n240/240 [==============================] - 0s 923us/step - loss: 0.3299 - accuracy: 0.8820 - val_loss: 0.3610 - val_accuracy: 0.8748\nEpoch 11/200\n240/240 [==============================] - 0s 853us/step - loss: 0.3229 - accuracy: 0.8858 - val_loss: 0.3574 - val_accuracy: 0.8717\nEpoch 12/200\n240/240 [==============================] - 0s 904us/step - loss: 0.3157 - accuracy: 0.8873 - val_loss: 0.3572 - val_accuracy: 0.8743\nEpoch 13/200\n240/240 [==============================] - 0s 890us/step - loss: 0.3106 - accuracy: 0.8899 - val_loss: 0.3545 - val_accuracy: 0.8761\nEpoch 14/200\n240/240 [==============================] - 0s 911us/step - loss: 0.3046 - accuracy: 0.8914 - val_loss: 0.3493 - val_accuracy: 0.8759\nEpoch 15/200\n240/240 [==============================] - 0s 921us/step - loss: 0.3011 - accuracy: 0.8928 - val_loss: 0.3483 - val_accuracy: 0.8776\nEpoch 16/200\n240/240 [==============================] - 0s 937us/step - loss: 0.2988 - accuracy: 0.8935 - val_loss: 0.3733 - val_accuracy: 0.8716\nEpoch 17/200\n240/240 [==============================] - 0s 892us/step - loss: 0.2925 - accuracy: 0.8947 - val_loss: 0.3481 - val_accuracy: 0.8768\nEpoch 18/200\n240/240 [==============================] - 0s 933us/step - loss: 0.2880 - accuracy: 0.8951 - val_loss: 0.3396 - val_accuracy: 0.8801\nEpoch 19/200\n240/240 [==============================] - 0s 957us/step - loss: 0.2827 - accuracy: 0.8982 - val_loss: 0.3439 - val_accuracy: 0.8798\nEpoch 20/200\n240/240 [==============================] - 0s 881us/step - loss: 0.2791 - accuracy: 0.8986 - val_loss: 0.3489 - val_accuracy: 0.8779\nEpoch 21/200\n240/240 [==============================] - 0s 886us/step - loss: 0.2765 - accuracy: 0.9007 - val_loss: 0.3350 - val_accuracy: 0.8823\nEpoch 22/200\n240/240 [==============================] - 0s 912us/step - loss: 0.2709 - accuracy: 0.9016 - val_loss: 0.3350 - val_accuracy: 0.8812\nEpoch 23/200\n240/240 [==============================] - 0s 908us/step - loss: 0.2688 - accuracy: 0.9029 - val_loss: 0.3374 - val_accuracy: 0.8820\nEpoch 24/200\n240/240 [==============================] - 0s 930us/step - loss: 0.2658 - accuracy: 0.9041 - val_loss: 0.3445 - val_accuracy: 0.8805\nEpoch 25/200\n240/240 [==============================] - 0s 872us/step - loss: 0.2607 - accuracy: 0.9058 - val_loss: 0.3383 - val_accuracy: 0.8822\nEpoch 26/200\n240/240 [==============================] - 0s 928us/step - loss: 0.2607 - accuracy: 0.9056 - val_loss: 0.3415 - val_accuracy: 0.8811\nEpoch 27/200\n240/240 [==============================] - 0s 927us/step - loss: 0.2576 - accuracy: 0.9068 - val_loss: 0.3402 - val_accuracy: 0.8814\nEpoch 28/200\n240/240 [==============================] - 0s 905us/step - loss: 0.2525 - accuracy: 0.9098 - val_loss: 0.3469 - val_accuracy: 0.8802\n\n\n&lt;keras.callbacks.History at 0x7f1b24217a00&gt;\n\n\n\n# \n# 조기종료가 구현된 그림이 출력\n# %tensorboard --logdir logs --host 0.0.0.0 \n\n\n\n하이퍼파라메터 선택\n- 하이퍼파라메터 설정\n\nfrom tensorboard.plugins.hparams import api as hp\n\n\na=net.evaluate(XX,yy)\n\n313/313 [==============================] - 0s 859us/step - loss: 0.3803 - accuracy: 0.8704\n\n\n\n!rm -rf logs\nfor u in [50,5000]: \n    for d in [0.0,0.5]: \n        for o in ['adam','sgd']:\n            logdir = 'logs/hpguebin_{}_{}_{}'.format(u,d,o)\n            with tf.summary.create_file_writer(logdir).as_default():\n                net = tf.keras.Sequential()\n                net.add(tf.keras.layers.Flatten())\n                net.add(tf.keras.layers.Dense(u,activation='relu'))\n                net.add(tf.keras.layers.Dropout(d))\n                net.add(tf.keras.layers.Dense(10,activation='softmax'))\n                net.compile(optimizer=o,loss=tf.losses.categorical_crossentropy,metrics=['accuracy','Recall'])\n                cb3 = hp.KerasCallback(logdir, {'유닛수':u, '드랍아웃비율':d, '옵티마이저':o})\n                net.fit(X,y,epochs=3,callbacks=cb3)\n                _rslt=net.evaluate(XX,yy)\n                _mymetric=_rslt[1]*0.8 + _rslt[2]*0.2  \n                tf.summary.scalar('애큐러시와리컬의가중평균(테스트셋)', _mymetric, step=1) \n\nEpoch 1/3\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.5255 - accuracy: 0.8180 - recall: 0.7546\nEpoch 2/3\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.3993 - accuracy: 0.8588 - recall: 0.8294\nEpoch 3/3\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.3648 - accuracy: 0.8698 - recall: 0.8443\n313/313 [==============================] - 0s 830us/step - loss: 0.4063 - accuracy: 0.8545 - recall: 0.8286\nEpoch 1/3\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.7744 - accuracy: 0.7503 - recall: 0.5797\nEpoch 2/3\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.5204 - accuracy: 0.8223 - recall: 0.7565\nEpoch 3/3\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.4742 - accuracy: 0.8369 - recall: 0.7859\n313/313 [==============================] - 0s 828us/step - loss: 0.4899 - accuracy: 0.8304 - recall: 0.7831\nEpoch 1/3\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.7502 - accuracy: 0.7356 - recall: 0.6115\nEpoch 2/3\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.5738 - accuracy: 0.7923 - recall: 0.7133\nEpoch 3/3\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.5473 - accuracy: 0.8037 - recall: 0.7321\n313/313 [==============================] - 0s 865us/step - loss: 0.4319 - accuracy: 0.8448 - recall: 0.7919\nEpoch 1/3\n1875/1875 [==============================] - 2s 1ms/step - loss: 1.0932 - accuracy: 0.6228 - recall: 0.3971\nEpoch 2/3\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.7616 - accuracy: 0.7388 - recall: 0.5956\nEpoch 3/3\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.6828 - accuracy: 0.7684 - recall: 0.6478\n313/313 [==============================] - 0s 894us/step - loss: 0.5265 - accuracy: 0.8180 - recall: 0.7353\nEpoch 1/3\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.4777 - accuracy: 0.8292 - recall: 0.7890\nEpoch 2/3\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.3603 - accuracy: 0.8682 - recall: 0.8427\nEpoch 3/3\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.3197 - accuracy: 0.8817 - recall: 0.8605\n313/313 [==============================] - 0s 846us/step - loss: 0.3803 - accuracy: 0.8628 - recall: 0.8428\nEpoch 1/3\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.6685 - accuracy: 0.7883 - recall: 0.6444\nEpoch 2/3\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.4815 - accuracy: 0.8372 - recall: 0.7781\nEpoch 3/3\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.4408 - accuracy: 0.8498 - recall: 0.8021\n313/313 [==============================] - 0s 859us/step - loss: 0.4634 - accuracy: 0.8390 - recall: 0.7962\nEpoch 1/3\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.5708 - accuracy: 0.7991 - recall: 0.7556\nEpoch 2/3\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.4418 - accuracy: 0.8393 - recall: 0.8057\nEpoch 3/3\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.4091 - accuracy: 0.8514 - recall: 0.8211\n313/313 [==============================] - 0s 850us/step - loss: 0.3937 - accuracy: 0.8587 - recall: 0.8238\nEpoch 1/3\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.6930 - accuracy: 0.7752 - recall: 0.6338\nEpoch 2/3\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.5048 - accuracy: 0.8274 - recall: 0.7651\nEpoch 3/3\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.4608 - accuracy: 0.8417 - recall: 0.7910\n313/313 [==============================] - 0s 854us/step - loss: 0.4625 - accuracy: 0.8396 - recall: 0.7957\n\n\n\n#\n#%tensorboard --logdir logs --host 0.0.0.0"
  },
  {
    "objectID": "posts/2022-05-23-(12주차) 5월23일.html#숙제",
    "href": "posts/2022-05-23-(12주차) 5월23일.html#숙제",
    "title": "[STBDA] 12wk: CONV,MAXPOOL,CNN",
    "section": "숙제",
    "text": "숙제\n- 아래의 네트워크에서 옵티마이저를 adam, sgd를 선택하여 각각 적합시켜보고 testset의 loss를 성능비교를 하라. epoch은 5정도로 설정하라.\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Flatten())\nnet.add(tf.keras.layers.Dense(50,activation='relu'))\nnet.add(tf.keras.layers.Dense(50,activation='relu'))\nnet.add(tf.keras.layers.Dense(10,activation='softmax'))\nnet.compile(optimizer=???,loss=tf.losses.categorical_crossentropy,metrics=['accuracy','Recall'])"
  },
  {
    "objectID": "posts/2022_03_07_(1주차)_3월7일.html",
    "href": "posts/2022_03_07_(1주차)_3월7일.html",
    "title": "[STBDA] 01wk: 단순선형회귀",
    "section": "",
    "text": "해당 강의노트는 전북대학교 최규빈교수님 STBDA2022 자료임\n\n\n강의보충자료\n- https://github.com/guebin/STBDA2022/blob/master/_notebooks/2022-03-07-supp1.pdf\n- https://github.com/guebin/STBDA2022/blob/master/_notebooks/2022-03-07-supp2.pdf\n\n\n로드맵\n- 오늘수업할내용: 단순선형회귀\n- 단순선형회귀를 배우는 이유?\n\n우리가 배우고싶은것: 심층신경망(DNN) \\(\\to\\) 합성곱신경망(CNN) \\(\\to\\) 적대적생성신경망(GAN)\n심층신경망을 바로 이해하기 어려움\n다음의 과정으로 이해해야함: (선형대수학 \\(\\to\\)) 회귀분석 \\(\\to\\) 로지스틱회귀분석 \\(\\to\\) 심층신경망\n\n\n\n선형회귀\n- 상황극 - 나는 동네에 커피점을 하나 차렸음. - 장사를 하다보니까 날이 더울수록 아이스아메리카노의 판매량이 증가한다는 사실을 깨달았다. - 일기예보는 미리 나와있으니까 그 정보를 잘 이용하면 ‘온도 -&gt; 아이스아메리카노 판매량 예측’ 이 가능할것 같다. (내가 앞으로 얼마나 벌지 예측가능)\n- 가짜자료 생성\n\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\n\n2023-06-16 10:41:22.228223: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n\n\n온도 \\({\\bf x}\\)가 아래와 같다고 하자.\n\nx=tf.constant([20.1, 22.2, 22.7, 23.3, 24.4, 25.1, 26.2, 27.3, 28.4, 30.4]) # 기온\nx\n\n&lt;tf.Tensor: shape=(10,), dtype=float32, numpy=\narray([20.1, 22.2, 22.7, 23.3, 24.4, 25.1, 26.2, 27.3, 28.4, 30.4],\n      dtype=float32)&gt;\n\n\n아이스아메리카노의 판매량 \\({\\bf y}\\)이 아래와 같다고 하자. (판매량은 정수로 나오겠지만 편의상 소수점도 가능하다고 생각하자)\n\\[{\\bf y} \\approx 10.2 +2.2 {\\bf x}\\]\n\n여기에서 10.2, 2.2 의 숫자는 제가 임의로 정한것임\n식의의미: 온도가 0일때 10.2잔정도 팔림 + 온도가 1도 증가하면 2.2잔정도 더 팔림\n물결의의미: 현실반영. 세상은 꼭 수식대로 정확하게 이루어지지 않음.\n\n\ntf.random.set_seed(43052)\nepsilon=tf.random.normal([10])\ny=10.2 + 2.2*x + epsilon\ny\n\n&lt;tf.Tensor: shape=(10,), dtype=float32, numpy=\narray([55.418365, 58.194283, 61.230827, 62.312557, 63.107002, 63.69569 ,\n       67.247055, 71.4365  , 73.1013  , 77.84988 ], dtype=float32)&gt;\n\n\n- 우리는 아래와 같은 자료를 모았다고 생각하자.\n\ntf.transpose(tf.concat([[x],[y]],0))\n\n&lt;tf.Tensor: shape=(10, 2), dtype=float32, numpy=\narray([[20.1     , 55.418365],\n       [22.2     , 58.194283],\n       [22.7     , 61.230827],\n       [23.3     , 62.312557],\n       [24.4     , 63.107002],\n       [25.1     , 63.69569 ],\n       [26.2     , 67.247055],\n       [27.3     , 71.4365  ],\n       [28.4     , 73.1013  ],\n       [30.4     , 77.84988 ]], dtype=float32)&gt;\n\n\n- 그려보자.\n\nplt.plot(x,y,'.') # 파란점, 관측한 데이터\nplt.plot(x,10.2 + 2.2*x, '--')  # 주황색점선, 세상의 법칙\n\n\n\n\n- 우리의 목표: 파란색점 \\(\\to\\) 주황색점선을 추론 // 데이터를 바탕으로 세상의 법칙을 추론\n- 아이디어: 데이터를 보니까 \\(x\\)와 \\(y\\)가 선형의 관계에 있는듯 보인다. 즉 모든 \\(i=1,2,\\dots, 10\\)에 대하여 아래를 만족하는 적당한 a,b (혹은 \\(\\beta_0,\\beta_1\\)) 가 존재할것 같다. - \\(y_{i} \\approx ax_{i}+b\\) - \\(y_{i} \\approx \\beta_1 x_{i}+\\beta_0\\)\n- 어림짐작으로 \\(a,b\\)를 알아내보자.\n데이터를 살펴보자.\n\ntf.transpose(tf.concat([[x],[y]],0))\n\n&lt;tf.Tensor: shape=(10, 2), dtype=float32, numpy=\narray([[20.1     , 55.418365],\n       [22.2     , 58.194283],\n       [22.7     , 61.230827],\n       [23.3     , 62.312557],\n       [24.4     , 63.107002],\n       [25.1     , 63.69569 ],\n       [26.2     , 67.247055],\n       [27.3     , 71.4365  ],\n       [28.4     , 73.1013  ],\n       [30.4     , 77.84988 ]], dtype=float32)&gt;\n\n\n적당히 왼쪽*2+15 = 오른쪽의 관계가 성립하는것 같다.\n따라서 \\(a=2, b=15\\) 혹은 \\(\\beta_0=15, \\beta_1=2\\) 로 추론할 수 있겠다.\n- 누군가가 \\((\\beta_0,\\beta_1)=(14,2)\\) 이라고 주장할 수 있다. (어차피 지금은 감각으로 추론하는 과정이니까)\n- 새로운 주장으로 인해서 \\((\\beta_0,\\beta_1)=(15,2)\\) 로 볼 수도 있고 \\((\\beta_0,\\beta_1)=(14,2)\\) 로 볼 수도 있다. 이중에서 어떠한 추정치가 좋은지 판단할 수 있을까? - 후보1: \\((\\beta_0,\\beta_1)=(15,2)\\) - 후보2: \\((\\beta_0,\\beta_1)=(14,2)\\)\n- 가능한 \\(y_i \\approx \\beta_0 + \\beta_1 x_i\\) 이 되도록 만드는 \\((\\beta_0,\\beta_1)\\) 이 좋을 것이다. \\(\\to\\) 후보 1,2를 비교해보자.\n(관찰에 의한 비교)\n후보1에 대해서 \\(i=1,2\\)를 넣고 관찰하여 보자.\n\n20.1 * 2 + 15 , 55.418365 # i=1\n\n(55.2, 55.418365)\n\n\n\n22.2 * 2 + 15 , 58.194283 # i=2\n\n(59.4, 58.194283)\n\n\n후보2에 대하여 \\(i=1,2\\)를 넣고 관찰하여 보자.\n\n20.1 * 2 + 14 , 55.418365 # i=1\n\n(54.2, 55.418365)\n\n\n\n22.2 * 2 + 14 , 58.194283 # i=2\n\n(58.4, 58.194283)\n\n\n\\(i=1\\)인 경우에는 후보1이 더 잘맞는것 같은데 \\(i=2\\)인 경우는 후보2가 더 잘맞는것 같다.\n(좀 더 체계적인 비교)\n\\(i=1,2,3, \\dots, 10\\) 에서 후보1과 후보2중 어떤것이 더 좋은지 비교하는 체계적인 방법을 생각해보자.\n후보 1,2에 대하여 \\(\\sum_{i=1}^{10} (y_i -\\beta_0 -\\beta_1 x_i)^2\\)를 계산하여 비교해보자.\n\nsum1=0\nfor i in range(10):\n    sum1=sum1+(y[i]-15-2*x[i])**2\n\n\nsum2=0\nfor i in range(10):\n    sum2=sum2+(y[i]-14-2*x[i])**2\n\n\nsum1,sum2\n\n(&lt;tf.Tensor: shape=(), dtype=float32, numpy=14.734169&gt;,\n &lt;tf.Tensor: shape=(), dtype=float32, numpy=31.521088&gt;)\n\n\n후보1이 더 \\(\\sum_{i=1}^{10} (y_i -\\beta_0 -\\beta_1 x_i)^2\\)의 값이 작다.\n후보1이 종합적으로 후보2에 비하여 좋다. 이 과정을 무한번 반복하면 최적의 추정치를 찾을 수 있다.\n- 그런데 이 알고리즘은 현실적으로 구현이 불가능하다. (무한번 계산하기도 힘들고, 언제 멈출지도 애매함)\n- 수학을 이용해서 좀 더 체계적으로 찾아보자. 결국 아래식을 가장 작게 만드는 \\(\\beta_0,\\beta_1\\)을 찾으면 된다.\n\\(\\sum_{i=1}^{10} (y_i -\\beta_0 -\\beta_1 x_i)^2\\)\n그런데 결국 \\(\\beta_0, \\beta_1\\)에 대한 이차식인데 이 식을 최소화하는 \\(\\beta_0,\\beta_1\\)을 구하기 위해서는 아래를 연립하여 풀면된다.\n\\(\\begin{cases} \\frac{\\partial}{\\partial \\beta_0}\\sum_{i=1}^{10} (y_i -\\beta_0 -\\beta_1 x_i)^2=0 \\\\ \\frac{\\partial}{\\partial \\beta_1}\\sum_{i=1}^{10} (y_i -\\beta_0 -\\beta_1 x_i)^2=0 \\end{cases}\\)\n- 풀어보자.\n\\(\\begin{cases} \\sum_{i=1}^{10} -2(y_i -\\beta_0 -\\beta_1 x_i)=0 \\\\ \\sum_{i=1}^{10} -2x_i(y_i -\\beta_0 -\\beta_1 x_i)=0 \\end{cases}\\)\n정리하면\n\\[\\hat{\\beta}_0= \\bar{y}-\\hat{\\beta}_1 \\bar{x}\\]\n\\[\\hat{\\beta}_1= \\frac{S_{xy}}{S_{xx}}=\\frac{\\sum_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^{n}(x_i-\\bar{x})^2}\\]\n- 따라서 최적의 추정치 \\((\\hat{\\beta}_0,\\hat{\\beta}_1)\\)를 이용한 추세선을 아래와 같이 계산할 수 있음.\n\nSxx= sum((x-sum(x)/10)**2)\nSxx\n\n&lt;tf.Tensor: shape=(), dtype=float32, numpy=87.848976&gt;\n\n\n\nSxy=  sum((x-sum(x)/10)*(y-sum(y)/10))\nSxy\n\n&lt;tf.Tensor: shape=(), dtype=float32, numpy=194.64737&gt;\n\n\n\nbeta1_estimated = Sxy/Sxx\nbeta1_estimated\n\n&lt;tf.Tensor: shape=(), dtype=float32, numpy=2.2157044&gt;\n\n\n\nbeta0_estimated = sum(y)/10 - beta1_estimated * sum(x)/10\nbeta0_estimated\n\n&lt;tf.Tensor: shape=(), dtype=float32, numpy=9.944572&gt;\n\n\n\nplt.plot(x,y,'.')\nplt.plot(x,beta0_estimated + beta1_estimated * x, '--') # 주황색선: 세상의 법칙을 추정한선\nplt.plot(x,10.2 + 2.2* x, '--') # 초록색선: ture, 세상의법칙\n\n\n\n\n\nNote: 샘플수가 커질수록 주황색선은 점점 초록색선으로 가까워진다.\n\n- 꽤 훌륭한 도구임. 그런데 약간의 단점이 존재한다.\n\n공식이 좀 복잡함..\n\\(x\\)가 여러개일 경우 확장이 어려움\n\n- 단점을 극복하기 위해서 우리가 지금까지 했던 논의를 매트릭스로 바꾸어서 다시 써보자.\n- 모형의 매트릭스화\n우리의 모형은 아래와 같다.\n\\(y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i, \\quad i=1,2,\\dots,10\\)\n풀어서 쓰면\n\\(\\begin{cases} y_1 = \\beta_0 +\\beta_1 x_1 + \\epsilon_1 \\\\ y_2 = \\beta_0 +\\beta_1 x_2 + \\epsilon_2 \\\\ \\dots \\\\ y_{10} = \\beta_0 +\\beta_1 x_{10} + \\epsilon_{10} \\end{cases}\\)\n아래와 같이 쓸 수 있다.\n\\(\\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\dots \\\\ y_{10} \\end{bmatrix} = \\begin{bmatrix} 1 & x_1 \\\\ 1 & x_2 \\\\ \\dots & \\dots \\\\ 1 & x_{10} \\end{bmatrix}\\begin{bmatrix}\\beta_0 \\\\ \\beta_1 \\end{bmatrix} + \\begin{bmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\dots \\\\ \\epsilon_{10} \\end{bmatrix}\\)\n벡터와 매트릭스 형태로 정리하면\n\\({\\bf y} = {\\bf X} {\\boldsymbol \\beta} + \\boldsymbol{\\epsilon}\\)\n- 손실함수의 매트릭스화: 우리가 최소화 하려던 손실함수는 아래와 같다.\n\\(loss=\\sum_{i=1}^{n}(y_i-\\beta_0-\\beta_1x_i)^2\\)\n이것을 벡터표현으로 하면 아래와 같다.\n\\(loss=\\sum_{i=1}^{n}(y_i-\\beta_0-\\beta_1x_i)^2=({\\bf y}-{\\bf X}{\\boldsymbol \\beta})^\\top({\\bf y}-{\\bf X}{\\boldsymbol \\beta})\\)\n풀어보면\n\\(loss=({\\bf y}-{\\bf X}{\\boldsymbol \\beta})^\\top({\\bf y}-{\\bf X}{\\boldsymbol \\beta})={\\bf y}^\\top {\\bf y} - {\\bf y}^\\top {\\bf X}{\\boldsymbol\\beta} - {\\boldsymbol\\beta}^\\top {\\bf X}^\\top {\\bf y} + {\\boldsymbol\\beta}^\\top {\\bf X}^\\top {\\bf X} {\\boldsymbol\\beta}\\)\n- 미분하는 과정의 매트릭스화\nloss를 최소화하는 \\({\\boldsymbol \\beta}\\)를 구해야하므로 loss를 \\({\\boldsymbol \\beta}\\)로 미분한식을 0이라고 놓고 풀면 된다.\n\\(\\frac{\\partial}{\\partial \\boldsymbol{\\beta}} loss = \\frac{\\partial}{\\partial \\boldsymbol{\\beta}} {\\bf y}^\\top {\\bf y} - \\frac{\\partial}{\\partial \\boldsymbol{\\beta}} {\\bf y}^\\top {\\bf X}{\\boldsymbol\\beta} - \\frac{\\partial}{\\partial \\boldsymbol{\\beta}} {\\boldsymbol\\beta}^\\top {\\bf X}^\\top {\\bf y} + \\frac{\\partial}{\\partial \\boldsymbol{\\beta}} {\\boldsymbol\\beta}^\\top {\\bf X}^\\top {\\bf X} {\\boldsymbol\\beta}\\)\n\\(= 0 - {\\bf X}^\\top {\\bf y}- {\\bf X}^\\top {\\bf y} + 2{\\bf X}^\\top {\\bf X}{\\boldsymbol\\beta}\\)\n따라서 \\(\\frac{\\partial}{\\partial \\boldsymbol{\\beta}}loss=0\\)을 풀면 아래와 같다.\n\\(\\boldsymbol{\\hat\\beta}= ({\\bf X}^\\top {\\bf X})^{-1}{\\bf X}^\\top {\\bf y}\\)\n- 공식도 매트릭스로 표현하면: \\(\\boldsymbol{\\hat\\beta}= ({\\bf X}^\\top {\\bf X})^{-1}{\\bf X}^\\top {\\bf y}\\) &lt;– 외우세요\n- 적용을 해보자.\n(X를 만드는 방법1)\n\nX=tf.transpose(tf.concat([[[1.0]*10],[x]],0)) #\nX\n\n&lt;tf.Tensor: shape=(10, 2), dtype=float32, numpy=\narray([[ 1. , 20.1],\n       [ 1. , 22.2],\n       [ 1. , 22.7],\n       [ 1. , 23.3],\n       [ 1. , 24.4],\n       [ 1. , 25.1],\n       [ 1. , 26.2],\n       [ 1. , 27.3],\n       [ 1. , 28.4],\n       [ 1. , 30.4]], dtype=float32)&gt;\n\n\n\n1.0을 10개 곱하고,, x를\n\n(X를 만드는 방법2)\n\nfrom tensorflow.python.ops.numpy_ops import np_config\nnp_config.enable_numpy_behavior()\n\n\nX=tf.concat([[[1.0]*10],[x]],0).T\nX\n\n&lt;tf.Tensor: shape=(10, 2), dtype=float32, numpy=\narray([[ 1. , 20.1],\n       [ 1. , 22.2],\n       [ 1. , 22.7],\n       [ 1. , 23.3],\n       [ 1. , 24.4],\n       [ 1. , 25.1],\n       [ 1. , 26.2],\n       [ 1. , 27.3],\n       [ 1. , 28.4],\n       [ 1. , 30.4]], dtype=float32)&gt;\n\n\n\ntf.linalg.inv(X.T @ X) @ X.T @ y\n\n&lt;tf.Tensor: shape=(2,), dtype=float32, numpy=array([9.944702, 2.215706], dtype=float32)&gt;\n\n\n- 잘 구해진다.\n- 그런데..\n\nbeta0_estimated,beta1_estimated\n\n(&lt;tf.Tensor: shape=(), dtype=float32, numpy=9.944572&gt;,\n &lt;tf.Tensor: shape=(), dtype=float32, numpy=2.2157044&gt;)\n\n\n값이 좀 다르다..?\n- 같은 값입니다! 신경쓰지 마세요! 텐서플로우가 좀 대충계산합니다.\n\nimport tensorflow.experimental.numpy as tnp\n\n\nx=tnp.array([20.1, 22.2, 22.7, 23.3, 24.4, 25.1, 26.2, 27.3, 28.4, 30.4])\ny=10.2 + 2.2*x + epsilon\n\n\nbeta1_estimated = sum((x-sum(x)/10)*(y-sum(y)/10)) / sum((x-sum(x)/10)**2) # Sxy * Sxx\nbeta0_estimated = sum(y)/10 - beta1_estimated * sum(x)/10\n\n\nbeta0_estimated, beta1_estimated\n\n(&lt;tf.Tensor: shape=(), dtype=float64, numpy=9.944573294798559&gt;,\n &lt;tf.Tensor: shape=(), dtype=float64, numpy=2.2157046054834106&gt;)\n\n\n\nX=tnp.concatenate([[tnp.array([1.0]*10)],[x]],0).T\ntf.linalg.inv(X.T @ X) @ X.T @ y\n\n&lt;tf.Tensor: shape=(2,), dtype=float64, numpy=array([9.94457329, 2.21570461])&gt;\n\n\n\n\n앞으로 할것\n- 선형대수학의 미분이론..\n- 실습 (tensorflow에서 매트릭스를 자유롭게 다루비)"
  },
  {
    "objectID": "posts/2022_03_14_(2주차)_3월14일.html",
    "href": "posts/2022_03_14_(2주차)_3월14일.html",
    "title": "[STBDA] 02wk: tensorflow_matrix",
    "section": "",
    "text": "해당 강의노트는 전북대학교 최규빈교수님 STBDA2022 자료임\n\n\nimport\n\nimport tensorflow as tf\nimport numpy as np\n\n2023-06-16 15:15:55.862384: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n\n\n\ntf.config.experimental.list_physical_devices('GPU')\n\n[]\n\n\n\n\ntf.constant\n\n예비학습: 중첩리스트\n- 리스트\n\nlst = [1,2,4,5,6]\nlst\n\n[1, 2, 4, 5, 6]\n\n\n\nlst[1] # 두번쨰원소\n\n2\n\n\n\nlst[-1] # 마지막원소\n\n6\n\n\n- (2,2) matrix 느낌의 list\n\nlst= [[1,2],[3,4]]\nlst\n\n[[1, 2], [3, 4]]\n\n\n위를 아래와 같은 매트릭스로 생각할수 있다.\n1 2\n3 4\n\nprint(lst[0][0]) # (1,1)\nprint(lst[0][1]) # (1,2)\nprint(lst[1][0]) # (2,1)\nprint(lst[1][1]) # (2,2)\n\n1\n2\n3\n4\n\n\n\nnp.array(lst)\n\narray([[1, 2],\n       [3, 4]])\n\n\n- (4,1) matrix 느낌의 list\n\nlst=[[1],[2],[3],[4]] # (4,1) matrix = 길이가 4인 col-vector\nlst\n\n[[1], [2], [3], [4]]\n\n\n\nnp.array(lst)\n\narray([[1],\n       [2],\n       [3],\n       [4]])\n\n\n- (1,4) matrix 느낌의 list\n\nlst=[[1,2,3,4]] # (1,4) matrix = 길이가 4인 row-vector\nlst\n\n[[1, 2, 3, 4]]\n\n\n\nnp.array(lst)\n\narray([[1, 2, 3, 4]])\n\n\n\n\n선언\n- 스칼라\n\ntf.constant(3.14)\n\n&lt;tf.Tensor: shape=(), dtype=float32, numpy=3.14&gt;\n\n\n\ntf.constant(3.14)+tf.constant(3.14)\n\n&lt;tf.Tensor: shape=(), dtype=float32, numpy=6.28&gt;\n\n\n- 벡터\n\n_vector=tf.constant([1,2,3])\n\n\n_vector[-1]\n\n&lt;tf.Tensor: shape=(), dtype=int32, numpy=3&gt;\n\n\n- 매트릭스\n\n_matrix= tf.constant([[1,0],[0,1]])\n_matrix\n\n&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[1, 0],\n       [0, 1]], dtype=int32)&gt;\n\n\n- array\n\ntf.constant([[[0,1,1],[1,2,-1]],[[0,1,2],[1,2,-1]]])\n\n&lt;tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy=\narray([[[ 0,  1,  1],\n        [ 1,  2, -1]],\n\n       [[ 0,  1,  2],\n        [ 1,  2, -1]]], dtype=int32)&gt;\n\n\n\n\n타입\n\ntype(tf.constant(3.14))\n\ntensorflow.python.framework.ops.EagerTensor\n\n\n\n\n인덱싱\n\n_matrix = tf.constant([[1,2],[3,4]])\n_matrix\n\n&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[1, 2],\n       [3, 4]], dtype=int32)&gt;\n\n\n\n_matrix[0][0]\n\n&lt;tf.Tensor: shape=(), dtype=int32, numpy=1&gt;\n\n\n\n_matrix[0]\n\n&lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 2], dtype=int32)&gt;\n\n\n\n_matrix[0,:]\n\n&lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 2], dtype=int32)&gt;\n\n\n\n_matrix[:,0] # column\n\n&lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 3], dtype=int32)&gt;\n\n\n\n\ntf.constant는 불편하다.\n- 불편한점 1. 모든 원소가 같은 dtype을 가지고 있어야함. 2. 원소 수정이 불가능함. 3. 묵시적 형변환이 불가능하다.\n- 원소수정이 불가능함\n\na=tf.constant([1,22,33])\na\n\n&lt;tf.Tensor: shape=(3,), dtype=int32, numpy=array([ 1, 22, 33], dtype=int32)&gt;\n\n\n\na[0]\n\n&lt;tf.Tensor: shape=(), dtype=int32, numpy=1&gt;\n\n\n\na[0]=11\n\nTypeError: 'tensorflow.python.framework.ops.EagerTensor' object does not support item assignment\n\n\n- 묵시적 형변환이 불가능하다\n\n1+3.14\n\n4.140000000000001\n\n\n\ntf.constant(1)+tf.constant(3.14)\n\nInvalidArgumentError: cannot compute AddV2 as input #1(zero-based) was expected to be a int32 tensor but is a float tensor [Op:AddV2]\n\n\n\nint형+float형 계산이 불가하다.\n\n\ntf.constant(1.0)+tf.constant(3.14)\n\n&lt;tf.Tensor: shape=(), dtype=float32, numpy=4.1400003&gt;\n\n\n\nfloat형+float형 으로 변경해서 계산해줘야함\n\n- 같은 float도 안되는 경우가 있음\n\ntf.constant(1.0,dtype=tf.float64)\n\n&lt;tf.Tensor: shape=(), dtype=float64, numpy=1.0&gt;\n\n\n\ntf.constant(3.14)\n\n&lt;tf.Tensor: shape=(), dtype=float32, numpy=3.14&gt;\n\n\n\ntf.constant(1.0,dtype=tf.float64)+tf.constant(3.14)\n\nInvalidArgumentError: cannot compute AddV2 as input #1(zero-based) was expected to be a double tensor but is a float tensor [Op:AddV2]\n\n\n\n\ntf.constant \\(\\to\\) 넘파이\n\ntf.constant(np.array([3.14,-3.14])+np.array([1,2]))\n\n&lt;tf.Tensor: shape=(2,), dtype=float64, numpy=array([ 4.14, -1.14])&gt;\n\n\n\nnp.array(tf.constant(1)) # 방법1\n\narray(1, dtype=int32)\n\n\n\na=tf.constant([3.14,-3.14])\ntype(a)\n\ntensorflow.python.framework.ops.EagerTensor\n\n\n\na.numpy()\n\narray([ 3.14, -3.14], dtype=float32)\n\n\n\n\n연산\n- 더하기\n\na=tf.constant([1,2])\nb=tf.constant([3,4])\na+b\n\n&lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([4, 6], dtype=int32)&gt;\n\n\n\ntf.add(a,b)\n\n&lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([4, 6], dtype=int32)&gt;\n\n\n- 곱하기\n\na=tf.constant([[1,2],[3,4]])\nb=tf.constant([[5,6],[7,8]])\na*b\n\n&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[ 5, 12],\n       [21, 32]], dtype=int32)&gt;\n\n\n\ntf.multiply(a,b)\n\n&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[ 5, 12],\n       [21, 32]], dtype=int32)&gt;\n\n\n- 매트릭스의곱\n\na=tf.constant([[1,0],[0,1]]) # (2,2)\nb=tf.constant([[5],[7]]) # (2,1)\na@b\n\n&lt;tf.Tensor: shape=(2, 1), dtype=int32, numpy=\narray([[5],\n       [7]], dtype=int32)&gt;\n\n\n\ntf.matmul(a,b)\n\n&lt;tf.Tensor: shape=(2, 1), dtype=int32, numpy=\narray([[5],\n       [7]], dtype=int32)&gt;\n\n\n- 역행렬\n\na=tf.constant([[1,0],[0,2]])\na\n\n&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[1, 0],\n       [0, 2]], dtype=int32)&gt;\n\n\n\ntf.linalg.inv(a)\n\nInvalidArgumentError: Value for attr 'T' of int32 is not in the list of allowed values: double, float, half, complex64, complex128\n    ; NodeDef: {{node MatrixInverse}}; Op&lt;name=MatrixInverse; signature=input:T -&gt; output:T; attr=adjoint:bool,default=false; attr=T:type,allowed=[DT_DOUBLE, DT_FLOAT, DT_HALF, DT_COMPLEX64, DT_COMPLEX128]&gt; [Op:MatrixInverse]\n\n\n\n1/2만들려먼 float로…\n\n\na=tf.constant([[1.0,0.0],[0.0,2.0]])\ntf.linalg.inv(a)\n\n&lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy=\narray([[1. , 0. ],\n       [0. , 0.5]], dtype=float32)&gt;\n\n\n- tf.linalg. + tab을 누르면 좋아보이는 연산들 많음\n\na=tf.constant([[1.0,2.0],[3.0,4.0]])\nprint(a)\ntf.linalg.det(a) #determinant:4-2\n\ntf.Tensor(\n[[1. 2.]\n [3. 4.]], shape=(2, 2), dtype=float32)\n\n\n&lt;tf.Tensor: shape=(), dtype=float32, numpy=-2.0&gt;\n\n\n\ntf.linalg.trace(a) # 대각선 원소의 합\n\n&lt;tf.Tensor: shape=(), dtype=float32, numpy=5.0&gt;\n\n\n\n\n형태변환\n- 기본: tf.reshape() 를 이용\n\na=tf.constant([1,2,3,4])\na\n\n&lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)&gt;\n\n\n\ntf.reshape(a,(4,1))\n\n&lt;tf.Tensor: shape=(4, 1), dtype=int32, numpy=\narray([[1],\n       [2],\n       [3],\n       [4]], dtype=int32)&gt;\n\n\n\ntf.reshape(a,(2,2))\n\n&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[1, 2],\n       [3, 4]], dtype=int32)&gt;\n\n\n\ntf.reshape(a,(2,2,1))\n\n&lt;tf.Tensor: shape=(2, 2, 1), dtype=int32, numpy=\narray([[[1],\n        [2]],\n\n       [[3],\n        [4]]], dtype=int32)&gt;\n\n\n- 다차원\n\na=tf.constant([1,2,3,4,5,6,7,8,9,10,11,12])\na\n\n&lt;tf.Tensor: shape=(12,), dtype=int32, numpy=array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12], dtype=int32)&gt;\n\n\n\ntf.reshape(a,(2,2,3))\n\n&lt;tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy=\narray([[[ 1,  2,  3],\n        [ 4,  5,  6]],\n\n       [[ 7,  8,  9],\n        [10, 11, 12]]], dtype=int32)&gt;\n\n\n\ntf.reshape(a,(4,3))\n\n&lt;tf.Tensor: shape=(4, 3), dtype=int32, numpy=\narray([[ 1,  2,  3],\n       [ 4,  5,  6],\n       [ 7,  8,  9],\n       [10, 11, 12]], dtype=int32)&gt;\n\n\n- tf.resh\n\na=tf.constant([1,2,3,4,5,6,7,8,9,10,11,12])\na\n\n&lt;tf.Tensor: shape=(12,), dtype=int32, numpy=array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12], dtype=int32)&gt;\n\n\n\ntf.reshape(a,(4,-1))\n\n&lt;tf.Tensor: shape=(4, 3), dtype=int32, numpy=\narray([[ 1,  2,  3],\n       [ 4,  5,  6],\n       [ 7,  8,  9],\n       [10, 11, 12]], dtype=int32)&gt;\n\n\n\ntf.reshape(a,(2,2,-1))\n\n&lt;tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy=\narray([[[ 1,  2,  3],\n        [ 4,  5,  6]],\n\n       [[ 7,  8,  9],\n        [10, 11, 12]]], dtype=int32)&gt;\n\n\n\nb=tf.reshape(a,(2,2,-1))\nb\n\n&lt;tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy=\narray([[[ 1,  2,  3],\n        [ 4,  5,  6]],\n\n       [[ 7,  8,  9],\n        [10, 11, 12]]], dtype=int32)&gt;\n\n\n\ntf.reshape(b,-1)\n\n&lt;tf.Tensor: shape=(12,), dtype=int32, numpy=array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12], dtype=int32)&gt;\n\n\n\n\n선언고급\n- 다른 자료형 (리스트나 넘파이)로 만들고 바꾸는것도 좋다.\n\nnp.diag([1,2,3,4])\n\narray([[1, 0, 0, 0],\n       [0, 2, 0, 0],\n       [0, 0, 3, 0],\n       [0, 0, 0, 4]])\n\n\n\ntf.constant(np.diag([1,2,3,4]))\n\n&lt;tf.Tensor: shape=(4, 4), dtype=int64, numpy=\narray([[1, 0, 0, 0],\n       [0, 2, 0, 0],\n       [0, 0, 3, 0],\n       [0, 0, 0, 4]])&gt;\n\n\n- tf.ones, tf.zeros\n\ntf.zeros([3,3])\n\n&lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=\narray([[0., 0., 0.],\n       [0., 0., 0.],\n       [0., 0., 0.]], dtype=float32)&gt;\n\n\n\ntf.reshape(tf.constant([0]*9),(3,3))\n\n&lt;tf.Tensor: shape=(3, 3), dtype=int32, numpy=\narray([[0, 0, 0],\n       [0, 0, 0],\n       [0, 0, 0]], dtype=int32)&gt;\n\n\n- range(10)\n\na=range(0,12)\ntf.constant(a)\n\n&lt;tf.Tensor: shape=(12,), dtype=int32, numpy=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11], dtype=int32)&gt;\n\n\n\ntf.constant(range(1,20,3))\n\n&lt;tf.Tensor: shape=(7,), dtype=int32, numpy=array([ 1,  4,  7, 10, 13, 16, 19], dtype=int32)&gt;\n\n\n- tf.linspace\n\ntf.linspace(0,1,10)\n\n&lt;tf.Tensor: shape=(10,), dtype=float64, numpy=\narray([0.        , 0.11111111, 0.22222222, 0.33333333, 0.44444444,\n       0.55555556, 0.66666667, 0.77777778, 0.88888889, 1.        ])&gt;\n\n\n\n\ntf.concat\n- (2,1) concat (2,1) =&gt; (2,2) - 두번째 축이 바뀌었다. =&gt; axis=1\n\na=tf.constant([[1],[2]])\nb=tf.constant([[3],[4]])\na,b\n\n(&lt;tf.Tensor: shape=(2, 1), dtype=int32, numpy=\n array([[1],\n        [2]], dtype=int32)&gt;,\n &lt;tf.Tensor: shape=(2, 1), dtype=int32, numpy=\n array([[3],\n        [4]], dtype=int32)&gt;)\n\n\n\ntf.concat([a,b],axis=1)\n\n&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[1, 3],\n       [2, 4]], dtype=int32)&gt;\n\n\n- (2,1) concat (2,1) =&gt; (4,1) - 첫번째 축이 바뀌었다. =&gt; axis=0\n\na=tf.constant([[1],[2]])\nb=tf.constant([[3],[4]])\na,b\n\n(&lt;tf.Tensor: shape=(2, 1), dtype=int32, numpy=\n array([[1],\n        [2]], dtype=int32)&gt;,\n &lt;tf.Tensor: shape=(2, 1), dtype=int32, numpy=\n array([[3],\n        [4]], dtype=int32)&gt;)\n\n\n\ntf.concat([a,b],axis=0)\n\n&lt;tf.Tensor: shape=(4, 1), dtype=int32, numpy=\narray([[1],\n       [2],\n       [3],\n       [4]], dtype=int32)&gt;\n\n\n- (1,2) concat (1,2) =&gt; (2,2) - 첫번째 // axis=0\n\na=tf.constant([[1,2]])\nb=tf.constant([[3,4]])\n\n\ntf.concat([a,b],axis=0)\n\n&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[1, 2],\n       [3, 4]], dtype=int32)&gt;\n\n\n- (1,2) concat (1,2) =&gt; (1,4) - 두번째 // axis=1\n\na=tf.constant([[1,2]])\nb=tf.constant([[3,4]])\n\n\ntf.concat([a,b],axis=1)\n\n&lt;tf.Tensor: shape=(1, 4), dtype=int32, numpy=array([[1, 2, 3, 4]], dtype=int32)&gt;\n\n\n- (2,3,4,5) concat (2,3,4,5) =&gt; (4,3,4,5) - 첫번째 // axis=0\n\na=tf.reshape(tf.constant(range(120)),(2,3,4,5))\nb=-a\n\n\ntf.concat([a,b],axis=0)\n\n&lt;tf.Tensor: shape=(4, 3, 4, 5), dtype=int32, numpy=\narray([[[[   0,    1,    2,    3,    4],\n         [   5,    6,    7,    8,    9],\n         [  10,   11,   12,   13,   14],\n         [  15,   16,   17,   18,   19]],\n\n        [[  20,   21,   22,   23,   24],\n         [  25,   26,   27,   28,   29],\n         [  30,   31,   32,   33,   34],\n         [  35,   36,   37,   38,   39]],\n\n        [[  40,   41,   42,   43,   44],\n         [  45,   46,   47,   48,   49],\n         [  50,   51,   52,   53,   54],\n         [  55,   56,   57,   58,   59]]],\n\n\n       [[[  60,   61,   62,   63,   64],\n         [  65,   66,   67,   68,   69],\n         [  70,   71,   72,   73,   74],\n         [  75,   76,   77,   78,   79]],\n\n        [[  80,   81,   82,   83,   84],\n         [  85,   86,   87,   88,   89],\n         [  90,   91,   92,   93,   94],\n         [  95,   96,   97,   98,   99]],\n\n        [[ 100,  101,  102,  103,  104],\n         [ 105,  106,  107,  108,  109],\n         [ 110,  111,  112,  113,  114],\n         [ 115,  116,  117,  118,  119]]],\n\n\n       [[[   0,   -1,   -2,   -3,   -4],\n         [  -5,   -6,   -7,   -8,   -9],\n         [ -10,  -11,  -12,  -13,  -14],\n         [ -15,  -16,  -17,  -18,  -19]],\n\n        [[ -20,  -21,  -22,  -23,  -24],\n         [ -25,  -26,  -27,  -28,  -29],\n         [ -30,  -31,  -32,  -33,  -34],\n         [ -35,  -36,  -37,  -38,  -39]],\n\n        [[ -40,  -41,  -42,  -43,  -44],\n         [ -45,  -46,  -47,  -48,  -49],\n         [ -50,  -51,  -52,  -53,  -54],\n         [ -55,  -56,  -57,  -58,  -59]]],\n\n\n       [[[ -60,  -61,  -62,  -63,  -64],\n         [ -65,  -66,  -67,  -68,  -69],\n         [ -70,  -71,  -72,  -73,  -74],\n         [ -75,  -76,  -77,  -78,  -79]],\n\n        [[ -80,  -81,  -82,  -83,  -84],\n         [ -85,  -86,  -87,  -88,  -89],\n         [ -90,  -91,  -92,  -93,  -94],\n         [ -95,  -96,  -97,  -98,  -99]],\n\n        [[-100, -101, -102, -103, -104],\n         [-105, -106, -107, -108, -109],\n         [-110, -111, -112, -113, -114],\n         [-115, -116, -117, -118, -119]]]], dtype=int32)&gt;\n\n\n- (2,3,4,5) concat (2,3,4,5) =&gt; (2,6,4,5) - 두번째 // axis=1\n\na=tf.reshape(tf.constant(range(120)),(2,3,4,5))\nb=-a\n\n\ntf.concat([a,b],axis=1)\n\n&lt;tf.Tensor: shape=(2, 6, 4, 5), dtype=int32, numpy=\narray([[[[   0,    1,    2,    3,    4],\n         [   5,    6,    7,    8,    9],\n         [  10,   11,   12,   13,   14],\n         [  15,   16,   17,   18,   19]],\n\n        [[  20,   21,   22,   23,   24],\n         [  25,   26,   27,   28,   29],\n         [  30,   31,   32,   33,   34],\n         [  35,   36,   37,   38,   39]],\n\n        [[  40,   41,   42,   43,   44],\n         [  45,   46,   47,   48,   49],\n         [  50,   51,   52,   53,   54],\n         [  55,   56,   57,   58,   59]],\n\n        [[   0,   -1,   -2,   -3,   -4],\n         [  -5,   -6,   -7,   -8,   -9],\n         [ -10,  -11,  -12,  -13,  -14],\n         [ -15,  -16,  -17,  -18,  -19]],\n\n        [[ -20,  -21,  -22,  -23,  -24],\n         [ -25,  -26,  -27,  -28,  -29],\n         [ -30,  -31,  -32,  -33,  -34],\n         [ -35,  -36,  -37,  -38,  -39]],\n\n        [[ -40,  -41,  -42,  -43,  -44],\n         [ -45,  -46,  -47,  -48,  -49],\n         [ -50,  -51,  -52,  -53,  -54],\n         [ -55,  -56,  -57,  -58,  -59]]],\n\n\n       [[[  60,   61,   62,   63,   64],\n         [  65,   66,   67,   68,   69],\n         [  70,   71,   72,   73,   74],\n         [  75,   76,   77,   78,   79]],\n\n        [[  80,   81,   82,   83,   84],\n         [  85,   86,   87,   88,   89],\n         [  90,   91,   92,   93,   94],\n         [  95,   96,   97,   98,   99]],\n\n        [[ 100,  101,  102,  103,  104],\n         [ 105,  106,  107,  108,  109],\n         [ 110,  111,  112,  113,  114],\n         [ 115,  116,  117,  118,  119]],\n\n        [[ -60,  -61,  -62,  -63,  -64],\n         [ -65,  -66,  -67,  -68,  -69],\n         [ -70,  -71,  -72,  -73,  -74],\n         [ -75,  -76,  -77,  -78,  -79]],\n\n        [[ -80,  -81,  -82,  -83,  -84],\n         [ -85,  -86,  -87,  -88,  -89],\n         [ -90,  -91,  -92,  -93,  -94],\n         [ -95,  -96,  -97,  -98,  -99]],\n\n        [[-100, -101, -102, -103, -104],\n         [-105, -106, -107, -108, -109],\n         [-110, -111, -112, -113, -114],\n         [-115, -116, -117, -118, -119]]]], dtype=int32)&gt;\n\n\n- (2,3,4,5) concat (2,3,4,5) =&gt; (2,3,8,5) - 세번째 // axis=2\n\na=tf.reshape(tf.constant(range(120)),(2,3,4,5))\nb=-a\n\n\ntf.concat([a,b],axis=2)\n\n&lt;tf.Tensor: shape=(2, 3, 8, 5), dtype=int32, numpy=\narray([[[[   0,    1,    2,    3,    4],\n         [   5,    6,    7,    8,    9],\n         [  10,   11,   12,   13,   14],\n         [  15,   16,   17,   18,   19],\n         [   0,   -1,   -2,   -3,   -4],\n         [  -5,   -6,   -7,   -8,   -9],\n         [ -10,  -11,  -12,  -13,  -14],\n         [ -15,  -16,  -17,  -18,  -19]],\n\n        [[  20,   21,   22,   23,   24],\n         [  25,   26,   27,   28,   29],\n         [  30,   31,   32,   33,   34],\n         [  35,   36,   37,   38,   39],\n         [ -20,  -21,  -22,  -23,  -24],\n         [ -25,  -26,  -27,  -28,  -29],\n         [ -30,  -31,  -32,  -33,  -34],\n         [ -35,  -36,  -37,  -38,  -39]],\n\n        [[  40,   41,   42,   43,   44],\n         [  45,   46,   47,   48,   49],\n         [  50,   51,   52,   53,   54],\n         [  55,   56,   57,   58,   59],\n         [ -40,  -41,  -42,  -43,  -44],\n         [ -45,  -46,  -47,  -48,  -49],\n         [ -50,  -51,  -52,  -53,  -54],\n         [ -55,  -56,  -57,  -58,  -59]]],\n\n\n       [[[  60,   61,   62,   63,   64],\n         [  65,   66,   67,   68,   69],\n         [  70,   71,   72,   73,   74],\n         [  75,   76,   77,   78,   79],\n         [ -60,  -61,  -62,  -63,  -64],\n         [ -65,  -66,  -67,  -68,  -69],\n         [ -70,  -71,  -72,  -73,  -74],\n         [ -75,  -76,  -77,  -78,  -79]],\n\n        [[  80,   81,   82,   83,   84],\n         [  85,   86,   87,   88,   89],\n         [  90,   91,   92,   93,   94],\n         [  95,   96,   97,   98,   99],\n         [ -80,  -81,  -82,  -83,  -84],\n         [ -85,  -86,  -87,  -88,  -89],\n         [ -90,  -91,  -92,  -93,  -94],\n         [ -95,  -96,  -97,  -98,  -99]],\n\n        [[ 100,  101,  102,  103,  104],\n         [ 105,  106,  107,  108,  109],\n         [ 110,  111,  112,  113,  114],\n         [ 115,  116,  117,  118,  119],\n         [-100, -101, -102, -103, -104],\n         [-105, -106, -107, -108, -109],\n         [-110, -111, -112, -113, -114],\n         [-115, -116, -117, -118, -119]]]], dtype=int32)&gt;\n\n\n- (2,3,4,5) concat (2,3,4,5) =&gt; (2,3,4,10) - 네번째 // axis=3 # 0,1,2,3 // -4 -3 -2 -1\n\na=tf.reshape(tf.constant(range(120)),(2,3,4,5))\nb=-a\n\n\ntf.concat([a,b],axis=-1)\n\n&lt;tf.Tensor: shape=(2, 3, 4, 10), dtype=int32, numpy=\narray([[[[   0,    1,    2,    3,    4,    0,   -1,   -2,   -3,   -4],\n         [   5,    6,    7,    8,    9,   -5,   -6,   -7,   -8,   -9],\n         [  10,   11,   12,   13,   14,  -10,  -11,  -12,  -13,  -14],\n         [  15,   16,   17,   18,   19,  -15,  -16,  -17,  -18,  -19]],\n\n        [[  20,   21,   22,   23,   24,  -20,  -21,  -22,  -23,  -24],\n         [  25,   26,   27,   28,   29,  -25,  -26,  -27,  -28,  -29],\n         [  30,   31,   32,   33,   34,  -30,  -31,  -32,  -33,  -34],\n         [  35,   36,   37,   38,   39,  -35,  -36,  -37,  -38,  -39]],\n\n        [[  40,   41,   42,   43,   44,  -40,  -41,  -42,  -43,  -44],\n         [  45,   46,   47,   48,   49,  -45,  -46,  -47,  -48,  -49],\n         [  50,   51,   52,   53,   54,  -50,  -51,  -52,  -53,  -54],\n         [  55,   56,   57,   58,   59,  -55,  -56,  -57,  -58,  -59]]],\n\n\n       [[[  60,   61,   62,   63,   64,  -60,  -61,  -62,  -63,  -64],\n         [  65,   66,   67,   68,   69,  -65,  -66,  -67,  -68,  -69],\n         [  70,   71,   72,   73,   74,  -70,  -71,  -72,  -73,  -74],\n         [  75,   76,   77,   78,   79,  -75,  -76,  -77,  -78,  -79]],\n\n        [[  80,   81,   82,   83,   84,  -80,  -81,  -82,  -83,  -84],\n         [  85,   86,   87,   88,   89,  -85,  -86,  -87,  -88,  -89],\n         [  90,   91,   92,   93,   94,  -90,  -91,  -92,  -93,  -94],\n         [  95,   96,   97,   98,   99,  -95,  -96,  -97,  -98,  -99]],\n\n        [[ 100,  101,  102,  103,  104, -100, -101, -102, -103, -104],\n         [ 105,  106,  107,  108,  109, -105, -106, -107, -108, -109],\n         [ 110,  111,  112,  113,  114, -110, -111, -112, -113, -114],\n         [ 115,  116,  117,  118,  119, -115, -116, -117, -118, -119]]]],\n      dtype=int32)&gt;\n\n\n- (4,) concat (4,) =&gt; (8,) - 첫번째축? // axis=0\n\na=tf.constant([1,2,3,4])\nb=-a\na,b\n\n(&lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)&gt;,\n &lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([-1, -2, -3, -4], dtype=int32)&gt;)\n\n\n\ntf.concat([a,b],axis=0)\n\n&lt;tf.Tensor: shape=(8,), dtype=int32, numpy=array([ 1,  2,  3,  4, -1, -2, -3, -4], dtype=int32)&gt;\n\n\n- (4,) concat (4,) =&gt; (4,2) - 두번째축? // axis=1 ==&gt; 이런거없다..\n\na=tf.constant([1,2,3,4])\nb=-a\na,b\n\n(&lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)&gt;,\n &lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([-1, -2, -3, -4], dtype=int32)&gt;)\n\n\n\ntf.concat([a,b],axis=1)\n\nInvalidArgumentError: {{function_node __wrapped__ConcatV2_N_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} ConcatOp : Expected concatenating dimensions in the range [-1, 1), but got 1 [Op:ConcatV2] name: concat\n\n\n\n\ntf.stack (concat이랑 달라,, 헷갈려!)\n\na=tf.constant([1,2,3,4])\nb=-a\na,b\n\n(&lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)&gt;,\n &lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([-1, -2, -3, -4], dtype=int32)&gt;)\n\n\n\ntf.stack([a,b],axis=0)\n\n&lt;tf.Tensor: shape=(2, 4), dtype=int32, numpy=\narray([[ 1,  2,  3,  4],\n       [-1, -2, -3, -4]], dtype=int32)&gt;\n\n\n\ntf.stack([a,b],axis=1)\n\n&lt;tf.Tensor: shape=(4, 2), dtype=int32, numpy=\narray([[ 1, -1],\n       [ 2, -2],\n       [ 3, -3],\n       [ 4, -4]], dtype=int32)&gt;\n\n\n\n\n\ntnp\n- tf는 넘파이에 비하여 텐서만들기가 너무힘듬\n\nnp.diag([1,2,3])\n\narray([[1, 0, 0],\n       [0, 2, 0],\n       [0, 0, 3]])\n\n\n\nnp.diag([1,2,3]).reshape(-1)\n\narray([1, 0, 0, 0, 2, 0, 0, 0, 3])\n\n\n\n넘파이는 이런식으로 np.diag()도 쓸수 있고 reshape을 메소드로 쓸 수도 있는데…\n\n\ntnp 사용방법 (불만해결방법)\n\nimport tensorflow.experimental.numpy as tnp\ntnp.experimental_enable_numpy_behavior()\n\n\ntype(tnp.array([1,2,3]))\n\ntensorflow.python.framework.ops.EagerTensor\n\n\n- int와 float을 더할 수 있음\n\ntnp.array([1,2,3])+tnp.array([1.0,2.0,3.0])\n\n&lt;tf.Tensor: shape=(3,), dtype=float64, numpy=array([2., 4., 6.])&gt;\n\n\n\ntf.constant([1,2,3])+tf.constant([1.0,2.0,3.0])\n\n&lt;tf.Tensor: shape=(3,), dtype=float64, numpy=array([2., 4., 6.])&gt;\n\n\n\ntnp.array(1)+tnp.array([1.0,2.0,3.0])\n\n&lt;tf.Tensor: shape=(3,), dtype=float64, numpy=array([2., 3., 4.])&gt;\n\n\n\ntnp.diag([1,2,3])\n\n&lt;tf.Tensor: shape=(3, 3), dtype=int64, numpy=\narray([[1, 0, 0],\n       [0, 2, 0],\n       [0, 0, 3]])&gt;\n\n\n\na=tnp.diag([1,2,3])\ntype(a)\n\ntensorflow.python.framework.ops.EagerTensor\n\n\n\na=tf.constant([1,2,3])\na.reshape(3,1)\n\n&lt;tf.Tensor: shape=(3, 1), dtype=int32, numpy=\narray([[1],\n       [2],\n       [3]], dtype=int32)&gt;\n\n\n\na.min()\n\n&lt;tf.Tensor: shape=(), dtype=int32, numpy=1&gt;\n\n\n\na.max()\n\n&lt;tf.Tensor: shape=(), dtype=int32, numpy=3&gt;\n\n\n\n\n선언고급\n\nnp.random.randn(5)\n\narray([-0.47166768, -0.3030497 ,  0.73806592,  1.31800301, -1.19399028])\n\n\n\ntnp.random.randn(5) # 넘파이가 되면 나도 된다.\n\n&lt;tf.Tensor: shape=(5,), dtype=float64, numpy=array([-2.59664514, -0.70758736,  0.37895368,  0.23741269, -1.46921997])&gt;\n\n\n\n\n타입\n\ntype(tnp.random.randn(5))\n\ntensorflow.python.framework.ops.EagerTensor\n\n\n\n\ntf.contant로 만들어도 마치 넘파이인듯 쓰는 기능들\n- 묵시적형변환이 가능\n\ntf.constant([1,1])+tf.constant([2.2,3.3])\n\n&lt;tf.Tensor: shape=(2,), dtype=float64, numpy=array([3.20000005, 4.29999995])&gt;\n\n\n- 메소드를 쓸수 있음.\n\na= tnp.array([[1,2,3,4]])\na.T\n\n&lt;tf.Tensor: shape=(4, 1), dtype=int64, numpy=\narray([[1],\n       [2],\n       [3],\n       [4]])&gt;\n\n\n\n\n그렇지만 np.array는 아님\n- 원소를 할당하는것은 불가능\n\na=tf.constant([1,2,3])\na\n\n&lt;tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 2, 3], dtype=int32)&gt;\n\n\n\na[0]=11\n\nTypeError: 'tensorflow.python.framework.ops.EagerTensor' object does not support item assignment"
  },
  {
    "objectID": "posts/2022_03_28_(4주차)_3월28일.html",
    "href": "posts/2022_03_28_(4주차)_3월28일.html",
    "title": "[STBDA] 04wk: tensorflow_경사하강법",
    "section": "",
    "text": "해당 강의노트는 전북대학교 최규빈교수님 STBDA2022 자료임"
  },
  {
    "objectID": "posts/2022_03_28_(4주차)_3월28일.html#경사하강법",
    "href": "posts/2022_03_28_(4주차)_3월28일.html#경사하강법",
    "title": "[STBDA] 04wk: tensorflow_경사하강법",
    "section": "경사하강법",
    "text": "경사하강법\n\n최적화문제\n- \\(loss=(\\frac{1}{2}\\beta-1)^2\\)를 최소하는 \\(\\beta\\)를 컴퓨터를 활용하여 구하는 문제를 생각해보자. - 답은 \\(\\beta=2\\)임을 알고 있다.\n\n\n방법1: grid search\n\n알고리즘\n\nbeta = [-10.00,-9.99,…,10.00] 와 같은 리스트를 만든다.\n(1)의 리스트의 각원소에 해당하는 loss를 구한다.\n(2)에서 구한 loss를 제일 작게 만드는 beta를 찾는다.\n\n\n\n구현코드\n\nbeta = np.linspace(-10,10,100)\nloss = (beta/2 -1)**2\n\n\ntnp.argmin([1,2,-3,3,4])\n\n&lt;tf.Tensor: shape=(), dtype=int64, numpy=2&gt;\n\n\n\n제일 작은 원소의 숫자?(번째)를 리턴해줌. -3이 제일 작으니까 0,1,“2”\n\n\ntnp.argmin([1,2,3,-3,4])\n\n&lt;tf.Tensor: shape=(), dtype=int64, numpy=3&gt;\n\n\n\ntnp.argmin(loss)\n\n&lt;tf.Tensor: shape=(), dtype=int64, numpy=59&gt;\n\n\n\nbeta[59]\n\n1.9191919191919187\n\n\n\nbeta[60]\n\n2.121212121212121\n\n\n\n(beta[59]/2-1)**2\n\n0.0016324864809713505\n\n\n\n(beta[60]/2-1)**2\n\n0.0036730945821854847\n\n\n\n\n그리드서치의 문제점\n- 비판1: [-10,10]이외에 해가 존재하면? - 이 예제의 경우는 운좋게 [-10,10]에서 해가 존재했음 - 하지만 임의의 고정된 \\(x,y\\)에 대하여 \\(loss(\\beta)=(x\\beta-y)^2\\) 의 형태의 해가 항상 [-10,10]에서 존재한다는 보장은 없음 - 해결책: 더 넓게 많은 범위를 탐색하자?\n- 비판2: 효율적이지 않음 - 알고리즘을 요약하면 결국 -10부터 10까지 작은 간격으로 조금씩 이동하며 loss를 조사하는 것이 grid search의 아이디어 - \\(\\to\\) 생각해보니까 \\(\\beta=2\\)인 순간 \\(loss=(\\frac{1}{2}\\beta-1)^2=0\\)이 되어서 이것보다 작은 최소값은 존재하지 않는다(제곱은 항상 양수이어야 하므로) - \\(\\to\\) 따라서 \\(\\beta=2\\) 이후로는 탐색할 필요가 없다\n\n\n\n방법2: gradient descent\n\n알고리즘!\n\nbeta = -5 로 셋팅한다.\n\n\n(-5/2-1)**2\n\n12.25\n\n\n\nbeta=-5 근처에서 조금씩 이동하여 loss를 조사해본다.\n\n\n(-4.99/2-1)**2 ## 오른쪽으로 0.01 이동하고 loss조사\n\n12.215025\n\n\n\n(-5.01/2-1)**2 ## 왼쪽으로 0.01 이동하고 loss조사\n\n12.285025\n\n\n\n(2)의 결과를 잘 해석하고 더 유리한 쪽으로 이동\n위의 과정을 반복하고 왼쪽, 오른쪽 어느쪽으로 움직여도 이득이 없다면 멈춘다.\n\n\n\n알고리즘 분석\n- (2)-(3)의 과정은 beta=-5 에서 미분계수를 구하고 미분계수가 양수이면 왼쪽으로 움직이고 음수이면 오른쪽으로 움직인다고 해석가능. 아래그림을 보면 더 잘 이해가 된다.\n\nplt.plot(beta,loss)\n\n\n\n\n\n\n왼쪽/오른쪽중에 어디로 갈지 어떻게 판단하는 과정을 수식화?\n- 아래와 같이 해석가능\n\n오른쪽으로 0.01 간다 = beta_old에 0.01을 더함. (if, 미분계수가 음수)\n왼쪽으로 0.01 간다. = beta_old에 0.01을 뺀다. (if, 미분계수가 양수)\n\n- 그렇다면 $_{new} =\n\\[\\begin{cases}\n\\beta_{old} + 0.01, & loss'(\\beta_{old})&lt; 0  \\\\\n\\beta_{old} - 0.01, & loss'(\\beta_{old})&gt; 0\n\\end{cases}\\]\n$\n\n\n혹시 알고리즘을 좀 개선할수 있을까?\n- 항상 0.01씩 움직여야 하는가?\n\nplt.plot(beta,loss)\n\n\n\n\n- \\(\\beta=-10\\) 일 경우의 접선의 기울기? \\(\\beta=-4\\) 일때 접선의 기울기?\n\n\\(\\beta=-10\\) =&gt; 기울기는 -6\n\\(\\beta=-4\\) =&gt; 기울기는 -3\n\n- 실제로 6,3씩 이동할순 없으니 적당한 \\(\\alpha\\) (예를들면 \\(\\alpha=0.01\\)) 를 잡아서 곱한만큼 이동하자.\n- 수식화하면\n\n\\(\\beta_{new} = \\beta_{old} - \\alpha~ loss'(\\beta_{old})\\)\n\\(\\beta_{new} = \\beta_{old} - \\alpha~ \\left[\\frac{\\partial}{\\partial \\beta }loss(\\beta)\\right]_{\\beta=\\beta_{old}}\\)\n\n- \\(\\alpha\\)의 의미 - \\(\\alpha\\)가 크면 크게크게 움직이고 작으면 작게작게 움직인다. - \\(\\alpha&gt;0\\) 이어야 한다.\n\n\n구현코드\n- iter 1\n\\(\\beta=-10\\)이라고 하자.\n\nbeta = tf.Variable(-10.0)\n\n\nwith tf.GradientTape(persistent=True) as tape:\n    loss = (beta/2-1)**2\n\n\ntape.gradient(loss,beta)\n\n&lt;tf.Tensor: shape=(), dtype=float32, numpy=-6.0&gt;\n\n\n\\(\\beta = -10\\) 에서 0.01만큼 움직이고 싶음\n\nalpha= 0.01/6\n\n\nalpha * tape.gradient(loss,beta)\n\n&lt;tf.Tensor: shape=(), dtype=float32, numpy=-0.01&gt;\n\n\n\nbeta.assign_sub(alpha * tape.gradient(loss,beta))\n\n&lt;tf.Variable 'UnreadVariable' shape=() dtype=float32, numpy=-9.99&gt;\n\n\n\nbeta\n\n&lt;tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-9.99&gt;\n\n\n- iter2\n\nwith tf.GradientTape(persistent=True) as tape:\n    loss = (beta/2-1)**2\n\n\nbeta.assign_sub(tape.gradient(loss,beta)*alpha)\n\n&lt;tf.Variable 'UnreadVariable' shape=() dtype=float32, numpy=-9.980008&gt;\n\n\n- for 문을 이용하자.\n(강의용)\n\nbeta = tf.Variable(-10.0)\n\n\nfor k in range(10000):\n    with tf.GradientTape(persistent=True) as tape:\n        loss = (beta/2-1)**2\n    beta.assign_sub(tape.gradient(loss,beta)*alpha)\n\n\nbeta\n\n&lt;tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.9971251&gt;\n\n\n(시도1)\n\nbeta = tf.Variable(-10.0)\n\n\nfor k in range(100):\n    with tf.GradientTape(persistent=True) as tape:\n        loss = (beta/2-1)**2\n    beta.assign_sub(tape.gradient(loss,beta)*alpha)\n\n\nbeta\n\n&lt;tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-9.040152&gt;\n\n\n(시도2)\n\nbeta = tf.Variable(-10.0)\n\n\nfor k in range(1000):\n    with tf.GradientTape(persistent=True) as tape:\n        loss = (beta/2-1)**2\n    beta.assign_sub(tape.gradient(loss,beta)*alpha)\n\n\nbeta\n\n&lt;tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-3.2133684&gt;\n\n\n- 너무 느린 것 같다? \\(\\to\\) \\(\\alpha\\)를 키워보자!\n\n\n학습률\n- 목표: \\(\\alpha\\)에 따라서 수렴과정이 어떻게 달라지는 시각화해보자.\n\n[시각화 코드 예비학습]\n\nfig = plt.figure() # 도화지가 만들어지고 fig라는 이름을 붙인다.\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\nax = fig.add_subplot() # fig는 ax라는 물체를 만든다.\n\n\nid(fig.axes[0])\n\n139963527032544\n\n\n\nid(ax)\n\n139963527032544\n\n\n\npnts, = ax.plot([1,2,3],[4,5,6],'or')\npnts\n\n&lt;matplotlib.lines.Line2D at 0x7f4bcc48baf0&gt;\n\n\n\npnts뒤에 ‘콤마’ 붙임-&gt; 튜플\n\n\npnts.get_xdata()\n\narray([1, 2, 3])\n\n\n\npnts.get_ydata()\n\narray([4, 5, 6])\n\n\n\nfig\n\n\n\n\n\npnts.set_ydata([5,5,5])\n\n\npnts.get_ydata()\n\n[5, 5, 5]\n\n\n\nfig\n\n\n\n\n- 응용\n\nplt.rcParams[\"animation.html\"]=\"jshtml\"\nfrom matplotlib import animation\n\n\ndef animate(i):\n    if i%2 == 0:\n        pnts.set_ydata([4,5,6])\n    else:\n        pnts.set_ydata([5,5,5])\n\n\nani = animation.FuncAnimation(fig,animate,frames=10)\nani\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n예비학습 끝\n- beta_lst=[-10,-9,-8] 로 이동한다고 하자.\n\nbeta_lst = [-10,-9,-8]\nloss_lst = [(-10/2-1)**2,(-9/2-1)**2,(-8/2-1)**2]\n\n\nfig = plt.figure()\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\nax= fig.add_subplot()\n\n\n_beta = np.linspace(-15,19,100)\n\n\nax.plot(_beta,(_beta/2-1)**2)\n\n\nfig\n\n\n\n\n\npnts, = ax.plot(beta_lst[0],loss_lst[0],'ro')\nfig\n\n\n\n\n\ndef animate(i):\n    pnts.set_xdata(beta_lst[:(i+1)])\n    pnts.set_ydata(loss_lst[:(i+1)])\n\n\nani =animation.FuncAnimation(fig, animate, frames=3)\nani\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n- 최종아웃풋\n\nbeta = tf.Variable(-10.0)\nalpha = 0.01/6\n\n\nbeta_lst=[]\nloss_lst=[]\n\n\nbeta_lst.append(beta.numpy())\nloss_lst.append((beta.numpy()/2-1)**2)\n\n\nwith tf.GradientTape(persistent=True) as tape:\n    tape.watch(beta)\n    loss = (beta/2-1)**2\n\n\nbeta.assign_sub(tape.gradient(loss,beta)*alpha)\n\n&lt;tf.Variable 'UnreadVariable' shape=() dtype=float32, numpy=-9.99&gt;\n\n\n\nbeta_lst.append(beta.numpy())\nloss_lst.append((beta.numpy()/2-1)**2)\n\n\nbeta_lst, loss_lst\n\n([-10.0, -9.99], [36.0, 35.94002362785341])\n\n\n- for\n\nbeta = tf.Variable(-10.0)\nalpha = 0.01/6\nbeta_lst=[]\nloss_lst=[]\nbeta_lst.append(beta.numpy())\nloss_lst.append((beta.numpy()/2-1)**2)\nfor k in range(100):\n    with tf.GradientTape(persistent=True) as tape:\n        tape.watch(beta)\n        loss = (beta/2-1)**2\n    beta.assign_sub(tape.gradient(loss,beta)*alpha)\n    beta_lst.append(beta.numpy())\n    loss_lst.append((beta.numpy()/2-1)**2)\n\n\nfig = plt.figure()\nax = fig.add_subplot()\nax.plot(_beta,(_beta/2-1)**2)\npnts, = ax.plot(beta_lst[0],loss_lst[0],'or')\n\n\n\n\n\nani = animation.FuncAnimation(fig,animate,frames=100)\nani\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\n\n\n숙제\n\\(y=(x-1)^2\\)를 최소화 하는 \\(x\\)를 경사하강법을 이용하여 찾아라. 수렴과정을 animation으로 시각화하라. - x의 초기값은 -3으로 설정한다. - 적당한 \\(\\alpha\\)를 골라서 100번의 반복안에 수렴하도록 하라."
  },
  {
    "objectID": "posts/2022_05_09_(10주차)_5월9일.html",
    "href": "posts/2022_05_09_(10주차)_5월9일.html",
    "title": "[STBDA] 10wk: softmax function, 평가지표, flatten layer",
    "section": "",
    "text": "해당 강의노트는 전북대학교 최규빈교수님 STBDA2022 자료임"
  },
  {
    "objectID": "posts/2022_05_09_(10주차)_5월9일.html#imports",
    "href": "posts/2022_05_09_(10주차)_5월9일.html#imports",
    "title": "[STBDA] 10wk: softmax function, 평가지표, flatten layer",
    "section": "imports",
    "text": "imports\n\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow.experimental.numpy as tnp\n\n\ntnp.experimental_enable_numpy_behavior()\n\n\ntf.config.experimental.list_physical_devices()\n\n[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n\n\n\nimport graphviz\ndef gv(s): return graphviz.Source('digraph G{ rankdir=\"LR\"'+ s + ';}')"
  },
  {
    "objectID": "posts/2022_05_09_(10주차)_5월9일.html#softmax-function",
    "href": "posts/2022_05_09_(10주차)_5월9일.html#softmax-function",
    "title": "[STBDA] 10wk: softmax function, 평가지표, flatten layer",
    "section": "softmax function",
    "text": "softmax function\n\n로지스틱 모형 (1): 활성화함수로 sigmoid 선택\n- 기본버전은 아래와 같다\n\\[y_i \\approx \\text{sigmoid}(b + w_1 x_{1,i} + \\dots + w_{784}x_{784,i})= \\frac{\\exp(b + w_1 x_{1,i} + \\dots + w_{784}x_{784,i})}{1+\\exp(b + w_1 x_{1,i} + \\dots + w_{784}x_{784,i})}\\]\n- 벡터버전은 아래와 같다.\n\\[{\\boldsymbol y} \\approx \\text{sigmoid}({\\bf X}{\\bf W} + b) = \\frac{\\exp({\\bf XW} +b)}{1+\\exp({\\bf XW} +b)}\\]\n- 벡터버전에 익숙해지도록 하자. 벡터버전에 사용된 차원 및 연산을 정리하면 아래와 같다.\n\n\\({\\bf X}\\): (n,784) matrix\n\\({\\boldsymbol y}\\): (n,1) matrix\n\\({\\bf W}\\): (784,1) matrix\n\\(b\\): (1,1) matrix\n+, exp 는 브로드캐스팅\n\n\n\n로지스틱 모형 (2): 활성화함수로 softmax 선택\n- \\(y_i=0 \\text{ or } 1\\) 대신에 \\(\\boldsymbol{y}_i=[y_{i1},y_{i2}]= [1,0] \\text { or } [0,1]\\)와 같이 코딩하면 어떠할까? (즉 원핫인코딩을 한다면?)\n- 활성화 함수를 취하기 전의 버전은 아래와 같이 볼 수 있다.\n\\[[{\\boldsymbol y}_1 ~ {\\boldsymbol y}_2] \\propto  [ {\\bf X}{\\bf W}_1  ~ {\\bf X}{\\bf W}_2] + [b_1 ~ b_2]= {\\bf X} [{\\bf W}_1 {\\bf W}_2] + [b_1 ~ b_2]= {\\bf X}{\\bf W} + {\\boldsymbol b}\\]\n여기에서 매트릭스 및 연산의 차원을 정리하면 아래와 같다.\n\n\\({\\bf X}\\): (n,784) matrix\n\\({\\boldsymbol y}_1,{\\boldsymbol y}_2\\): (n,1) matrix\n\\({\\boldsymbol y}:=[{\\boldsymbol y}_1~ {\\boldsymbol y}_2]\\): (n,2) matrix\n\\({\\bf W}_1\\), \\({\\bf W}_2\\): (784,1) matrix\n\\({\\bf W}:=[{\\bf W}_1~ {\\bf W}_2]\\): (784,2) matrix\n\\(b_1,b_2\\): (1,1) matrix\n$:= [b_1 ~b_2] $: (1,2) matrix\n+ 는 브로드캐스팅\n\n- 즉 로지스틱 모형 (1)의 형태를 겹쳐놓은 형태로 해석할 수 있음. 따라서 \\({\\bf X} {\\bf W}_1 + b_1\\)와 \\({\\bf X} {\\bf W}_2 + b_2\\)의 row값이 클수록 \\({\\boldsymbol y}_1\\)와 \\({\\boldsymbol y}_2\\)의 row값이 1이어야 함\n\n\\({\\boldsymbol y}_1 \\propto {\\bf X} {\\bf W}_1 + b_1\\) \\(\\to\\) \\({\\bf X} {\\bf W}_1 + b_1\\)의 row값이 클수록 \\(\\boldsymbol{y}_1\\)의 row 값이 1이라면 모형계수를 잘 추정한것\n\\({\\boldsymbol y}_2 \\propto {\\bf X} {\\bf W}_2 + b_2\\) \\(\\to\\) \\({\\bf X} {\\bf W}_2 + b_2\\)의 row값이 클수록 \\(\\boldsymbol{y}_2\\)의 row 값을 1이라면 모형계수를 잘 추정한것\n\n- (문제) \\({\\bf X}{\\bf W}_1 +b_1\\)의 값이 500, \\({\\bf X}{\\bf W}_2 +b_2\\)의 값이 200 인 row가 있다고 하자. 대응하는 \\(\\boldsymbol{y}_1, \\boldsymbol{y}_2\\)의 row값은 얼마로 적합되어야 하는가?\n\n\\([0,0]\\)\n\\([0,1]\\)\n\\([1,0]\\) &lt;– 이게 답이다!\n\\([1,1]\\)\n\n\nnote: 둘다 0 혹은 둘다 1로 적합할수는 없으니까 (1), (4)는 제외한다. \\({\\bf X}{\\bf W}_1 +b_1\\)의 값이 \\({\\bf X}{\\bf W}_2 +b_2\\)의 값보다 크므로 (3)번이 합리적임\n\n- 목표: 위와 같은 문제의 답을 유도해주는 활성화함수를 설계하자. 즉 합리적인 \\(\\hat{\\boldsymbol{y}}_1,\\hat{\\boldsymbol{y}}_2\\)를 구해주는 활성화 함수를 설계해보자. 이를 위해서는 아래의 사항들이 충족되어야 한다.\n\n\\(\\hat{\\boldsymbol{y}}_1\\), \\(\\hat{\\boldsymbol{y}}_2\\)의 각 원소는 0보다 크고 1보다 작아야 한다. (확률을 의미해야 하니까)\n\\(\\hat{\\boldsymbol{y}}_1+\\hat{\\boldsymbol{y}}_2={\\bf 1}\\) 이어야 한다. (확률의 총합은 1이니까!)\n\\(\\hat{\\boldsymbol{y}}_1\\)와 \\(\\hat{\\boldsymbol{y}}_2\\)를 각각 따로해석하면 로지스틱처럼 되면 좋겠다.\n\n- 아래와 같은 활성화 함수를 도입하면 어떨까?\n\\[\\hat{\\boldsymbol{y}}=[\\hat{\\boldsymbol y}_1 ~ \\hat{\\boldsymbol y}_2] =  \\big[ \\frac{\\exp({\\bf X}\\hat{\\bf W}_1+\\hat{b}_1)}{\\exp({\\bf X}\\hat{\\bf W}_1+\\hat{b}_1)+\\exp({\\bf X}\\hat{\\bf W}_2+\\hat{b}_2)}  ~~ \\frac{\\exp({\\bf X}\\hat{\\bf W}_2+\\hat{b}_2)}{\\exp({\\bf X}\\hat{\\bf W}_1+\\hat{b}_1)+\\exp({\\bf X}\\hat{\\bf W}_2+\\hat{b}_2)}  \\big]\\]\n- (1),(2)는 만족하는 듯 하다. (3)은 바로 이해되지는 않는다\n\n\\(\\hat{\\boldsymbol{y}}_1\\), \\(\\hat{\\boldsymbol{y}}_2\\)의 각 원소는 0보다 크고 1보다 작아야 한다. –&gt; OK!\n\\(\\hat{\\boldsymbol{y}}_1+\\hat{\\boldsymbol{y}}_2={\\bf 1}\\) 이어야 한다. –&gt; OK!\n\\(\\hat{\\boldsymbol{y}}_1\\)와 \\(\\hat{\\boldsymbol{y}}_2\\)를 각각 따로해석하면 로지스틱처럼 되면 좋겠다. –&gt; ???\n\n- 그런데 조금 따져보면 (3)도 만족된다는 것을 알 수 있다. (sigmoid, softmax Section 참고)\n- 위와 같은 함수를 softmax라고 하자. 즉 아래와 같이 정의하자.\n\\[\n\\hat{\\boldsymbol y} = \\text{softmax}({\\bf X}\\hat{\\bf W} + {\\boldsymbol b})\n= \\big[ \\frac{\\exp({\\bf X}\\hat{\\bf W}_1+\\hat{b}_1)}{\\exp({\\bf X}\\hat{\\bf W}_1+\\hat{b}_1)+\\exp({\\bf X}\\hat{\\bf W}_2+\\hat{b}_2)}  ~~ \\frac{\\exp({\\bf X}\\hat{\\bf W}_2+\\hat{b}_2)}{\\exp({\\bf X}\\hat{\\bf W}_1+\\hat{b}_1)+\\exp({\\bf X}\\hat{\\bf W}_2+\\hat{b}_2)}  \\big]\n\\]\n\n\nsigmoid, softmax\n\nsoftmax는 sigmoid의 확장형\n- 아래의 수식을 관찰하자. \\[\\frac{\\exp(\\beta_0+\\beta_1 x_i)}{1+\\exp(\\beta_0+\\beta_1x_i)}=\\frac{\\exp(\\beta_0+\\beta_1 x_i)}{e^0+\\exp(\\beta_0+\\beta_1x_i)}\\]\n- 1을 \\(e^0\\)로 해석하면 모형2의 해석을 아래와 같이 모형1의 해석으로 적용할수 있다. - 모형2: \\({\\bf X}\\hat{\\bf W}_1 +\\hat{b}_1\\) 와 \\({\\bf X}\\hat{\\bf W}_2 +\\hat{b}_2\\) 의 크기를 비교하고 확률 결정 - 모형1: \\({\\bf X}\\hat{\\bf W} +\\hat{b}\\) 와 \\(0\\)의 크기를 비교하고 확률 결정 = \\({\\bf X}\\hat{\\bf W} +\\hat{b}\\)의 row값이 양수이면 1로 예측하고 음수이면 0으로 예측\n- 이항분포를 차원이 2인 다항분포로 해석가능한 것처럼 sigmoid는 차원이 2인 softmax로 해석가능하다. 즉 다항분포가 이항분포의 확장형으로 해석가능한 것처럼 softmax도 sigmoid의 확장형으로 해석가능하다.\n\n\n클래스의 수가 2인 경우 softmax vs sigmoid\n- 언뜻 생각하면 클래스가 2인 경우에도 sigmoid 대신 softmax로 활성화함수를 이용해도 될 듯 하다. 즉 \\(y=0 \\text{ or } 1\\)와 같이 정리하지 않고 \\(y=[0,1] \\text{ or } [1,0]\\) 와 같이 정리해도 무방할 듯 하다.\n- 하지만 sigmoid가 좀 더 좋은 선택이다. 즉 \\(y= 0 \\text{ or } 1\\)로 데이터를 정리하는 것이 더 좋은 선택이다. 왜냐하면 sigmoid는 softmax와 비교하여 파라메터의 수가 적지만 표현력은 동등하기 때문이다.\n- 표현력이 동등한 이유? 아래 수식을 관찰하자.\n\\[\\big(\\frac{e^{300}}{e^{300}+e^{500}},\\frac{e^{500}}{e^{300}+e^{500}}\\big) =\\big( \\frac{e^{0}}{e^{0}+e^{200}}, \\frac{e^{200}}{e^{0}+e^{200}}\\big)\\]\n\n\\(\\big(\\frac{e^{300}}{e^{300}+e^{500}},\\frac{e^{500}}{e^{300}+e^{500}}\\big)\\)를 표현하기 위해서 300, 500 이라는 2개의 숫자가 필요한것이 아니고 따지고보면 200이라는 하나의 숫자만 필요하다.\n\\((\\hat{\\boldsymbol{y}}_1,\\hat{\\boldsymbol{y}}_2)\\)의 표현에서도 \\({\\bf X}\\hat{\\bf W}_1 +\\hat{b}_1\\) 와 \\({\\bf X}\\hat{\\bf W}_2 +\\hat{b}_2\\) 라는 숫자 각각이 필요한 것이 아니고 \\(({\\bf X}\\hat{\\bf W}_1 +\\hat{b}_1)-({\\bf X}\\hat{\\bf W}_2 +\\hat{b}_2)\\)의 값만 알면 된다.\n\n- 클래스의 수가 2개일 경우는 softmax가 sigmoid에 비하여 장점이 없다. 하지만 softmax는 클래스의 수가 3개 이상일 경우로 쉽게 확장할 수 있다는 점에서 매력적인 활성화 함수이다.\n\n\n\n분류할 클래스가 3개 이상일 경우 신경망 모형의 설계\n- y의 모양: [0 1 0 0 0 0 0 0 0 0]\n- 활성화함수의 선택: softmax\n- 손실함수의 선택: cross entropy\n\n\nFashion_MNIST 여러클래스의 분류 (softmax의 실습)\n- 데이터정리\n\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n\n\nX= x_train.reshape(-1,784)\ny= tf.keras.utils.to_categorical(y_train)\nXX = x_test.reshape(-1,784)\nyy = tf.keras.utils.to_categorical(y_test)\n\n- 시도1: 간단한 신경망\n\n#collapse\ngv('''\nsplines=line\nsubgraph cluster_1{\n    style=filled;\n    color=lightgrey;\n    \"x1\"\n    \"x2\"\n    \"..\"\n    \"x784\"\n    label = \"Layer 0\"\n}\nsubgraph cluster_2{\n    style=filled;\n    color=lightgrey;\n    \"x1\" -&gt; \"node1\"\n    \"x2\" -&gt; \"node1\"\n    \"..\" -&gt; \"node1\"\n\n    \"x784\" -&gt; \"node1\"\n    \"x1\" -&gt; \"node2\"\n    \"x2\" -&gt; \"node2\"\n    \"..\" -&gt; \"node2\"\n    \"x784\" -&gt; \"node2\"\n\n    \"x1\" -&gt; \"...\"\n    \"x2\" -&gt; \"...\"\n    \"..\" -&gt; \"...\"\n    \"x784\" -&gt; \"...\"\n\n    \"x1\" -&gt; \"node30\"\n    \"x2\" -&gt; \"node30\"\n    \"..\" -&gt; \"node30\"\n    \"x784\" -&gt; \"node30\"\n\n\n    label = \"Layer 1: relu\"\n}\nsubgraph cluster_3{\n    style=filled;\n    color=lightgrey;\n\n    \"node1\" -&gt; \"y10\"\n    \"node2\" -&gt; \"y10\"\n    \"...\" -&gt; \"y10\"\n    \"node30\" -&gt; \"y10\"\n\n    \"node1\" -&gt; \"y1\"\n    \"node2\" -&gt; \"y1\"\n    \"...\" -&gt; \"y1\"\n    \"node30\" -&gt; \"y1\"\n\n    \"node1\" -&gt; \".\"\n    \"node2\" -&gt; \".\"\n    \"...\" -&gt; \".\"\n    \"node30\" -&gt; \".\"\n\n    label = \"Layer 2: softmax\"\n}\n''')\n\n\n\n\n\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(30,activation = 'relu'))\nnet.add(tf.keras.layers.Dense(10,activation = 'softmax'))\nnet.compile(loss=tf.losses.categorical_crossentropy, optimizer='adam',metrics=['accuracy'])\nnet.fit(X,y,epochs=5)\n\nEpoch 1/5\n1875/1875 [==============================] - 1s 515us/step - loss: 1.9693 - accuracy: 0.4597\nEpoch 2/5\n1875/1875 [==============================] - 1s 511us/step - loss: 1.0388 - accuracy: 0.5878\nEpoch 3/5\n1875/1875 [==============================] - 1s 511us/step - loss: 0.8715 - accuracy: 0.6531\nEpoch 4/5\n1875/1875 [==============================] - 1s 515us/step - loss: 0.7917 - accuracy: 0.6804\nEpoch 5/5\n1875/1875 [==============================] - 1s 508us/step - loss: 0.7568 - accuracy: 0.6905\n\n\n&lt;keras.callbacks.History at 0x7f03e0052730&gt;\n\n\n\nnet.evaluate(XX,yy)\n\n313/313 [==============================] - 0s 394us/step - loss: 0.7765 - accuracy: 0.6829\n\n\n[0.776503324508667, 0.6829000115394592]\n\n\n\nnet.summary()\n\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense (Dense)               (None, 30)                23550     \n                                                                 \n dense_1 (Dense)             (None, 10)                310       \n                                                                 \n=================================================================\nTotal params: 23,860\nTrainable params: 23,860\nNon-trainable params: 0\n_________________________________________________________________\n\n\n- 시도2: 더 깊은 신경망\n\n#collapse\ngv('''\nsplines=line\nsubgraph cluster_1{\n    style=filled;\n    color=lightgrey;\n    \"x1\"\n    \"x2\"\n    \"..\"\n    \"x784\"\n    label = \"Layer 0\"\n}\nsubgraph cluster_2{\n    style=filled;\n    color=lightgrey;\n    \"x1\" -&gt; \"node1\"\n    \"x2\" -&gt; \"node1\"\n    \"..\" -&gt; \"node1\"\n\n    \"x784\" -&gt; \"node1\"\n    \"x1\" -&gt; \"node2\"\n    \"x2\" -&gt; \"node2\"\n    \"..\" -&gt; \"node2\"\n    \"x784\" -&gt; \"node2\"\n\n    \"x1\" -&gt; \"...\"\n    \"x2\" -&gt; \"...\"\n    \"..\" -&gt; \"...\"\n    \"x784\" -&gt; \"...\"\n\n    \"x1\" -&gt; \"node500\"\n    \"x2\" -&gt; \"node500\"\n    \"..\" -&gt; \"node500\"\n    \"x784\" -&gt; \"node500\"\n\n\n    label = \"Layer 1: relu\"\n}\n\nsubgraph cluster_3{\n    style=filled;\n    color=lightgrey;\n\n    \"node1\" -&gt; \"node1(2)\"\n    \"node2\" -&gt; \"node1(2)\"\n    \"...\" -&gt; \"node1(2)\"\n    \"node500\" -&gt; \"node1(2)\"\n\n    \"node1\" -&gt; \"node2(2)\"\n    \"node2\" -&gt; \"node2(2)\"\n    \"...\" -&gt; \"node2(2)\"\n    \"node500\" -&gt; \"node2(2)\"\n\n    \"node1\" -&gt; \"....\"\n    \"node2\" -&gt; \"....\"\n    \"...\" -&gt; \"....\"\n    \"node500\" -&gt; \"....\"\n\n    \"node1\" -&gt; \"node500(2)\"\n    \"node2\" -&gt; \"node500(2)\"\n    \"...\" -&gt; \"node500(2)\"\n    \"node500\" -&gt; \"node500(2)\"\n\n\n    label = \"Layer 2: relu\"\n}\n\nsubgraph cluster_4{\n    style=filled;\n    color=lightgrey;\n\n    \"node1(2)\" -&gt; \"y10\"\n    \"node2(2)\" -&gt; \"y10\"\n    \"....\" -&gt; \"y10\"\n    \"node500(2)\" -&gt; \"y10\"\n\n    \"node1(2)\" -&gt; \"y1\"\n    \"node2(2)\" -&gt; \"y1\"\n    \"....\" -&gt; \"y1\"\n    \"node500(2)\" -&gt; \"y1\"\n\n    \"node1(2)\" -&gt; \".\"\n    \"node2(2)\" -&gt; \".\"\n    \"....\" -&gt; \".\"\n    \"node500(2)\" -&gt; \".\"\n\n    label = \"Layer 3: softmax\"\n}\n''')\n\n\n\n\n\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(500,activation = 'relu'))\nnet.add(tf.keras.layers.Dense(500,activation = 'relu'))\nnet.add(tf.keras.layers.Dense(10,activation = 'softmax'))\nnet.compile(loss=tf.losses.categorical_crossentropy, optimizer='adam',metrics=['accuracy'])\nnet.fit(X,y,epochs=5)\n\nEpoch 1/5\n1875/1875 [==============================] - 3s 1ms/step - loss: 2.1970 - accuracy: 0.7556\nEpoch 2/5\n1875/1875 [==============================] - 3s 1ms/step - loss: 0.6298 - accuracy: 0.7915\nEpoch 3/5\n1875/1875 [==============================] - 3s 1ms/step - loss: 0.5368 - accuracy: 0.8200\nEpoch 4/5\n1875/1875 [==============================] - 3s 1ms/step - loss: 0.4760 - accuracy: 0.8366\nEpoch 5/5\n1875/1875 [==============================] - 3s 1ms/step - loss: 0.4351 - accuracy: 0.8476\n\n\n&lt;keras.callbacks.History at 0x7f03707e7610&gt;\n\n\n\nnet.evaluate(XX,yy)\n\n313/313 [==============================] - 0s 567us/step - loss: 0.5347 - accuracy: 0.8281\n\n\n[0.5346611142158508, 0.8281000256538391]\n\n\n\nnet.summary()\n\nModel: \"sequential_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense_2 (Dense)             (None, 500)               392500    \n                                                                 \n dense_3 (Dense)             (None, 500)               250500    \n                                                                 \n dense_4 (Dense)             (None, 10)                5010      \n                                                                 \n=================================================================\nTotal params: 648,010\nTrainable params: 648,010\nNon-trainable params: 0\n_________________________________________________________________"
  },
  {
    "objectID": "posts/2022_05_09_(10주차)_5월9일.html#평가지표",
    "href": "posts/2022_05_09_(10주차)_5월9일.html#평가지표",
    "title": "[STBDA] 10wk: softmax function, 평가지표, flatten layer",
    "section": "평가지표",
    "text": "평가지표\n\n다양한 평가지표들\n- 의문: 왜 다양한 평가지표가 필요한가? (accuray면 끝나는거 아닌가? 더 이상 뭐가 필요해?)\n- 여러가지 평가지표들: https://en.wikipedia.org/wiki/Positive_and_negative_predictive_values - 이걸 다 암기하는건 불가능함. - 몇 개만 뽑아서 암기하고 왜 쓰는지만 생각해보고 넘어가자!\n\n\nconfusion matrix의 이해\n- 표1\n\n\n\n\n퇴사(예측)\n안나감(예측)\n\n\n\n\n퇴사(실제)\nTP\nFN\n\n\n안나감(실제)\nFP\nTN\n\n\n\n- 표2 (책에없음)\n\n\n\n\n퇴사(예측)\n안나감(예측)\n\n\n\n\n퇴사(실제)\n$(y,)= $ (O,O)\n$(y,)= $(O,X)\n\n\n안나감(실제)\n$(y,)= $(X,O)\n$(y,)= $(X,X)\n\n\n\n- 표3 (책에없음)\n\n\n\n\n퇴사(예측)\n안나감(예측)\n\n\n\n\n퇴사(실제)\nTP, \\(\\# O/O\\)\nFN, \\(\\#O/X\\)\n\n\n안나감(실제)\nFP, \\(\\#X/O\\)\nTN, \\(\\#X/X\\)\n\n\n\n\n암기법, (1) 두번째 글자를 그대로 쓴다 (2) 첫글자가 T이면 분류를 제대로한것, 첫글자가 F이면 분류를 잘못한것\n\n- 표4 (위키등에 있음)\n\n\n\n\n\n\n\n\n\n\n퇴사(예측)\n안나감(예측)\n\n\n\n\n\n퇴사(실제)\nTP, \\(\\# O/O\\)\nFN, \\(\\# O/X\\)\nSensitivity(민감도)=Recall(재현율)=\\(\\frac{TP}{TP+FN}\\)=\\(\\frac{\\#O/O}{\\# O/O+ \\#O/X}\\)\n\n\n안나감(실제)\nFP, \\(\\# X/O\\)\nTN, \\(\\# X/X\\)\n\n\n\n\nPrecision(프리시즌)=\\(\\frac{TP}{TP+FP}\\)=\\(\\frac{\\# O/O}{\\# O/O+\\# X/O}\\)\n\nAccuracy(애큐러시)=\\(\\frac{TP+TN}{total}\\)=\\(\\frac{\\#O/O+\\# X/X}{total}\\)\n\n\n\n\n\n상황극\n- 최규빈은 입사하여 “퇴사자 예측시스템”의 개발에 들어갔다.\n- 자료의 특성상 대부분의 사람이 퇴사하지 않고 회사에 잘 다닌다. 즉 1000명이 있으면 10명정도 퇴사한다.\n\n\nAccuracy\n- 정의: Accuracy(애큐러시)=\\(\\frac{TP+TN}{total}\\)=\\(\\frac{\\#O/O+ \\#X/X}{total}\\) - 한국말로는 정확도, 정분류율이라고 한다. - 한국말이 헷갈리므로 그냥 영어를 외우는게 좋다. (어차피 Keras에서 옵션도 영어로 넣음)\n- (상확극 시점1) 왜 애큐러시는 불충분한가? - 회사: 퇴사자예측프로그램 개발해 - 최규빈: 귀찮은데 다 안나간다고 하자! -&gt; 99퍼의 accuracy\n\n모델에 사용한 파라메터 = 0. 그런데 애큐러시 = 99! 이거 엄청 좋은 모형이다?\n\n\n\nSensitivity(민감도), Recall(재현율), True Positive Rate(TPR)\n- 정의: Sensitivity(민감도)=Recall(재현율)=\\(\\frac{TP}{TP+FN}\\)=\\(\\frac{\\# O/O}{\\# O/O+\\# O/X}\\) - 분모: 실제 O인 관측치 수 - 분자: 실제 O를 O라고 예측한 관측치 수 - 뜻: 실제 O를 O라고 예측한 비율\n- (상황극 시점2) recall을 봐야하는 이유 - 인사팀: 실제 퇴사자를 퇴사자로 예측해야 의미가 있음! 우리는 퇴사할것 같은 10명을 찍어달란 의미였어요! (그래야 면담을 하든 할거아냐!) - 최규빈: 가볍고(=파라메터 적고) 잘 맞추는 모형 만들어 달라면서요?\n\n인사팀: (고민중..) 사실 생각해보니까 이 경우는 애큐러시는 의미가 없네. 실제 나간 사람 중 최규빈이 나간다고 한 사람이 몇인지 카운트 하는게 더 의미가 있겠다. 우리는 앞으로 리컬(혹은 민감도)를 보겠다!\n\n\n예시1: 실제로 퇴사한 10명중 최규빈이 나간다고 찍은 사람이 5명이면 리컬이 50%\n\n\n예시2: 최규빈이 아무도 나가지 않는다고 예측해버린다? 실제 10명중에서 최규빈이 나간다고 적중시킨사람은 0명이므로 이 경우 리컬은 0%\n\n\n결론: 우리가 필요한건 recall이니까 앞으로 recall을 가져와! accuracy는 큰 의미없어. (그래도 명색이 모델인데 accuracy가 90은 되면 좋겠다)\n\n\n\nPrecision\n- 정의: Precision(프리시즌)=\\(\\frac{TP}{TP+FP}\\)=\\(\\frac{\\# O/O}{\\# O/O+\\# X/O}\\) - 분모: O라고 예측한 관측치 - 분자: O라고 예측한 관측치중 진짜 O인 관측치 - 뜻: O라고 예측한 관측치중 진짜 O인 비율\n- (상황극 시점3) recall 만으로 불충분한 이유\n\n최규빈: 에휴.. 귀찮은데 그냥 좀만 수틀리면 다 나갈것 같다고 해야겠다. -&gt; 한 100명 나간다고 했음 -&gt; 실제로 최규빈이 찍은 100명중에 10명이 다 나감!\n\n\n이 경우 애큐러시는 91%, 리컬은 100% (퇴사자 10명을 일단은 다 맞췄으므로).\n\n\n인사팀: (화가 많이 남) 멀쩡한 사람까지 다 퇴사할 것 같다고 하면 어떡해요? 최규빈 연구원이 나간다고 한 100명중에 실제로 10명만 나갔어요.\n인사팀: 마치 총으로 과녁중앙에 맞춰 달라고 했더니 기관총을 가져와서 한번 긁은것이랑 뭐가 달라요? 맞추는게 문제가 아니고 precision이 너무 낮아요.\n최규빈: accuracy 90% 이상, recall은 높을수록 좋다는게 주문 아니었나요?\n인사팀: (고민중..) 앞으로는 recall과 함께 precision도 같이 제출하세요. precision은 당신이 나간다고 한 사람중에 실제 나간사람의 비율을 의미해요. 이 경우는 \\(\\frac{10}{100}\\)이니까 precision이 10%입니다. (속마음: recall 올리겠다고 무작정 너무 많이 예측하지 말란 말이야!)\n\n\n\nF1 score\n- 정의: recall과 precision의 조화평균\n- (상황극 시점4) recall, precision을 모두 고려\n\n최규빈: recall/precision을 같이 내는건 좋은데요, 둘은 trade off의 관계에 있습니다. 물론 둘다 올리는 모형이 있다면 좋지만 그게 쉽지는 않아요. 보통은 precision을 올리려면 recall이 희생되는 면이 있고요, recall을 올리려고 하면 precision이 다소 떨어집니다.\n최규빈: 평가기준이 애매하다는 의미입니다. 모형1,2가 있는데 모형1은 모형2보다 precision이 약간 좋고 대신 recall이 떨어진다면 모형1이 좋은것입니까? 아니면 모형2가 좋은것입니까?\n인사팀: 그렇다면 둘을 평균내서 F1score를 계산해서 제출해주세요.\n\n\n\nSpecificity(특이도), False Positive Rate(FPR)\n- 정의:\n\nSpecificity(특이도)=\\(\\frac{TN}{FP+TN}\\)=\\(\\frac{\\# X/X}{\\# X/O+\\# X/X}\\)\nFalse Positive Rate (FPR) = 1-Specificity(특이도) = \\(\\frac{FP}{FP+TN}\\)=\\(\\frac{\\# X/O}{\\# X/O+\\# X/X}\\)\n\n- 의미: FPR = 오해해서 미안해, recall(=TPR)을 올리려고 보니 어쩔 수 없었어 ㅠㅠ - specificity는 안나간 사람을 안나갔다고 찾아낸 비율인데 별로 안중요하다. - FPR은 recall을 올리기 위해서 “실제로는 회사 잘 다니고 있는 사람 중 최규빈이 나갈것 같다고 찍은 사람들” 의 비율이다.\n\n즉 생사람잡은 비율.. 오해해서 미안한 사람의 비율..\n\n\n\nROC curve\n- 정의: \\(x\\)축=FPR, \\(y\\)축=TPR 을 그린 커브\n- 의미: - 결국 “오해해서 미안해 vs recall”을 그린 곡선이 ROC커브이다. - 생각해보면 오해하는 사람이 많을수록 당연히 recall은 올라간다. 따라서 우상향하는 곡선이다. - 오해한 사람이 매우 적은데 recall이 우수하면 매우 좋은 모형이다. 그래서 초반부터 ROC값이 급격하게 올라가면 좋은 모형이다.\n\n\nFashion MNIST 다양한 평가지표활용\n- data\n\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n\n\nx_train.shape\n\n(60000, 28, 28)\n\n\n\n이미지는 원래 가로픽셀 세로픽셀 3 이어야 한다. (색을 표현하는 basis는 빨,녹,파)\n\n- 따라서 이미지의 차원이 단지 (28,28)이라는 것은 흑백이미지라는 뜻이다.\n\nplt.imshow(x_train[0]) \n\n&lt;matplotlib.image.AxesImage at 0x7f0370eaf880&gt;\n\n\n\n\n\n\n아닌데요?! 칼라인데요?! -&gt; 흑백이다. 그냥 밝을수록 노란색, 어두울수록 남색으로 표현한것 뿐임 (colormap이 viridis일 뿐임)\n\n\nX= x_train.reshape(-1,784)\ny= tf.keras.utils.to_categorical(y_train)\nXX = x_test.reshape(-1,784)\nyy = tf.keras.utils.to_categorical(y_test)\n\n- 다양한 평가지표를 넣는 방법 (1)\n\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(500,activation = 'relu'))\nnet.add(tf.keras.layers.Dense(500,activation = 'relu'))\nnet.add(tf.keras.layers.Dense(10,activation = 'softmax'))\nnet.compile(loss=tf.losses.categorical_crossentropy, optimizer='adam',metrics=['accuracy','Recall'])\nnet.fit(X,y,epochs=5)\n\nEpoch 1/5\n1875/1875 [==============================] - 3s 1ms/step - loss: 2.1585 - accuracy: 0.7524 - recall: 0.7193\nEpoch 2/5\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.6118 - accuracy: 0.8008 - recall: 0.7607\nEpoch 3/5\n1875/1875 [==============================] - 3s 1ms/step - loss: 0.4938 - accuracy: 0.8299 - recall: 0.7945\nEpoch 4/5\n1875/1875 [==============================] - 3s 1ms/step - loss: 0.4319 - accuracy: 0.8451 - recall: 0.8122\nEpoch 5/5\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.4209 - accuracy: 0.8493 - recall: 0.8166\n\n\n&lt;keras.callbacks.History at 0x7f0371715760&gt;\n\n\n\nnet.evaluate(XX,yy)\n\n313/313 [==============================] - 0s 610us/step - loss: 0.4402 - accuracy: 0.8457 - recall: 0.8152\n\n\n[0.4401674270629883, 0.8457000255584717, 0.8151999711990356]\n\n\n- 다양한 평가지표를 넣는 방법 (2)\n\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(500,activation = 'relu'))\nnet.add(tf.keras.layers.Dense(500,activation = 'relu'))\nnet.add(tf.keras.layers.Dense(10,activation = 'softmax'))\nnet.compile(loss=tf.losses.categorical_crossentropy, optimizer='adam',metrics=[tf.metrics.CategoricalAccuracy(),tf.metrics.Recall()])\nnet.fit(X,y,epochs=5)\n\nEpoch 1/5\n1875/1875 [==============================] - 3s 1ms/step - loss: 2.2775 - categorical_accuracy: 0.7602 - recall: 0.7325\nEpoch 2/5\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.5607 - categorical_accuracy: 0.8086 - recall: 0.7679\nEpoch 3/5\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.4602 - categorical_accuracy: 0.8357 - recall: 0.7977\nEpoch 4/5\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.4284 - categorical_accuracy: 0.8478 - recall: 0.8100\nEpoch 5/5\n1875/1875 [==============================] - 3s 1ms/step - loss: 0.4080 - categorical_accuracy: 0.8540 - recall: 0.8190\n\n\n&lt;keras.callbacks.History at 0x7f03717996d0&gt;\n\n\n\nnet.evaluate(XX,yy)\n\n313/313 [==============================] - 0s 620us/step - loss: 0.4263 - categorical_accuracy: 0.8528 - recall: 0.8190\n\n\n[0.42630136013031006, 0.8528000116348267, 0.8190000057220459]"
  },
  {
    "objectID": "posts/2022_05_09_(10주차)_5월9일.html#flatten-layer",
    "href": "posts/2022_05_09_(10주차)_5월9일.html#flatten-layer",
    "title": "[STBDA] 10wk: softmax function, 평가지표, flatten layer",
    "section": "flatten layer",
    "text": "flatten layer\n- 이미지 데이터를 분류하기 좋은 형태로 자료를 재정리하자.\n\nX = tf.constant(x_train.reshape(-1,28,28,1),dtype=tf.float64)\ny = tf.keras.utils.to_categorical(y_train)\nXX = tf.constant(x_test.reshape(-1,28,28,1),dtype=tf.float64)\nyy = tf.keras.utils.to_categorical(y_test)\n\n\nX.shape,XX.shape,y.shape,yy.shape\n\n(TensorShape([60000, 28, 28, 1]),\n TensorShape([10000, 28, 28, 1]),\n (60000, 10),\n (10000, 10))\n\n\n- 일반적인 이미지 분석 모형을 적용하기 용이한 데이터 형태로 정리했다. -&gt; 그런데 모형에 넣고 돌릴려면 다시 차원을 펼쳐야 하지 않을까?\n- 안펼치고 하고싶다.\n\nflttn = tf.keras.layers.Flatten()\n\n\nset(dir(flttn)) & {'__call__'}\n\n{'__call__'}\n\n\n\nX.shape,flttn(X).shape, X.reshape(-1,784).shape\n\n(TensorShape([60000, 28, 28, 1]),\n TensorShape([60000, 784]),\n TensorShape([60000, 784]))\n\n\n- flttn\n\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Flatten())\nnet.add(tf.keras.layers.Dense(500,activation = 'relu'))\nnet.add(tf.keras.layers.Dense(500,activation = 'relu'))\nnet.add(tf.keras.layers.Dense(10,activation = 'softmax'))\nnet.compile(loss=tf.losses.categorical_crossentropy, optimizer='adam',metrics=[tf.metrics.CategoricalAccuracy(),tf.metrics.Recall()])\nnet.fit(X,y,epochs=5)\n\nEpoch 1/5\n1875/1875 [==============================] - 3s 1ms/step - loss: 2.1290 - categorical_accuracy: 0.7476 - recall_1: 0.7071\nEpoch 2/5\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.6487 - categorical_accuracy: 0.7915 - recall_1: 0.7416\nEpoch 3/5\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.5178 - categorical_accuracy: 0.8205 - recall_1: 0.7747\nEpoch 4/5\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.4729 - categorical_accuracy: 0.8329 - recall_1: 0.7914\nEpoch 5/5\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.4462 - categorical_accuracy: 0.8401 - recall_1: 0.8023\n\n\n&lt;keras.callbacks.History at 0x7f037109e160&gt;\n\n\n\nnet.layers\n\n[&lt;keras.layers.reshaping.flatten.Flatten at 0x7f03712a2b20&gt;,\n &lt;keras.layers.core.dense.Dense at 0x7f037133d5e0&gt;,\n &lt;keras.layers.core.dense.Dense at 0x7f03711284c0&gt;,\n &lt;keras.layers.core.dense.Dense at 0x7f0371089b20&gt;]\n\n\n\nprint(X.shape)\nprint(net.layers[0](X).shape)\nprint(net.layers[1](net.layers[0](X)).shape)\nprint(net.layers[2](net.layers[1](net.layers[0](X))).shape)\n\n(60000, 28, 28, 1)\n(60000, 784)\n(60000, 500)\n(60000, 500)\n\n\n- 좀 더 복잡한 네트워크 -&gt; 하지만 한계가 보인다 -&gt; 좀 더 나은 아키텍처는 없을까\n\ntf.random.set_seed(43052)\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Flatten())\nnet.add(tf.keras.layers.Dense(500,activation='relu'))\nnet.add(tf.keras.layers.Dense(500,activation='relu'))\nnet.add(tf.keras.layers.Dense(500,activation='relu'))\nnet.add(tf.keras.layers.Dense(500,activation='relu'))\nnet.add(tf.keras.layers.Dense(10,activation='softmax'))\nnet.compile(loss=tf.losses.categorical_crossentropy, optimizer='adam',metrics='accuracy')\nnet.fit(X,y,epochs=10)\n\nEpoch 1/10\n1875/1875 [==============================] - 5s 2ms/step - loss: 1.0624 - accuracy: 0.7910\nEpoch 2/10\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.4438 - accuracy: 0.8393\nEpoch 3/10\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.4170 - accuracy: 0.8525\nEpoch 4/10\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.3961 - accuracy: 0.8597\nEpoch 5/10\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.3765 - accuracy: 0.8671\nEpoch 6/10\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.3552 - accuracy: 0.8729\nEpoch 7/10\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.3532 - accuracy: 0.8747\nEpoch 8/10\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.3367 - accuracy: 0.8798\nEpoch 9/10\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.3419 - accuracy: 0.8802\nEpoch 10/10\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.3232 - accuracy: 0.8863\n\n\n&lt;keras.callbacks.History at 0x7f03e0070d60&gt;\n\n\n\nnet.evaluate(XX,yy)\n\n313/313 [==============================] - 0s 769us/step - loss: 0.4121 - accuracy: 0.8543\n\n\n[0.41211792826652527, 0.8543000221252441]\n\n\n- layer중에 우리는 끽해야 Dense정도 쓰고있었음. \\(\\to\\) flatten과 같은 다른 layer도 많음. \\(\\to\\) 이런것도 써보자\n\nnet.summary()\n\nModel: \"sequential_5\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n flatten_2 (Flatten)         (None, 784)               0         \n                                                                 \n dense_14 (Dense)            (None, 500)               392500    \n                                                                 \n dense_15 (Dense)            (None, 500)               250500    \n                                                                 \n dense_16 (Dense)            (None, 500)               250500    \n                                                                 \n dense_17 (Dense)            (None, 500)               250500    \n                                                                 \n dense_18 (Dense)            (None, 10)                5010      \n                                                                 \n=================================================================\nTotal params: 1,149,010\nTrainable params: 1,149,010\nNon-trainable params: 0\n_________________________________________________________________"
  },
  {
    "objectID": "posts/2022_05_16_(11주차)_5월16일.html",
    "href": "posts/2022_05_16_(11주차)_5월16일.html",
    "title": "[STBDA] 11wk: MaxPool2D, Conv2D",
    "section": "",
    "text": "해당 강의노트는 전북대학교 최규빈교수님 STBDA2022 자료임\n\n\nimports\n\nimport tensorflow as tf\nimport tensorflow.experimental.numpy as tnp\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\ntnp.experimental_enable_numpy_behavior()\n\n\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n\n\nX = tf.constant(x_train.reshape(-1,28,28,1),dtype=tf.float64)\ny = tf.keras.utils.to_categorical(y_train)\nXX = tf.constant(x_test.reshape(-1,28,28,1),dtype=tf.float64)\nyy = tf.keras.utils.to_categorical(y_test)\n\n- 첫시도\n\nnet1 = tf.keras.Sequential()\nnet1.add(tf.keras.layers.Flatten())\nnet1.add(tf.keras.layers.Dense(500,activation='relu'))\nnet1.add(tf.keras.layers.Dense(500,activation='relu'))\nnet1.add(tf.keras.layers.Dense(500,activation='relu'))\nnet1.add(tf.keras.layers.Dense(500,activation='relu'))\nnet1.add(tf.keras.layers.Dense(10,activation='softmax'))\nnet1.compile(optimizer='adam', loss=tf.losses.categorical_crossentropy,metrics='accuracy')\nnet1.fit(X,y,epochs=5)\n\nEpoch 1/5\n1875/1875 [==============================] - 5s 3ms/step - loss: 1.0329 - accuracy: 0.7892\nEpoch 2/5\n1875/1875 [==============================] - 5s 2ms/step - loss: 0.4472 - accuracy: 0.8415\nEpoch 3/5\n1875/1875 [==============================] - 5s 2ms/step - loss: 0.4179 - accuracy: 0.8497\nEpoch 4/5\n1875/1875 [==============================] - 5s 2ms/step - loss: 0.3942 - accuracy: 0.8598\nEpoch 5/5\n1875/1875 [==============================] - 5s 2ms/step - loss: 0.3756 - accuracy: 0.8668\n\n\n&lt;keras.callbacks.History at 0x7f07e021dfa0&gt;\n\n\n\nnet1.evaluate(XX,yy)\n\n313/313 [==============================] - 0s 754us/step - loss: 0.4255 - accuracy: 0.8505\n\n\n[0.4255034327507019, 0.8504999876022339]\n\n\n\nnet1.summary()\n\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n flatten (Flatten)           (None, 784)               0         \n                                                                 \n dense (Dense)               (None, 500)               392500    \n                                                                 \n dense_1 (Dense)             (None, 500)               250500    \n                                                                 \n dense_2 (Dense)             (None, 500)               250500    \n                                                                 \n dense_3 (Dense)             (None, 500)               250500    \n                                                                 \n dense_4 (Dense)             (None, 10)                5010      \n                                                                 \n=================================================================\nTotal params: 1,149,010\nTrainable params: 1,149,010\nNon-trainable params: 0\n_________________________________________________________________\n\n\n- 두번째 시도\n\nnet2 = tf.keras.Sequential()\nnet2.add(tf.keras.layers.Conv2D(30,(2,2),activation='relu'))\nnet2.add(tf.keras.layers.MaxPool2D())\nnet2.add(tf.keras.layers.Conv2D(30,(2,2),activation='relu'))\nnet2.add(tf.keras.layers.MaxPool2D())\nnet2.add(tf.keras.layers.Flatten())\n#net2.add(tf.keras.layers.Dense(500,activation='relu'))\nnet2.add(tf.keras.layers.Dense(10,activation='softmax'))\nnet2.compile(optimizer='adam', loss=tf.losses.categorical_crossentropy,metrics='accuracy')\nnet2.fit(X,y,epochs=5)\n\nEpoch 1/5\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.7960 - accuracy: 0.8061\nEpoch 2/5\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.3814 - accuracy: 0.8644\nEpoch 3/5\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.3400 - accuracy: 0.8769\nEpoch 4/5\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.3182 - accuracy: 0.8854\nEpoch 5/5\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.3011 - accuracy: 0.8914\n\n\n&lt;keras.callbacks.History at 0x7f07b4176850&gt;\n\n\n\nnet2.evaluate(XX,yy)\n\n313/313 [==============================] - 0s 815us/step - loss: 0.3572 - accuracy: 0.8707\n\n\n[0.3571898341178894, 0.8707000017166138]\n\n\n\nnet2.summary()\n\nModel: \"sequential_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d (Conv2D)             (None, 27, 27, 30)        150       \n                                                                 \n max_pooling2d (MaxPooling2D  (None, 13, 13, 30)       0         \n )                                                               \n                                                                 \n conv2d_1 (Conv2D)           (None, 12, 12, 30)        3630      \n                                                                 \n max_pooling2d_1 (MaxPooling  (None, 6, 6, 30)         0         \n 2D)                                                             \n                                                                 \n flatten_1 (Flatten)         (None, 1080)              0         \n                                                                 \n dense_5 (Dense)             (None, 10)                10810     \n                                                                 \n=================================================================\nTotal params: 14,590\nTrainable params: 14,590\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\n14590/ 1149010\n\n0.012697887746842934\n\n\n\nc1, m1, c2, m2, flttn, dns = net2.layers\n\n\nprint(X.shape) # 입력이미지 = 2D\nprint(c1(X).shape) #2D\nprint(m1(c1(X)).shape)  #2D\nprint(c2(m1(c1(X))).shape) #2D\nprint(m2(c2(m1(c1(X)))).shape) #2D\nprint(flttn(m2(c2(m1(c1(X))))).shape)# 1D\nprint(dns(flttn(m2(c2(m1(c1(X)))))).shape)# 1D\n\n(60000, 28, 28, 1)\n(60000, 27, 27, 30)\n(60000, 13, 13, 30)\n(60000, 12, 12, 30)\n(60000, 6, 6, 30)\n(60000, 1080)\n(60000, 10)\n\n\n\n\nMaxPool2D\n\n테스트1\n- 레이어생성\n\nm=tf.keras.layers.MaxPool2D()\n\n- 입력데이터\n\nXXX = tnp.arange(1*4*4*1).reshape(1,4,4,1)\nXXX.reshape(1,4,4)\n\n&lt;tf.Tensor: shape=(1, 4, 4), dtype=int64, numpy=\narray([[[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11],\n        [12, 13, 14, 15]]])&gt;\n\n\n\n 흑백 이미지다 생각하고 뒤에 채널 생략하자 걍 보기쉽게\n\n- 입력데이터가 레이어를 통과한 모습\n\nm(XXX).reshape(1,2,2)\n\n&lt;tf.Tensor: shape=(1, 2, 2), dtype=int64, numpy=\narray([[[ 5,  7],\n        [13, 15]]])&gt;\n\n\n- MaxPool2D layer의 역할: (2,2)윈도우를 만들고 (2,2)윈도우에서 max를 뽑아 값을 기록, 윈도우를 움직이면서 반복\n\n\n테스트2\n\nXXX = tnp.arange(1*6*6*1).reshape(1,6,6,1)\nXXX.reshape(1,6,6)\n\n&lt;tf.Tensor: shape=(1, 6, 6), dtype=int64, numpy=\narray([[[ 0,  1,  2,  3,  4,  5],\n        [ 6,  7,  8,  9, 10, 11],\n        [12, 13, 14, 15, 16, 17],\n        [18, 19, 20, 21, 22, 23],\n        [24, 25, 26, 27, 28, 29],\n        [30, 31, 32, 33, 34, 35]]])&gt;\n\n\n\nm(XXX).reshape(1,3,3)\n\n&lt;tf.Tensor: shape=(1, 3, 3), dtype=int64, numpy=\narray([[[ 7,  9, 11],\n        [19, 21, 23],\n        [31, 33, 35]]])&gt;\n\n\n\n\n테스트3\n\nm=tf.keras.layers.MaxPool2D(pool_size=(3, 3))\n\n\nXXX = tnp.arange(1*6*6*1).reshape(1,6,6,1)\nXXX.reshape(1,6,6)\n\n&lt;tf.Tensor: shape=(1, 6, 6), dtype=int64, numpy=\narray([[[ 0,  1,  2,  3,  4,  5],\n        [ 6,  7,  8,  9, 10, 11],\n        [12, 13, 14, 15, 16, 17],\n        [18, 19, 20, 21, 22, 23],\n        [24, 25, 26, 27, 28, 29],\n        [30, 31, 32, 33, 34, 35]]])&gt;\n\n\n\nm(XXX).reshape(1,2,2)\n\n&lt;tf.Tensor: shape=(1, 2, 2), dtype=int64, numpy=\narray([[[14, 17],\n        [32, 35]]])&gt;\n\n\n\n\n테스트4\n\nm=tf.keras.layers.MaxPool2D(pool_size=(2, 2))\n\n\nXXX = tnp.arange(1*5*5*1).reshape(1,5,5,1)\nXXX.reshape(1,5,5)\n\n&lt;tf.Tensor: shape=(1, 5, 5), dtype=int64, numpy=\narray([[[ 0,  1,  2,  3,  4],\n        [ 5,  6,  7,  8,  9],\n        [10, 11, 12, 13, 14],\n        [15, 16, 17, 18, 19],\n        [20, 21, 22, 23, 24]]])&gt;\n\n\n\nm(XXX).reshape(1,2,2)\n\n&lt;tf.Tensor: shape=(1, 2, 2), dtype=int64, numpy=\narray([[[ 6,  8],\n        [16, 18]]])&gt;\n\n\n\nm=tf.keras.layers.MaxPool2D(pool_size=(2, 2),padding=\"same\")\n\n\nXXX = tnp.arange(1*5*5*1).reshape(1,5,5,1)\nXXX.reshape(1,5,5)\n\n&lt;tf.Tensor: shape=(1, 5, 5), dtype=int64, numpy=\narray([[[ 0,  1,  2,  3,  4],\n        [ 5,  6,  7,  8,  9],\n        [10, 11, 12, 13, 14],\n        [15, 16, 17, 18, 19],\n        [20, 21, 22, 23, 24]]])&gt;\n\n\n\nm(XXX).reshape(1,3,3)\n\n&lt;tf.Tensor: shape=(1, 3, 3), dtype=int64, numpy=\narray([[[ 6,  8,  9],\n        [16, 18, 19],\n        [21, 23, 24]]])&gt;\n\n\n\n\n테스트5\n\nXXX = tnp.arange(2*4*4*1).reshape(2,4,4,1)\nXXX.reshape(2,4,4)\n\n&lt;tf.Tensor: shape=(2, 4, 4), dtype=int64, numpy=\narray([[[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11],\n        [12, 13, 14, 15]],\n\n       [[16, 17, 18, 19],\n        [20, 21, 22, 23],\n        [24, 25, 26, 27],\n        [28, 29, 30, 31]]])&gt;\n\n\n\nm(XXX).reshape(2,2,2)\n\n&lt;tf.Tensor: shape=(2, 2, 2), dtype=int64, numpy=\narray([[[ 5,  7],\n        [13, 15]],\n\n       [[21, 23],\n        [29, 31]]])&gt;\n\n\n\n\n테스트6\n\nXXX = tnp.arange(1*4*4*3).reshape(1,4,4,3)\n\n\nXXX[...,0]\n\n&lt;tf.Tensor: shape=(1, 4, 4), dtype=int64, numpy=\narray([[[ 0,  3,  6,  9],\n        [12, 15, 18, 21],\n        [24, 27, 30, 33],\n        [36, 39, 42, 45]]])&gt;\n\n\n\nm(XXX)[...,0]\n\n&lt;tf.Tensor: shape=(1, 2, 2), dtype=int64, numpy=\narray([[[15, 21],\n        [39, 45]]])&gt;\n\n\n\n\n\nConv2D\n\n테스트1\n- 레이어생성\n\ncnv = tf.keras.layers.Conv2D(1,(2,2))\n\n- XXX생성\n\nXXX = tnp.arange(1*4*4*1,dtype=tf.float64).reshape(1,4,4,1)\nXXX.reshape(1,4,4)\n\n&lt;tf.Tensor: shape=(1, 4, 4), dtype=float64, numpy=\narray([[[ 0.,  1.,  2.,  3.],\n        [ 4.,  5.,  6.,  7.],\n        [ 8.,  9., 10., 11.],\n        [12., 13., 14., 15.]]])&gt;\n\n\n\n dytpe이 int형이면 에러가 나므로 float형으로 바꿔주기\n\n\ncnv(XXX).reshape(1,3,3)\n\n&lt;tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=\narray([[[3.1202679, 3.3676739, 3.61508  ],\n        [4.109892 , 4.3572974, 4.6047034],\n        [5.099516 , 5.346921 , 5.594327 ]]], dtype=float32)&gt;\n\n\n\nXXX에서 cnv(XXX)로 가는 맵핑을 찾는건 쉽지 않아보인다.\n심지어 랜덤으로 결정되는 부분도 있어보임 \\(\\to\\)  계속 값이 바뀜\n\n- 코드정리 + 시드통일\n\ntf.random.set_seed(43052)\ncnv = tf.keras.layers.Conv2D(1,(2,2))\nXXX = tnp.arange(1*4*4*1,dtype=tf.float64).reshape(1,4,4,1)\n\n- conv의 입출력\n\nprint(XXX.reshape(1,4,4))\nprint(cnv(XXX).reshape(1,3,3))\n\ntf.Tensor(\n[[[ 0.  1.  2.  3.]\n  [ 4.  5.  6.  7.]\n  [ 8.  9. 10. 11.]\n  [12. 13. 14. 15.]]], shape=(1, 4, 4), dtype=float64)\ntf.Tensor(\n[[[ -5.372491  -6.5789    -7.785309]\n  [-10.198126 -11.404535 -12.610944]\n  [-15.023762 -16.230171 -17.436579]]], shape=(1, 3, 3), dtype=float32)\n\n\n- conv연산 추론\n\ntype(cnv.weights)\n\nlist\n\n\n\ntf.reshape(cnv.weights[0],(2,2))\n\n&lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy=\narray([[-0.2408014 ,  0.3118649 ],\n       [-0.70300657, -0.5744659 ]], dtype=float32)&gt;\n\n\n\n0 * -0.2408014 + 1 * 0.3118649 + 4 * -0.70300657 + 5 * -0.5744659 + 0\n\n-5.372490879999999\n\n\n- 내가 정의한 weights를 대입하여 conv 연산 확인\n\ncnv.get_weights()[0].shape\n\n(2, 2, 1, 1)\n\n\n\nw = np.array([1/4,1/4,1/4,1/4],dtype=np.float32).reshape(2, 2, 1, 1)\nb = np.array([3],dtype=np.float32)\n\n\ncnv.set_weights([w,b])\n\n\nXXX.reshape(1,4,4)\n\n&lt;tf.Tensor: shape=(1, 4, 4), dtype=float64, numpy=\narray([[[ 0.,  1.,  2.,  3.],\n        [ 4.,  5.,  6.,  7.],\n        [ 8.,  9., 10., 11.],\n        [12., 13., 14., 15.]]])&gt;\n\n\n\ncnv(XXX).reshape(1,3,3)\n\n&lt;tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=\narray([[[ 5.5,  6.5,  7.5],\n        [ 9.5, 10.5, 11.5],\n        [13.5, 14.5, 15.5]]], dtype=float32)&gt;\n\n\n\nnp.mean([0,1,4,5])+3, np.mean([1,2,5,6])+3, np.mean([2,3,6,7])+3\n\n(5.5, 6.5, 7.5)\n\n\n\n\ntf.keras.layers.Conv2D(1,kernel_size=(2,2)) 요약\n- 요약\n\nsize=(2,2)인 윈도우를 만듬.\nXXX에 윈도우를 통과시켜서 (2,2)크기의 sub XXX 를 얻음. sub XXX의 각 원소에 conv2d.weights[0]의 각 원소를 element-wise하게 곱한다.\n(2)의 결과를 모두 더한다. 그리고 그 결과에 다시 conv2d.weights[1]을 수행\n윈도우를 이동시키면서 반복!\n\n\n\n테스트2\n- 레이어와 XXX생성\n\ntf.random.set_seed(43052)\ncnv = tf.keras.layers.Conv2D(1,(3,3))\nXXX = tnp.arange(1*5*5*1,dtype=tf.float64).reshape(1,5,5,1)\n\n\nXXX.reshape(1,5,5) ## 입력: XXX\n\n&lt;tf.Tensor: shape=(1, 5, 5), dtype=float64, numpy=\narray([[[ 0.,  1.,  2.,  3.,  4.],\n        [ 5.,  6.,  7.,  8.,  9.],\n        [10., 11., 12., 13., 14.],\n        [15., 16., 17., 18., 19.],\n        [20., 21., 22., 23., 24.]]])&gt;\n\n\n\ncnv(XXX).reshape(1,3,3) ## 출력: conv(XXX)\n\n&lt;tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=\narray([[[-3.3962166, -3.7455084, -4.0948005],\n        [-5.1426764, -5.4919686, -5.84126  ],\n        [-6.889136 , -7.238429 , -7.5877204]]], dtype=float32)&gt;\n\n\n\ntf.reshape(cnv.weights[0],(1,3,3)) ## 커널의 가중치\n\n&lt;tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=\narray([[[ 0.10026586, -0.17371601,  0.16699821],\n        [-0.4737161 ,  0.19852251,  0.09610051],\n        [ 0.09313911, -0.29948625, -0.05739981]]], dtype=float32)&gt;\n\n\n\ntf.reduce_sum(XXX.reshape(1,5,5)[0,:3,:3] * tf.reshape(cnv.weights[0],(3,3)))\n\n&lt;tf.Tensor: shape=(), dtype=float64, numpy=-3.396216869354248&gt;\n\n\n\n\n테스트3\n\n\nXXX = tf.constant([[3,3,2,1,0],[0,0,1,3,1],[3,1,2,2,3],[2,0,0,2,2],[2,0,0,0,1]],dtype=tf.float64).reshape(1,5,5,1)\nXXX.reshape(1,5,5)\n\n&lt;tf.Tensor: shape=(1, 5, 5), dtype=float64, numpy=\narray([[[3., 3., 2., 1., 0.],\n        [0., 0., 1., 3., 1.],\n        [3., 1., 2., 2., 3.],\n        [2., 0., 0., 2., 2.],\n        [2., 0., 0., 0., 1.]]])&gt;\n\n\n\ncnv = tf.keras.layers.Conv2D(1,(3,3))\n\n\ncnv.weights\n\n[]\n\n\n\n 처음엔 weights가 없다가\n\n\ncnv(XXX).reshape(1,3,3)\n\n&lt;tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=\narray([[[-0.34136963, -0.7676697 ,  2.3211648 ],\n        [ 1.1140423 , -0.15018106, -0.2899468 ],\n        [ 0.866542  , -2.0289044 , -1.186438  ]]], dtype=float32)&gt;\n\n\n\ncnv.weights[0]\n\n&lt;tf.Variable 'conv2d_8/kernel:0' shape=(3, 3, 1, 1) dtype=float32, numpy=\narray([[[[ 0.16768384]],\n\n        [[-0.33952343]],\n\n        [[-0.37316394]]],\n\n\n       [[[ 0.5341035 ]],\n\n        [[ 0.27497447]],\n\n        [[-0.38560677]]],\n\n\n       [[[ 0.19056737]],\n\n        [[-0.26161364]],\n\n        [[ 0.49799764]]]], dtype=float32)&gt;\n\n\n\n XXX가 cnv를 통과하면서 weights가 생긴다.\n\n\n_w = tf.constant([[0,1,2],[2,2,0],[0,1,2]],dtype=tf.float64).reshape(3,3,1,1)\n_b = tf.constant([0],dtype=tf.float64)\n\n\ncnv.set_weights([_w,_b])\n\n\ncnv(XXX).reshape(1,3,3)\n\n&lt;tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=\narray([[[12., 12., 17.],\n        [10., 17., 19.],\n        [ 9.,  6., 14.]]], dtype=float32)&gt;\n\n\n\n\n테스트4\n\ntf.random.set_seed(43052)\ncnv = tf.keras.layers.Conv2D(1,(2,2))\nXXX = tnp.arange(2*5*5*1,dtype=tf.float64).reshape(2,5,5,1)\n\n\nprint(XXX.reshape(2,5,5))\ncnv(XXX) # weights를 초기화 시키기 위해서 레이어를 1회 통과\ncnv.set_weights([w,b])\nprint(cnv(XXX).reshape(2,4,4))\n\ntf.Tensor(\n[[[ 0.  1.  2.  3.  4.]\n  [ 5.  6.  7.  8.  9.]\n  [10. 11. 12. 13. 14.]\n  [15. 16. 17. 18. 19.]\n  [20. 21. 22. 23. 24.]]\n\n [[25. 26. 27. 28. 29.]\n  [30. 31. 32. 33. 34.]\n  [35. 36. 37. 38. 39.]\n  [40. 41. 42. 43. 44.]\n  [45. 46. 47. 48. 49.]]], shape=(2, 5, 5), dtype=float64)\ntf.Tensor(\n[[[ 6.  7.  8.  9.]\n  [11. 12. 13. 14.]\n  [16. 17. 18. 19.]\n  [21. 22. 23. 24.]]\n\n [[31. 32. 33. 34.]\n  [36. 37. 38. 39.]\n  [41. 42. 43. 44.]\n  [46. 47. 48. 49.]]], shape=(2, 4, 4), dtype=float32)\n\n\n\nnp.mean([0,1,5,6])+3,np.mean([25,26,30,31])+3,\n\n(6.0, 31.0)\n\n\n\n\n테스트5\n-\n\ntf.random.set_seed(43052)\ncnv = tf.keras.layers.Conv2D(4,(2,2),activation='relu')\nXXX = tnp.arange(1*2*2*1,dtype=tf.float64).reshape(1,2,2,1)\n\n\nprint(XXX.reshape(1,2,2))\n\ntf.Tensor(\n[[[0. 1.]\n  [2. 3.]]], shape=(1, 2, 2), dtype=float64)\n\n\n\ncnv(XXX)\n\n&lt;tf.Tensor: shape=(1, 1, 1, 4), dtype=float32, numpy=\narray([[[[1.7573347 , 0.84183925, 1.7484205 , 0.        ]]]],\n      dtype=float32)&gt;\n\n\n\ncnv.weights[0] # (2,2) 커널의 크기 // 1은 XXX의 채널수 // 4는 conv(XXX)의 채널수\n\n&lt;tf.Variable 'conv2d_10/kernel:0' shape=(2, 2, 1, 4) dtype=float32, numpy=\narray([[[[-0.00448787,  0.17636931,  0.17748284,  0.5281223 ]],\n\n        [[-0.01255804,  0.0121181 , -0.1567888 ,  0.513507  ]]],\n\n\n       [[[ 0.3668834 ,  0.32297856,  0.32239443, -0.5187031 ]],\n\n        [[ 0.3453753 ,  0.06125468,  0.42014015, -0.36382926]]]],\n      dtype=float32)&gt;\n\n\n\ncnv.weights[0][...,0].reshape(2,2) ## conv(XXX)의 첫번째채널 출력을 얻기 위해 곱해지는 w\n\n&lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy=\narray([[-0.08230966, -0.36398047],\n       [ 0.19759327,  0.33916563]], dtype=float32)&gt;\n\n\n\ntf.reduce_sum(XXX.reshape(1,2,2) * cnv.weights[0][...,0].reshape(2,2)) ### conv(XXX)의 첫번째 채널 출력결과\n\n&lt;tf.Tensor: shape=(), dtype=float64, numpy=1.0487029552459717&gt;\n\n\n- 계산결과를 확인하기 쉽게 하기 위한 약간의 트릭\n\ntf.random.set_seed(43052)\ncnv = tf.keras.layers.Conv2D(4,(2,2))\nXXX = tnp.array([1]*1*2*2*1,dtype=tf.float64).reshape(1,2,2,1)\n\n\nprint(XXX.reshape(1,2,2))\n\ntf.Tensor(\n[[[1. 1.]\n  [1. 1.]]], shape=(1, 2, 2), dtype=float64)\n\n\n\n이렇게 XXX를 설정하면 cnv(XXX)의 결과는 단지 cnv의 weight들의 sum이 된다.\n\n\ncnv(XXX)\n\n&lt;tf.Tensor: shape=(1, 1, 1, 4), dtype=float32, numpy=\narray([[[[ 0.09046876, -0.6207629 , -0.25241536, -0.7710641 ]]]],\n      dtype=float32)&gt;\n\n\n\ncnv.weights[0] # (2,2) 커널의 크기 // 1은 XXX의 채널수 // 4는 conv(XXX)의 채널수\n\n&lt;tf.Variable 'conv2d_24/kernel:0' shape=(2, 2, 1, 4) dtype=float32, numpy=\narray([[[[-0.08230966, -0.15132892, -0.12760344, -0.38952267]],\n\n        [[-0.36398047,  0.07347518, -0.08780673,  0.46633136]]],\n\n\n       [[[ 0.19759327, -0.46042526, -0.15406173, -0.34838456]],\n\n        [[ 0.33916563, -0.08248386,  0.11705655, -0.49948823]]]],\n      dtype=float32)&gt;\n\n\n\ncnv.weights[0][...,0].reshape(2,2) ## conv(XXX)의 첫번째채널 출력을 얻기 위해 곱해지는 w\n\n&lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy=\narray([[-0.08230966, -0.36398047],\n       [ 0.19759327,  0.33916563]], dtype=float32)&gt;\n\n\n\ntf.reduce_sum(cnv.weights[0][...,0])\n#tf.reduce_sum(XXX.reshape(1,2,2) * cnv.weights[0][...,0].reshape(2,2)) ### conv(XXX)의 첫번째 채널 출력결과\n\n&lt;tf.Tensor: shape=(), dtype=float32, numpy=0.090468764&gt;\n\n\n\n\n테스트6\n- 결과확인을 쉽게하기 위해서 XXX를 1로 통일\n\ntf.random.set_seed(43052)\ncnv = tf.keras.layers.Conv2D(4,(2,2))\nXXX = tnp.array([1]*1*2*2*3,dtype=tf.float64).reshape(1,2,2,3)\n\n\ncnv(XXX)\n\n&lt;tf.Tensor: shape=(1, 1, 1, 4), dtype=float32, numpy=\narray([[[[ 0.3297621, -0.4498347, -1.0487393, -1.580095 ]]]],\n      dtype=float32)&gt;\n\n\n\ncnv.weights[0] ## (2,2)는 커널의 사이즈 // 3은 XXX의채널 // 4는 cnv(XXX)의 채널\n\n&lt;tf.Variable 'conv2d_33/kernel:0' shape=(2, 2, 3, 4) dtype=float32, numpy=\narray([[[[-0.06956434, -0.12789628, -0.10784459, -0.32920673],\n         [-0.30761963,  0.06209785, -0.07421023,  0.3941219 ],\n         [ 0.16699678, -0.38913035, -0.13020593, -0.29443866]],\n\n        [[ 0.28664726, -0.0697116 ,  0.09893084, -0.4221446 ],\n         [-0.23161241, -0.16410837, -0.36420006,  0.12424195],\n         [-0.14245945,  0.36286396, -0.10751781,  0.1733647 ]]],\n\n\n       [[[ 0.02764335,  0.15547717, -0.42024496, -0.31893867],\n         [ 0.22414821,  0.3619454 , -0.00282967, -0.3503708 ],\n         [ 0.4610079 , -0.17417148,  0.00401336, -0.29777044]],\n\n        [[-0.1620284 , -0.42066965, -0.01578814, -0.4240524 ],\n         [ 0.37925082,  0.24236053,  0.3949356 , -0.20996472],\n         [-0.30264795, -0.28889188, -0.3237777 ,  0.37506342]]]],\n      dtype=float32)&gt;\n\n\n\ncnv.weights[0][...,0] ## cnv(XXX)의 첫번째 채널결과를 얻기 위해서 사용하는 w\n\n&lt;tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=\narray([[[-0.06956434, -0.30761963,  0.16699678],\n        [ 0.28664726, -0.23161241, -0.14245945]],\n\n       [[ 0.02764335,  0.22414821,  0.4610079 ],\n        [-0.1620284 ,  0.37925082, -0.30264795]]], dtype=float32)&gt;\n\n\n\ntf.reduce_sum(cnv.weights[0][...,0]) ### cnv(XXX)의 첫번째 채널의 결과\n\n&lt;tf.Tensor: shape=(), dtype=float32, numpy=0.32976213&gt;\n\n\n\nprint(tf.reduce_sum(cnv.weights[0][...,0]))\nprint(tf.reduce_sum(cnv.weights[0][...,1]))\nprint(tf.reduce_sum(cnv.weights[0][...,2]))\nprint(tf.reduce_sum(cnv.weights[0][...,3])) ### cnv(XXX)의 결과\n\ntf.Tensor(0.32976213, shape=(), dtype=float32)\ntf.Tensor(-0.44983464, shape=(), dtype=float32)\ntf.Tensor(-1.0487392, shape=(), dtype=float32)\ntf.Tensor(-1.5800952, shape=(), dtype=float32)\n\n\n\nw_red = cnv.weights[0][...,0][...,0]\nw_green = cnv.weights[0][...,0][...,1]\nw_blue = cnv.weights[0][...,0][...,2]\n\n\ntf.reduce_sum(XXX[...,0] * w_red + XXX[...,1] * w_green + XXX[...,2] * w_blue) ## cnv(XXX)의 첫채널 출력결과\n\n&lt;tf.Tensor: shape=(), dtype=float64, numpy=0.32976213097572327&gt;\n\n\n\n\n\nhw\n아래와 같은 흑백이미지가 있다고 하자.\n0 0 0 1 1 1\n0 0 0 1 1 1\n0 0 0 1 1 1\n0 0 0 1 1 1\n0 0 0 1 1 1\n0 0 0 1 1 1\n위의 이미지에 아래와 같은 weight를 가진 필터를 적용하여 convolution한 결과를 계산하라. (bias는 0으로 가정한다)\n-1 1\n-1 1"
  },
  {
    "objectID": "posts/2022-06-09-(14주차) 6월9일.html",
    "href": "posts/2022-06-09-(14주차) 6월9일.html",
    "title": "[STBDA] 14wk: GAN",
    "section": "",
    "text": "해당 강의노트는 전북대학교 최규빈교수님 STBDA2022 자료임\n\n\nGAN (Generative Adversarial Network)\n- 저자: 이안굿펠로우 (이름이 특이함. 좋은친구..) - 제가 추천한 딥러닝 교재의 저자 - 천재임 - 지도교수가 요수아 벤지오\n- 논문 NIPS, 저는 이 논문 읽고 소름돋았어요.. - https://arxiv.org/abs/1406.2661 (현재시점, 38751회 인용되었음)\n- 최근 10년간 머신러닝 분야에서 가장 혁신적인 아이디어이다. (얀르쿤, 2014년 시점..)\n- 무슨내용? 생성모형\n\n생성모형이란? (쉬운 설명)\n\n만들수 없다면 이해하지 못한 것이다, 리처드 파인만 (천재 물리학자)\n\n- 사진속에 들어있는 동물이 개인지 고양이인지 맞출수 있는 기계와 개와 고양이를 그릴수 있는 기계중 어떤것이 더 시각적보에 대한 이해가 깊다고 볼수 있는가?\n- 진정으로 인공지능이 이미지를 이해했다면, 이미지를 만들수도 있어야 한다. \\(\\to\\) 이미지를 생성하는 모형을 만들어보자 \\(\\to\\) 성공\n\n\n\nGAN의 응용분야\n- 내가 찍은 사진이 피카소의 화풍으로 표현된다면? - https://www.lgsl.kr/sto/stories/60/ALMA2020070001\n- 퀸의 라이브에이드가 4k로 나온다면?\n- 1920년대 서울의 모습이 칼라로 복원된다면?\n- 딥페이크: 유명인의 가짜 포르노, 가짜뉴스, 협박(거짓기소)\n- 게임영상 (파이널판타지)\n- 거북이의 커버..\n- 너무 많아요…..\n\n\n생성모형이란? 통계학과 버전의 설명\n\n제한된 정보만으로 어떤 문제를 풀 떄, 그 과정에서 원래의 문제보다 일반적인 문제를 풀지 말고, 가능한 원래의 문제를 직접 풀어야한다. 배프닉 (SVM 창시자)\n\n- 이미지 \\(\\boldsymbol{x}\\)가 주어졌을 경우 라벨을 \\(y\\)라고 하자.\n- 이미지를 보고 라벨을 맞추는 일은 \\(p(y| \\boldsymbol{x})\\)에 관심이 있다.\n- 이미지를 생성하는 일은 \\(p(\\boldsymbol{x},y)\\)에 관심이 있는것이다.\n- 데이터의 생성확률 \\(p(\\boldsymbol{x},y)\\)을 알면 클래스의 사후확률 \\(p(y|\\boldsymbol{x})\\)를 알 수 있음. (아래의 수식 참고) 하지만 역은 불가능\n\\[p(y|x) = \\frac{p(x,y)}{p(x)} = \\frac{p(x,y)}{\\sum_{y}p(x,y)} \\]\n\n즉 이미지를 생성하는일은 분류문제보다 더 어려운 일이라 해석가능\n\n- 따라서 배프닉의 원리에 의하면 식별적 분류가 생성적 분류보다 바람직한 접근법이라 할 수 있음.\n- 하지만 다양한 현실문제에서 생성모형이 유용할떄가 많다.\n\n\n\nGAN의 원리\n- GAN은 생성모형중 하나임\n- GAN의 원리는 경찰과 위조지폐범이 서로 선의의(?) 경쟁을 통하여 서로 발전하는 모형으로 설명할 수 있다.\n\nThe generative model can be thought of as analogous to a team of counterfeiters, trying to produce fake currency and use it without detection, while the discriminative model is analogous to the police, trying to detect the counterfeit currency. Competition in this game drives both teams to improve their methods until the counterfeits are indistiguishable from the genuine articles.\n\n- 서로 적대적인(adversarial) 네트워크(network)를 동시에 학습시켜 가짜이미지를 만든다(generate)\n- 무식한 상황극..\n\n위조범: 가짜돈을 만들어서 부자가 되어야지! (가짜돈을 그림)\n경찰: (위조범이 만든 돈을 보고) 이건 가짜다!\n위조범: 걸렸군.. 더 정교하게 만들어야지..\n경찰: 이건 진짠가?… –&gt; 상사에게 혼남. 그것도 구분못하냐고\n위조범: 더 정교하게 만들자..\n경찰: 더 판별능력을 업그레이드 하자!\n반복..\n\n- 굉장히 우수한 경찰조차도 진짜와 가짜를 구분하지 못할때(=진짜 이미지를 0.5의 확률로만 진짜라고 말할때 = 가짜 이미지를 0.5의 확률로만 가짜라고 말할때) 학습을 멈춘다.\n\n\n구현\n- 목표: 노이즈에서 mnist자료의 이미지를 생성하여 보자.\n\nimport\n\nimport tensorflow as tf  \nimport tensorflow.experimental.numpy as tnp\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tqdm import tqdm\n\n\ntnp.experimental_enable_numpy_behavior()\n\n\n\n데이터\n\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n\n\nXreal = x_train.reshape(-1,784)/255\n\n\n\n위조지폐범의 설계: noise -&gt; 가짜이미지를 만들어내는 네트워크를 만들자.\n- 네트워크의 입력: 적당한 벡터 혹은 매트릭스에 노이즈 (랜덤으로 생성한 어떠한 숫자) 를 채운 것\n- 네트워크의 출력: (28,28) shape의 매트릭스 혹은 784개의 원소를 가지는 벡터\n\nnet_counterfeiter= tf.keras.Sequential()\nnet_counterfeiter.add(tf.keras.layers.Dense(256,activation='relu'))\nnet_counterfeiter.add(tf.keras.layers.Dense(512,activation='relu'))\nnet_counterfeiter.add(tf.keras.layers.Dense(1024,activation='relu'))\nnet_counterfeiter.add(tf.keras.layers.Dense(784))\n\n2022-06-09 12:43:21.685836: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n\n\n\n\n경찰의 설계: 진짜이미지는 1, 가짜이미지는 0으로 판별하는 DNN을 만들자.\n- 네트워크의 입력? - X: (28,28) shape의 matrix 혹은 784개의 원소를 가지는 벡터\n- 네트워크의 출력? yhat - yhat은 진짜이미지일수록1, 가짜이미지일수록 0이 되어야 한다. (왜냐하면 y가 진짜이미지이면 1, 가짜이미지이면 0 이므로)\n\nnet_police = tf.keras.Sequential()\nnet_police.add(tf.keras.layers.Dense(1024,activation='relu'))\nnet_police.add(tf.keras.layers.Dropout(0.3))\nnet_police.add(tf.keras.layers.Dense(512,activation='relu'))\nnet_police.add(tf.keras.layers.Dropout(0.3))\nnet_police.add(tf.keras.layers.Dense(256,activation='relu'))\nnet_police.add(tf.keras.layers.Dropout(0.3))               \nnet_police.add(tf.keras.layers.Dense(1,activation='sigmoid'))\n\n\n\n일단 스토리를 계속 진행해보겠습니다.\n- 진짜 이미지가 아래와 같이 있다.\n\nplt.imshow(Xreal[1].reshape(28,28))\n\n&lt;matplotlib.image.AxesImage at 0x7f549c0232b0&gt;\n\n\n\n\n\n- 이 이미지를 경찰이 봤습니다 -&gt; yhat이 나와야 하고, yhat \\(\\approx\\) 1 이어야 한다. (왜? 진짜 이미지니까)\n\npolicehat_from_realimage = net_police(Xreal)\npolicehat_from_realimage\n\n&lt;tf.Tensor: shape=(60000, 1), dtype=float32, numpy=\narray([[0.4638827 ],\n       [0.45302874],\n       [0.50195265],\n       ...,\n       [0.4384173 ],\n       [0.48654714],\n       [0.53714067]], dtype=float32)&gt;\n\n\n\n진짜 이미지이므로 위의 값들이 모두 1이어야함. 즉 yhat \\(\\approx\\) 1 이어야 좋은 것임\n하지만 그렇지 못함 (배운것이 없는 무능한 경찰)\n\n- 이번에는 가짜 이미지를 경찰이 봤다고 생각하자.\n(step1) 랜덤으로 아무숫자나 생성한다.\n\nNoise1=tnp.random.randn(10).reshape(1,10)\n\n(step2) 위조범을 시켜서 이미지를 생성시킨다.\n\nXfake1 = net_counterfeiter(Noise1)\nplt.imshow(Xfake1.reshape(28,28))\n\n&lt;matplotlib.image.AxesImage at 0x7f549c0eb5b0&gt;\n\n\n\n\n\n(step3) 위조범이 생성한 이미지를 경찰한테 넘겨본다.\n\npolicehat_from_Xfake1 = net_police(Xfake1)\npolicehat_from_Xfake1\n\n&lt;tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.49095842]], dtype=float32)&gt;\n\n\n- 경찰의 실력도 형편없고 위조범의 실력도 형편없다.\n\n\n\n경찰네트워크의 실력을 향상하자\n- 데이터정리 - 원래 n=60000개의 real image가 있음. 이 자료중 일부를 batch_size=500 만큼 뽑아 이것을 \\({\\bf X}_{batch}\\)라고 하자. 따라서 \\({\\bf X}_{batch}\\)의 차원은 (500,784) 이다. - 위조범이 만든 가짜자료를 원래자료의 batch_size와 같은 숫자인 500만큼 만듬. 그리고 이것을 \\({\\bf \\tilde X}_{batch}\\)라고 하자. 그러면 \\({\\bf \\tilde X}_{batch}\\)의 차원은 (500,784)이다. - 진짜자료는 1, 가짜자료는 0으로 라벨링\n\nbatch_size = 500\n\n\nNoise_batch = np.random.normal(0,1,size=(batch_size,10))\nXfake_batch = net_counterfeiter(Noise_batch)\n\n\nXreal_batch = Xreal[:batch_size]\nXpolice_batch = tf.concat([Xreal_batch,Xfake_batch],axis=0)\n\n\nypolice_batch = np.zeros(2*batch_size) \nypolice_batch[:batch_size] = 1\n\n- 학습전: yhat(경찰의 예측)을 관찰\n\nnet_police(Xreal_batch)[:5]\n\n&lt;tf.Tensor: shape=(5, 1), dtype=float32, numpy=\narray([[0.46388364],\n       [0.45302677],\n       [0.50195265],\n       [0.543617  ],\n       [0.44746903]], dtype=float32)&gt;\n\n\n\nnet_police(Xfake_batch)[:5]\n\n&lt;tf.Tensor: shape=(5, 1), dtype=float32, numpy=\narray([[0.49146503],\n       [0.49262333],\n       [0.49713606],\n       [0.494118  ],\n       [0.50205135]], dtype=float32)&gt;\n\n\n- 학습하자: compile and fit\n\nnet_police.compile(loss=tf.losses.binary_crossentropy, optimizer='adam') \n\n\nnet_police.fit(Xpolice_batch,ypolice_batch)\n\n32/32 [==============================] - 0s 1ms/step - loss: 0.0591\n\n\n&lt;keras.callbacks.History at 0x7f549041d360&gt;\n\n\n- 훈련된 경찰의 성능을 살펴보자.\n\nnet_police(Xreal_batch)[:5]\n\n&lt;tf.Tensor: shape=(5, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)&gt;\n\n\n\nnet_police(Xfake_batch)[:5]\n\n&lt;tf.Tensor: shape=(5, 1), dtype=float32, numpy=\narray([[6.4868483e-13],\n       [1.3337571e-19],\n       [2.0222801e-11],\n       [1.7586674e-22],\n       [5.3691273e-20]], dtype=float32)&gt;\n\n\n\n\n위조범네트워크의 성능을 향상시키자.\n- 자료구조: 네트워크의 입력은 임의의 노이즈, 네트워크의 출력은 fakeimage\n- 손실함수의 설계? - 위조범 네트워크의 출력은 fakeimage, 위조범 네트워크의 yhat은 fakeimage 이다! - 이 가짜이미지를 (위조범네트워크의 yhat을) 경찰이 진짜라고 판단해야 위조범 입장에서는 좋은것. 즉 “경찰네트워크(위조범네트워크의yhat) \\(\\approx\\) 1” 이어야 함\n\ndef loss_counterfeiter(y,yhat): \n    # note that yhat is fake image! \n    return tf.losses.binary_crossentropy(y,net_police(yhat)) # here label should be 1 \n\n\nycounterfeiter_batch = np.ones(batch_size) \nnet_counterfeiter.compile(loss=loss_counterfeiter,optimizer = 'adam')\n\n\nnet_counterfeiter.fit(Noise_batch, ycounterfeiter_batch)\n\n16/16 [==============================] - 0s 1ms/step - loss: 2.0207\n\n\n&lt;keras.callbacks.History at 0x7f54902cded0&gt;\n\n\n\nplt.imshow(net_counterfeiter(Noise1).reshape(28,28))\n\n&lt;matplotlib.image.AxesImage at 0x7f542c24cfa0&gt;\n\n\n\n\n\n\n학습된 이미지의 하나의 샘플 (아직 노이즈같음)\n\n\nnet_police(net_counterfeiter(Noise_batch))[:5]\n\n&lt;tf.Tensor: shape=(5, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)&gt;\n\n\n\n노이즈같지만 아무튼 이정도로도 경찰은 속는다!\n\n\n\n두 적대적 네트워크를 경쟁시키자.\n\nfor _ in tqdm(range(500)):\n    # step1: 가짜이미지 생성, 데이터정리 \n    Noise_batch = np.random.normal(0,1,size=(batch_size,10))\n    Xfake_batch = net_counterfeiter(Noise_batch)\n    Xreal_batch = Xreal[np.random.randint(0,Xreal.shape[0],size=batch_size)]\n    \n    # step2: 경찰네트워크용 데이터 정리 \n    Xpolice_batch = tf.concat([Xreal_batch,Xfake_batch],axis=0)\n    ypolice_batch = np.zeros(2*batch_size)\n    ypolice_batch[:batch_size] = 1 \n    \n    # step3: 경찰네트워크 훈련\n    net_police.fit(Xpolice_batch,ypolice_batch,verbose=0) \n    \n    # step4: 위조범네트워크 훈련 \n    Xcounterfeiter_batch = Noise_batch # &lt;- 위조범 네트워크의 X \n    ycounterfeiter_batch = np.ones(batch_size) # &lt;- 위조범네트워크의 y \n    net_counterfeiter.fit(Noise_batch, ycounterfeiter_batch, verbose=0) \n\n100%|██████████| 500/500 [00:48&lt;00:00, 10.25it/s]\n\n\n\nfig, ax = plt.subplots(5,5)\nk=0\nfor i in range(5):\n    for j in range(5): \n        ax[i][j].imshow(net_counterfeiter.predict(Noise_batch)[k].reshape(28,28),cmap='gray')\n        k=k+1\nfig.set_figwidth(16)\nfig.set_figheight(16)\nfig.tight_layout()"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  }
]